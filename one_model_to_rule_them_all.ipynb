{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 2000\n",
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0  As U.S. budget fight looms, Republicans flip t...  \\\n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject   \n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews  \\\n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/True.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# functions to clean data\n",
    "\n",
    "# filter out first part containing CITY (news agency) and separator \"-\"\n",
    "\n",
    "# to be replaced: “ ”\n",
    "\n",
    "# search and replace regex\n",
    "double_quotes = r'“|”'\n",
    "single_quotes = r'’|‘'\n",
    "backslashes = r'\\\\'\n",
    "multiple_whitespace = r'\\t|\\v|\\f| '\n",
    "double_quotes = re.compile(double_quotes)\n",
    "single_quotes = re.compile(single_quotes)\n",
    "backslashes = re.compile(backslashes)\n",
    "multiple_whitespace = re.compile(multiple_whitespace)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    txt = row[1].lower()\n",
    "    txt = txt[txt.find('-')+1:].lstrip()\n",
    "    txt = double_quotes.sub('\"', txt)\n",
    "    txt = txt.replace(\"’\", \"\")\n",
    "    txt = txt.replace(\"‘\", \"\")\n",
    "    txt = multiple_whitespace.sub(' ', txt)\n",
    "    # remove everything before the first dash (news agency and city)\n",
    "    txt = txt[txt.find('-')+1:]\n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD> '\n",
    "    token[','] = ' <COMMA> '\n",
    "    token['\"'] = ' <QUOTATION_MARK> '\n",
    "    token[':'] = ' <COLON>'\n",
    "    token[';'] = ' <SEMICOLON> '\n",
    "    token['!'] = ' <EXCLAIMATION_MARK> '\n",
    "    token['?'] = ' <QUESTION_MARK> '\n",
    "    token['('] = ' <LEFT_PAREN> '\n",
    "    token[')'] = ' <RIGHT_PAREN> '\n",
    "    token['-'] = ' <QUESTION_MARK> '\n",
    "    token['\\n'] = ' <NEW_LINE> '\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max(tokenized, max):\n",
    "    padding_length = max - len(tokenized)\n",
    "    if padding_length == 0:\n",
    "        return tokenized\n",
    "    padding = ['<pad>' for i in range(padding_length)]\n",
    "    tokenized.extend(padding)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defense \"discretionary\" spending on programs that support education, scientific research, infrastructure, public health and environmental protection. \"the (trump) administration has already been willing to say: were going to increase non-defense discretionary spending ... by about 7 percent,\" meadows, chairman of the small but influential house freedom caucus, said on the program. \"now, democrats are saying thats not enough, we need to give the government a pay raise of 10 to 11 percent. for a fiscal conservative, i dont see where the rationale is. ... eventually you run out of other peoples money,\" he said. meadows was among republicans who voted in late december for their partys debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. \"its interesting to hear mark talk about fiscal responsibility,\" democratic u.s. representative joseph crowley said on cbs. crowley said the republican tax bill would require the  united states to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. \"this is one of the least ... fiscally responsible bills weve ever seen passed in the history of the house of representatives. i think were going to be paying for this for many, many years to come,\" crowley said. republicans insist the tax package, the biggest u.s. tax overhaul in more than 30 years,  will boost the economy and job growth. house speaker paul ryan, who also supported the tax bill, recently went further than meadows, making clear in a radio interview that welfare or \"entitlement reform,\" as the party often calls it, would be a top republican priority in 2018. in republican parlance, \"entitlement\" programs mean food stamps, housing assistance, medicare and medicaid health insurance for the elderly, poor and disabled, as well as other programs created by washington to assist the needy. democrats seized on ryans early december remarks, saying they showed republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. but the goals of house republicans may have to take a back seat to the senate, where the votes of some democrats will be needed to approve a budget and prevent a government shutdown. democrats will use their leverage in the senate, which republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the \"dreamers,\" people brought illegally to the country as children. trump in september put a march 2018 expiration date on the deferred action for childhood arrivals, or daca, program, which protects the young immigrants from deportation and provides them with work permits. the president has said in recent twitter messages he wants funding for his proposed mexican border wall and other immigration law changes in exchange for agreeing to help the dreamers. representative debbie dingell told cbs she did not favor linking that issue to other policy objectives, such as wall funding. \"we need to do daca clean,\" she said.  on wednesday, trump aides will meet with congressional leaders to discuss those issues. that will be followed by a weekend of strategy sessions for trump and republican leaders on jan. 6 and 7, the white house said. trump was also scheduled to meet on sunday with florida republican governor rick scott, who wants more emergency aid. the house has passed an $81 billion aid package after hurricanes in florida, texas and puerto rico, and wildfires in california. the package far exceeded the $44 billion requested by the trump administration. the senate has not yet voted on the aid. \n",
      "longest article contains 1613 tokens\n",
      "there are 20448 unique tokens\n",
      "articles equal length: True\n",
      "defense <quotation_mark> discretionary <quotation_mark> spending on programs that support education <comma> scientific research <comma> infrastructure <comma> public health and environmental protection <period> <quotation_mark> the <left_paren> trump <right_paren> administration has already been willing to say <colon> were going to increase non <question_mark> defense discretionary spending <period> <period> <period> by about 7 percent <comma> <quotation_mark> meadows <comma> chairman of the small but influential house freedom caucus <comma> said on the program <period> <quotation_mark> now <comma> democrats are saying thats not enough <comma> we need to give the government a pay raise of 10 to 11 percent <period> for a fiscal conservative <comma> i dont see where the rationale is <period> <period> <period> <period> eventually you run out of other peoples money <comma> <quotation_mark> he said <period> meadows was among republicans who voted in late december for their partys debt <question_mark> financed tax overhaul <comma> which is expected to balloon the federal budget deficit and add about $1 <period> 5 trillion over 10 years to the $20 trillion national debt <period> <quotation_mark> its interesting to hear mark talk about fiscal responsibility <comma> <quotation_mark> democratic u <period> s <period> representative joseph crowley said on cbs <period> crowley said the republican tax bill would require the united states to borrow $1 <period> 5 trillion <comma> to be paid off by future generations <comma> to finance tax cuts for corporations and the rich <period> <quotation_mark> this is one of the least <period> <period> <period> fiscally responsible bills weve ever seen passed in the history of the house of representatives <period> i think were going to be paying for this for many <comma> many years to come <comma> <quotation_mark> crowley said <period> republicans insist the tax package <comma> the biggest u <period> s <period> tax overhaul in more than 30 years <comma> will boost the economy and job growth <period> house speaker paul ryan <comma> who also supported the tax bill <comma> recently went further than meadows <comma> making clear in a radio interview that welfare or <quotation_mark> entitlement reform <comma> <quotation_mark> as the party often calls it <comma> would be a top republican priority in 2018 <period> in republican parlance <comma> <quotation_mark> entitlement <quotation_mark> programs mean food stamps <comma> housing assistance <comma> medicare and medicaid health insurance for the elderly <comma> poor and disabled <comma> as well as other programs created by washington to assist the needy <period> democrats seized on ryans early december remarks <comma> saying they showed republicans would try to pay for their tax overhaul by seeking spending cuts for social programs <period> but the goals of house republicans may have to take a back seat to the senate <comma> where the votes of some democrats will be needed to approve a budget and prevent a government shutdown <period> democrats will use their leverage in the senate <comma> which republicans narrowly control <comma> to defend both discretionary non <question_mark> defense programs and social spending <comma> while tackling the issue of the <quotation_mark> dreamers <comma> <quotation_mark> people brought illegally to the country as children <period> trump in september put a march 2018 expiration date on the deferred action for childhood arrivals <comma> or daca <comma> program <comma> which protects the young immigrants from deportation and provides them with work permits <period> the president has said in recent twitter messages he wants funding for his proposed mexican border wall and other immigration law changes in exchange for agreeing to help the dreamers <period> representative debbie dingell told cbs she did not favor linking that issue to other policy objectives <comma> such as wall funding <period> <quotation_mark> we need to do daca clean <comma> <quotation_mark> she said <period> on wednesday <comma> trump aides will meet with congressional leaders to discuss those issues <period> that will be followed by a weekend of strategy sessions for trump and republican leaders on jan <period> 6 and 7 <comma> the white house said <period> trump was also scheduled to meet on sunday with florida republican governor rick scott <comma> who wants more emergency aid <period> the house has passed an $81 billion aid package after hurricanes in florida <comma> texas and puerto rico <comma> and wildfires in california <period> the package far exceeded the $44 billion requested by the trump administration <period> the senate has not yet voted on the aid <period> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "['defense', '<quotation_mark>', 'discretionary', '<quotation_mark>', 'spending', 'on', 'programs', 'that', 'support', 'education', '<comma>', 'scientific', 'research', '<comma>', 'infrastructure', '<comma>', 'public', 'health', 'and', 'environmental', 'protection', '<period>', '<quotation_mark>', 'the', '<left_paren>', 'trump', '<right_paren>', 'administration', 'has', 'already', 'been', 'willing', 'to', 'say', '<colon>', 'were', 'going', 'to', 'increase', 'non', '<question_mark>', 'defense', 'discretionary', 'spending', '<period>', '<period>', '<period>', 'by', 'about', '7', 'percent', '<comma>', '<quotation_mark>', 'meadows', '<comma>', 'chairman', 'of', 'the', 'small', 'but', 'influential', 'house', 'freedom', 'caucus', '<comma>', 'said', 'on', 'the', 'program', '<period>', '<quotation_mark>', 'now', '<comma>', 'democrats', 'are', 'saying', 'thats', 'not', 'enough', '<comma>', 'we', 'need', 'to', 'give', 'the', 'government', 'a', 'pay', 'raise', 'of', '10', 'to', '11', 'percent', '<period>', 'for', 'a', 'fiscal', 'conservative', '<comma>', 'i', 'dont', 'see', 'where', 'the', 'rationale', 'is', '<period>', '<period>', '<period>', '<period>', 'eventually', 'you', 'run', 'out', 'of', 'other', 'peoples', 'money', '<comma>', '<quotation_mark>', 'he', 'said', '<period>', 'meadows', 'was', 'among', 'republicans', 'who', 'voted', 'in', 'late', 'december', 'for', 'their', 'partys', 'debt', '<question_mark>', 'financed', 'tax', 'overhaul', '<comma>', 'which', 'is', 'expected', 'to', 'balloon', 'the', 'federal', 'budget', 'deficit', 'and', 'add', 'about', '$1', '<period>', '5', 'trillion', 'over', '10', 'years', 'to', 'the', '$20', 'trillion', 'national', 'debt', '<period>', '<quotation_mark>', 'its', 'interesting', 'to', 'hear', 'mark', 'talk', 'about', 'fiscal', 'responsibility', '<comma>', '<quotation_mark>', 'democratic', 'u', '<period>', 's', '<period>', 'representative', 'joseph', 'crowley', 'said', 'on', 'cbs', '<period>', 'crowley', 'said', 'the', 'republican', 'tax', 'bill', 'would', 'require', 'the', 'united', 'states', 'to', 'borrow', '$1', '<period>', '5', 'trillion', '<comma>', 'to', 'be', 'paid', 'off', 'by', 'future', 'generations', '<comma>', 'to', 'finance', 'tax', 'cuts', 'for', 'corporations', 'and', 'the', 'rich', '<period>', '<quotation_mark>', 'this', 'is', 'one', 'of', 'the', 'least', '<period>', '<period>', '<period>', 'fiscally', 'responsible', 'bills', 'weve', 'ever', 'seen', 'passed', 'in', 'the', 'history', 'of', 'the', 'house', 'of', 'representatives', '<period>', 'i', 'think', 'were', 'going', 'to', 'be', 'paying', 'for', 'this', 'for', 'many', '<comma>', 'many', 'years', 'to', 'come', '<comma>', '<quotation_mark>', 'crowley', 'said', '<period>', 'republicans', 'insist', 'the', 'tax', 'package', '<comma>', 'the', 'biggest', 'u', '<period>', 's', '<period>', 'tax', 'overhaul', 'in', 'more', 'than', '30', 'years', '<comma>', 'will', 'boost', 'the', 'economy', 'and', 'job', 'growth', '<period>', 'house', 'speaker', 'paul', 'ryan', '<comma>', 'who', 'also', 'supported', 'the', 'tax', 'bill', '<comma>', 'recently', 'went', 'further', 'than', 'meadows', '<comma>', 'making', 'clear', 'in', 'a', 'radio', 'interview', 'that', 'welfare', 'or', '<quotation_mark>', 'entitlement', 'reform', '<comma>', '<quotation_mark>', 'as', 'the', 'party', 'often', 'calls', 'it', '<comma>', 'would', 'be', 'a', 'top', 'republican', 'priority', 'in', '2018', '<period>', 'in', 'republican', 'parlance', '<comma>', '<quotation_mark>', 'entitlement', '<quotation_mark>', 'programs', 'mean', 'food', 'stamps', '<comma>', 'housing', 'assistance', '<comma>', 'medicare', 'and', 'medicaid', 'health', 'insurance', 'for', 'the', 'elderly', '<comma>', 'poor', 'and', 'disabled', '<comma>', 'as', 'well', 'as', 'other', 'programs', 'created', 'by', 'washington', 'to', 'assist', 'the', 'needy', '<period>', 'democrats', 'seized', 'on', 'ryans', 'early', 'december', 'remarks', '<comma>', 'saying', 'they', 'showed', 'republicans', 'would', 'try', 'to', 'pay', 'for', 'their', 'tax', 'overhaul', 'by', 'seeking', 'spending', 'cuts', 'for', 'social', 'programs', '<period>', 'but', 'the', 'goals', 'of', 'house', 'republicans', 'may', 'have', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'senate', '<comma>', 'where', 'the', 'votes', 'of', 'some', 'democrats', 'will', 'be', 'needed', 'to', 'approve', 'a', 'budget', 'and', 'prevent', 'a', 'government', 'shutdown', '<period>', 'democrats', 'will', 'use', 'their', 'leverage', 'in', 'the', 'senate', '<comma>', 'which', 'republicans', 'narrowly', 'control', '<comma>', 'to', 'defend', 'both', 'discretionary', 'non', '<question_mark>', 'defense', 'programs', 'and', 'social', 'spending', '<comma>', 'while', 'tackling', 'the', 'issue', 'of', 'the', '<quotation_mark>', 'dreamers', '<comma>', '<quotation_mark>', 'people', 'brought', 'illegally', 'to', 'the', 'country', 'as', 'children', '<period>', 'trump', 'in', 'september', 'put', 'a', 'march', '2018', 'expiration', 'date', 'on', 'the', 'deferred', 'action', 'for', 'childhood', 'arrivals', '<comma>', 'or', 'daca', '<comma>', 'program', '<comma>', 'which', 'protects', 'the', 'young', 'immigrants', 'from', 'deportation', 'and', 'provides', 'them', 'with', 'work', 'permits', '<period>', 'the', 'president', 'has', 'said', 'in', 'recent', 'twitter', 'messages', 'he', 'wants', 'funding', 'for', 'his', 'proposed', 'mexican', 'border', 'wall', 'and', 'other', 'immigration', 'law', 'changes', 'in', 'exchange', 'for', 'agreeing', 'to', 'help', 'the', 'dreamers', '<period>', 'representative', 'debbie', 'dingell', 'told', 'cbs', 'she', 'did', 'not', 'favor', 'linking', 'that', 'issue', 'to', 'other', 'policy', 'objectives', '<comma>', 'such', 'as', 'wall', 'funding', '<period>', '<quotation_mark>', 'we', 'need', 'to', 'do', 'daca', 'clean', '<comma>', '<quotation_mark>', 'she', 'said', '<period>', 'on', 'wednesday', '<comma>', 'trump', 'aides', 'will', 'meet', 'with', 'congressional', 'leaders', 'to', 'discuss', 'those', 'issues', '<period>', 'that', 'will', 'be', 'followed', 'by', 'a', 'weekend', 'of', 'strategy', 'sessions', 'for', 'trump', 'and', 'republican', 'leaders', 'on', 'jan', '<period>', '6', 'and', '7', '<comma>', 'the', 'white', 'house', 'said', '<period>', 'trump', 'was', 'also', 'scheduled', 'to', 'meet', 'on', 'sunday', 'with', 'florida', 'republican', 'governor', 'rick', 'scott', '<comma>', 'who', 'wants', 'more', 'emergency', 'aid', '<period>', 'the', 'house', 'has', 'passed', 'an', '$81', 'billion', 'aid', 'package', 'after', 'hurricanes', 'in', 'florida', '<comma>', 'texas', 'and', 'puerto', 'rico', '<comma>', 'and', 'wildfires', 'in', 'california', '<period>', 'the', 'package', 'far', 'exceeded', 'the', '$44', 'billion', 'requested', 'by', 'the', 'trump', 'administration', '<period>', 'the', 'senate', 'has', 'not', 'yet', 'voted', 'on', 'the', 'aid', '<period>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "df = df[:corpus_size]\n",
    "df = df.astype({'text': 'string'})\n",
    "\n",
    "df['text'] = df.apply(clean_data, axis=1)\n",
    "print(df['text'][0])\n",
    "articles = df['text'].values.tolist()\n",
    "\n",
    "longest_article = 0\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    for key, token in token_dict.items():\n",
    "        article = article.replace(key, token)\n",
    "    article = article.lower()\n",
    "    article = article.split()\n",
    "    if len(article) > longest_article:\n",
    "        longest_article = len(article)\n",
    "    tokenized_articles.append(article)\n",
    "# for key, token in token_dict.items():\n",
    "#     articles[0] = article[0].replace(key, token)\n",
    "\n",
    "print(f'longest article contains {longest_article} tokens')\n",
    "\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "for tokens in tokenized_articles:\n",
    "    tokens = pad_to_max(tokens, longest_article)\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "print(f'there are {len(unique_tokens)} unique tokens')\n",
    "\n",
    "print(\n",
    "    f'articles equal length: {len(tokenized_articles[0])==len(tokenized_articles[1])}')\n",
    "\n",
    "articles = [' '.join(art) for art in tokenized_articles]\n",
    "\n",
    "print(articles[0])\n",
    "print(tokenized_articles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(tokenized_articles, file)\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(unique_tokens, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 20449\n"
     ]
    }
   ],
   "source": [
    "# get words\n",
    "all_words = []\n",
    "\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'rb') as file:\n",
    "    all_words = pickle.load(file)\n",
    "\n",
    "all_words.append(' ')\n",
    "vocab_length = len(all_words)\n",
    "print(f'number of words: {vocab_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "722324\n"
     ]
    }
   ],
   "source": [
    "# read file\n",
    "corpus = []\n",
    "file_content = []\n",
    "with open('./data/tokenized_articles.dat', 'rb') as file:\n",
    "    file_content = pickle.load(file)\n",
    "\n",
    "# this approach uses embedding, and therefore doesn't need padding so we remove it from the prepared data\n",
    "print(len(file_content))\n",
    "for article in file_content:\n",
    "    corpus.extend([word.strip() for word in article if word not in ['<pad>', ' ']])\n",
    "\n",
    "print(len(corpus))\n",
    "# print(corpus[-30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_size * num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def save(f_path):\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, chunk_length=200, num_epochs=4000, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002):\n",
    "        self.chunk_len = chunk_length\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = self.num_epochs // 20 or 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "\n",
    "    def word_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_words.index(string[c])\n",
    "        return tensor\n",
    "\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(corpus) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.word_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.word_tensor(text_str[1:])\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "\n",
    "    def generate(self, initial_str='the president is dead', predict_len=200, temperature=0.85):\n",
    "        initial_words = initial_str.split(' ')\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.word_tensor(initial_words)\n",
    "        predicted = initial_words\n",
    "        \n",
    "        for p in range(len(initial_words) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_word = initial_input[-1]\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_word.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_word = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_word = [all_words[top_word]]\n",
    "            predicted.extend(predicted_word)\n",
    "            last_word = self.word_tensor(predicted_word)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(vocab_length, self.hidden_size, self.num_layers, vocab_length).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f'<{datetime.now()}>starting training')\n",
    "        lowest_loss = 100.0 # just a high value, should not be lower than 10\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            input, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(input[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "            if loss < lowest_loss:\n",
    "                self.best_model = self.rnn.state_dict()\n",
    "                print(f'<{datetime.now()}> better model found after {epoch}/{self.num_epochs} epochs with loss: {loss}')\n",
    "                lowest_loss = loss\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f'\\n\\n<{datetime.now()}> | epoch: {epoch}/{self.num_epochs} | loss: {loss}')\n",
    "                # print(self.generate())\n",
    "        file_path = f'./models/bidir_lstm_chunk_{self.chunk_len}_words_{vocab_length}_loss_{lowest_loss}.pt'\n",
    "        print(f'saving model at {file_path}')\n",
    "        torch.save(self.best_model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2023-05-29 17:16:40.936561>starting training\n",
      "<2023-05-29 17:16:41.839686> better model found after 1/10000 epochs with loss: 9.929571940104166\n",
      "<2023-05-29 17:16:42.746757> better model found after 2/10000 epochs with loss: 9.911407063802084\n",
      "<2023-05-29 17:16:43.600788> better model found after 3/10000 epochs with loss: 9.866660970052083\n",
      "<2023-05-29 17:16:44.472761> better model found after 4/10000 epochs with loss: 9.818041178385416\n",
      "<2023-05-29 17:16:45.345787> better model found after 5/10000 epochs with loss: 9.772354329427083\n",
      "<2023-05-29 17:16:46.208759> better model found after 6/10000 epochs with loss: 9.625013020833334\n",
      "<2023-05-29 17:16:47.080785> better model found after 7/10000 epochs with loss: 9.422939453125\n",
      "<2023-05-29 17:16:47.941787> better model found after 8/10000 epochs with loss: 8.801774088541666\n",
      "<2023-05-29 17:16:48.852787> better model found after 9/10000 epochs with loss: 8.1427197265625\n",
      "<2023-05-29 17:16:49.717760> better model found after 10/10000 epochs with loss: 7.83724853515625\n",
      "<2023-05-29 17:16:50.578757> better model found after 11/10000 epochs with loss: 7.38239013671875\n",
      "<2023-05-29 17:16:54.842759> better model found after 16/10000 epochs with loss: 7.283505859375\n",
      "<2023-05-29 17:16:55.715759> better model found after 17/10000 epochs with loss: 7.250443522135416\n",
      "<2023-05-29 17:16:59.056786> better model found after 21/10000 epochs with loss: 7.24990966796875\n",
      "<2023-05-29 17:17:00.744787> better model found after 23/10000 epochs with loss: 6.963158365885417\n",
      "<2023-05-29 17:17:02.417761> better model found after 25/10000 epochs with loss: 6.869869791666667\n",
      "<2023-05-29 17:17:09.172759> better model found after 33/10000 epochs with loss: 6.803305257161458\n",
      "<2023-05-29 17:17:17.727788> better model found after 43/10000 epochs with loss: 6.772777099609375\n",
      "<2023-05-29 17:17:19.392789> better model found after 45/10000 epochs with loss: 6.74474609375\n",
      "<2023-05-29 17:17:21.936763> better model found after 48/10000 epochs with loss: 6.72590576171875\n",
      "<2023-05-29 17:17:26.146784> better model found after 53/10000 epochs with loss: 6.486444905598958\n",
      "<2023-05-29 17:17:46.547786> better model found after 77/10000 epochs with loss: 6.370924072265625\n",
      "<2023-05-29 17:18:24.652758> better model found after 122/10000 epochs with loss: 6.275509440104167\n",
      "<2023-05-29 17:18:37.341786> better model found after 137/10000 epochs with loss: 6.222818603515625\n",
      "<2023-05-29 17:18:39.022786> better model found after 139/10000 epochs with loss: 6.072920328776042\n",
      "<2023-05-29 17:19:52.861782> better model found after 226/10000 epochs with loss: 5.994774576822917\n",
      "<2023-05-29 17:20:01.338787> better model found after 236/10000 epochs with loss: 5.956690266927083\n",
      "<2023-05-29 17:20:40.092756> better model found after 281/10000 epochs with loss: 5.955762939453125\n",
      "<2023-05-29 17:20:49.731781> better model found after 292/10000 epochs with loss: 5.95396484375\n",
      "<2023-05-29 17:20:50.612758> better model found after 293/10000 epochs with loss: 5.696877034505208\n",
      "<2023-05-29 17:21:49.392791> better model found after 362/10000 epochs with loss: 5.508555094401042\n",
      "<2023-05-29 17:22:46.235815> better model found after 429/10000 epochs with loss: 5.393789876302083\n",
      "<2023-05-29 17:23:46.269352> better model found after 500/10000 epochs with loss: 5.382697347005208\n",
      "\n",
      "\n",
      "<2023-05-29 17:23:46.269352> | epoch: 500/10000 | loss: 5.382697347005208\n",
      "<2023-05-29 17:24:19.419322> better model found after 539/10000 epochs with loss: 5.3757417805989585\n",
      "<2023-05-29 17:24:29.798348> better model found after 551/10000 epochs with loss: 5.3166597493489585\n",
      "<2023-05-29 17:24:42.770350> better model found after 566/10000 epochs with loss: 5.292700602213542\n",
      "<2023-05-29 17:25:16.633322> better model found after 606/10000 epochs with loss: 5.2365104166666665\n",
      "<2023-05-29 17:25:27.885351> better model found after 619/10000 epochs with loss: 5.108373209635417\n",
      "<2023-05-29 17:26:15.315345> better model found after 675/10000 epochs with loss: 5.023249104817708\n",
      "<2023-05-29 17:26:18.782322> better model found after 679/10000 epochs with loss: 5.00625\n",
      "<2023-05-29 17:26:32.473322> better model found after 695/10000 epochs with loss: 4.950972493489584\n",
      "<2023-05-29 17:26:54.645350> better model found after 721/10000 epochs with loss: 4.580943603515625\n",
      "\n",
      "\n",
      "<2023-05-29 17:30:47.579457> | epoch: 1000/10000 | loss: 5.06974609375\n",
      "<2023-05-29 17:31:19.522462> better model found after 1038/10000 epochs with loss: 4.155640869140625\n",
      "<2023-05-29 17:33:44.963533> better model found after 1212/10000 epochs with loss: 4.138538004557292\n",
      "<2023-05-29 17:36:17.888535> better model found after 1395/10000 epochs with loss: 3.97650634765625\n",
      "<2023-05-29 17:36:58.346534> better model found after 1443/10000 epochs with loss: 3.877213541666667\n",
      "\n",
      "\n",
      "<2023-05-29 17:37:46.247557> | epoch: 1500/10000 | loss: 5.441239827473958\n",
      "<2023-05-29 17:41:47.961774> better model found after 1789/10000 epochs with loss: 3.8290384928385417\n",
      "<2023-05-29 17:41:50.594803> better model found after 1792/10000 epochs with loss: 3.81736572265625\n",
      "<2023-05-29 17:42:31.999780> better model found after 1841/10000 epochs with loss: 3.4672245279947917\n",
      "\n",
      "\n",
      "<2023-05-29 17:44:45.872816> | epoch: 2000/10000 | loss: 4.964481201171875\n",
      "<2023-05-29 17:44:57.085816> better model found after 2013/10000 epochs with loss: 3.2464595540364583\n",
      "<2023-05-29 17:52:48.607652> better model found after 2496/10000 epochs with loss: 3.1070878092447916\n",
      "\n",
      "\n",
      "<2023-05-29 17:52:51.957648> | epoch: 2500/10000 | loss: 4.084676106770833\n",
      "<2023-05-29 17:54:44.760689> better model found after 2635/10000 epochs with loss: 2.9841253662109377\n",
      "<2023-05-29 17:56:17.303690> better model found after 2745/10000 epochs with loss: 2.580184122721354\n",
      "\n",
      "\n",
      "<2023-05-29 17:59:50.379713> | epoch: 3000/10000 | loss: 3.0817106119791666\n",
      "<2023-05-29 18:03:10.194694> better model found after 3237/10000 epochs with loss: 2.395590616861979\n",
      "\n",
      "\n",
      "<2023-05-29 18:06:51.854226> | epoch: 3500/10000 | loss: 5.265910237630209\n",
      "<2023-05-29 18:08:48.345248> better model found after 3638/10000 epochs with loss: 2.374522705078125\n",
      "\n",
      "\n",
      "<2023-05-29 18:13:54.425239> | epoch: 4000/10000 | loss: 4.492086181640625\n",
      "<2023-05-29 18:14:32.640234> better model found after 4045/10000 epochs with loss: 2.3079128011067707\n",
      "<2023-05-29 18:14:49.712212> better model found after 4065/10000 epochs with loss: 1.7396950276692709\n"
     ]
    }
   ],
   "source": [
    "# default parameters for generator: \n",
    "# chunk_length=200, num_epochs=4000, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002\n",
    "\n",
    "chunk_lengths = [50, 100, 150, 200, 300]\n",
    "epoch_numbers = [500, 1000, 2000, 3000, 5000]\n",
    "batch_sizes = [1, 2, 5, 10]\n",
    "hidden_sizes = [64, 128, 256, 512]\n",
    "layer_numbers = [2, 4]\n",
    "learning_rates = [0.001, 0.002, 0.003, 0.005]\n",
    "\n",
    "\n",
    "gentext = Generator()\n",
    "gentext.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    stmt = ' '.join(gentext.generate(initial_str='i would like', predict_len=150, temperature=temperature)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "    print(f'temperature: {temperature}\\nstatement:\\n{stmt}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
