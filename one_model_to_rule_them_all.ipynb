{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire project can be found at: https://github.com/mrulle/deep_learning_exam_project\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Text Generator\n",
    "I dette projekt er gruppen blevet hyret til at få produceret fake news til danske medier, får at skabe ravage i det danske samfund. Projektet vil bestå af en Model der vil generere tekst ud fra en start sætning. Tekst generering er svært at få en maskine til at forstå, da tekst har kontekst bagved(en sætning af ord af bogstaver). \n",
    "\n",
    "# Dataindsamling\n",
    "Der skulle indhentes data, hvilket blev taget fra kaggle: dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset.\n",
    "I dette datasæt skulle der være nyhedsartikler der både var rigtige og falske, hvorfra tekst modellen skulle have forskellige scenarier, der er kun brugt rigtige artikler.\n",
    "\n",
    "# Prepare the data\n",
    "Dataene tjekkes ud fra om de kan encodes rigtig i utf-8, hvorfra vi laver word 2 vector tokens, der kobler sekvens af ord sammen, som har til formål at modellen vil kunne forudsige ordenes betydning bedre i en vector matrix. \n",
    "\n",
    "# Model\n",
    "Vi har valgt at bruge LSTM grundet dens hukommelse, og oven i LSTM, har vi valgt at bruge Bidirectional, for at få den til at huske sekvensen, altså mere kontekst i en sætning.\n",
    "\n",
    "# Tuning af hyper parametre\n",
    "Vi testede forskellige parameter for at finde den bedste nøjagtighed, med de forskellige parameter.\n",
    "Herfra trænede vi modellen med de fundne bedste hyper parametre.\n",
    "\n",
    "# Evaluering\n",
    "Visualisering af nøjagtighed med plots, og lave en forudsigelse på generering af tekst. \n",
    "\n",
    "# How to do\n",
    "Der skal være 2 mapper, en models og en data. I data skal i tage de de 3 tokenized & unique_words i. I data mappen skal True.csv filen fra datasættet være i. Så kan i kører notebook’en.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 2000\n",
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0  As U.S. budget fight looms, Republicans flip t...  \\\n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject   \n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews  \\\n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/True.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# functions to clean data\n",
    "\n",
    "# filter out first part containing CITY (news agency) and separator \"-\"\n",
    "\n",
    "# to be replaced: “ ”\n",
    "\n",
    "# search and replace regex\n",
    "double_quotes = r'“|”'\n",
    "single_quotes = r'’|‘'\n",
    "backslashes = r'\\\\'\n",
    "multiple_whitespace = r'\\t|\\v|\\f| '\n",
    "double_quotes = re.compile(double_quotes)\n",
    "single_quotes = re.compile(single_quotes)\n",
    "backslashes = re.compile(backslashes)\n",
    "multiple_whitespace = re.compile(multiple_whitespace)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    txt = row[1].lower()\n",
    "    txt = txt[txt.find('-')+1:].lstrip()\n",
    "    txt = double_quotes.sub('\"', txt)\n",
    "    txt = txt.replace(\"’\", \"\")\n",
    "    txt = txt.replace(\"‘\", \"\")\n",
    "    txt = multiple_whitespace.sub(' ', txt)\n",
    "    # remove everything before the first dash (news agency and city)\n",
    "    txt = txt[txt.find('-')+1:]\n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD> '\n",
    "    token[','] = ' <COMMA> '\n",
    "    token['\"'] = ' <QUOTATION_MARK> '\n",
    "    token[':'] = ' <COLON>'\n",
    "    token[';'] = ' <SEMICOLON> '\n",
    "    token['!'] = ' <EXCLAIMATION_MARK> '\n",
    "    token['?'] = ' <QUESTION_MARK> '\n",
    "    token['('] = ' <LEFT_PAREN> '\n",
    "    token[')'] = ' <RIGHT_PAREN> '\n",
    "    token['-'] = ' <QUESTION_MARK> '\n",
    "    token['\\n'] = ' <NEW_LINE> '\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply padding for all articles to match length of longest article\n",
    "# this turned out to be unnecessary due to the get_random_batch and\n",
    "def pad_to_max(tokenized, max):\n",
    "    padding_length = max - len(tokenized)\n",
    "    if padding_length == 0:\n",
    "        return tokenized\n",
    "    padding = ['<pad>' for i in range(padding_length)]\n",
    "    tokenized.extend(padding)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defense \"discretionary\" spending on programs that support education, scientific research, infrastructure, public health and environmental protection. \"the (trump) administration has already been willing to say: were going to increase non-defense discretionary spending ... by about 7 percent,\" meadows, chairman of the small but influential house freedom caucus, said on the program. \"now, democrats are saying thats not enough, we need to give the government a pay raise of 10 to 11 percent. for a fiscal conservative, i dont see where the rationale is. ... eventually you run out of other peoples money,\" he said. meadows was among republicans who voted in late december for their partys debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. \"its interesting to hear mark talk about fiscal responsibility,\" democratic u.s. representative joseph crowley said on cbs. crowley said the republican tax bill would require the  united states to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. \"this is one of the least ... fiscally responsible bills weve ever seen passed in the history of the house of representatives. i think were going to be paying for this for many, many years to come,\" crowley said. republicans insist the tax package, the biggest u.s. tax overhaul in more than 30 years,  will boost the economy and job growth. house speaker paul ryan, who also supported the tax bill, recently went further than meadows, making clear in a radio interview that welfare or \"entitlement reform,\" as the party often calls it, would be a top republican priority in 2018. in republican parlance, \"entitlement\" programs mean food stamps, housing assistance, medicare and medicaid health insurance for the elderly, poor and disabled, as well as other programs created by washington to assist the needy. democrats seized on ryans early december remarks, saying they showed republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. but the goals of house republicans may have to take a back seat to the senate, where the votes of some democrats will be needed to approve a budget and prevent a government shutdown. democrats will use their leverage in the senate, which republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the \"dreamers,\" people brought illegally to the country as children. trump in september put a march 2018 expiration date on the deferred action for childhood arrivals, or daca, program, which protects the young immigrants from deportation and provides them with work permits. the president has said in recent twitter messages he wants funding for his proposed mexican border wall and other immigration law changes in exchange for agreeing to help the dreamers. representative debbie dingell told cbs she did not favor linking that issue to other policy objectives, such as wall funding. \"we need to do daca clean,\" she said.  on wednesday, trump aides will meet with congressional leaders to discuss those issues. that will be followed by a weekend of strategy sessions for trump and republican leaders on jan. 6 and 7, the white house said. trump was also scheduled to meet on sunday with florida republican governor rick scott, who wants more emergency aid. the house has passed an $81 billion aid package after hurricanes in florida, texas and puerto rico, and wildfires in california. the package far exceeded the $44 billion requested by the trump administration. the senate has not yet voted on the aid. \n",
      "longest article contains 1613 tokens\n",
      "there are 20448 unique tokens\n",
      "articles equal length: True\n",
      "defense <quotation_mark> discretionary <quotation_mark> spending on programs that support education <comma> scientific research <comma> infrastructure <comma> public health and environmental protection <period> <quotation_mark> the <left_paren> trump <right_paren> administration has already been willing to say <colon> were going to increase non <question_mark> defense discretionary spending <period> <period> <period> by about 7 percent <comma> <quotation_mark> meadows <comma> chairman of the small but influential house freedom caucus <comma> said on the program <period> <quotation_mark> now <comma> democrats are saying thats not enough <comma> we need to give the government a pay raise of 10 to 11 percent <period> for a fiscal conservative <comma> i dont see where the rationale is <period> <period> <period> <period> eventually you run out of other peoples money <comma> <quotation_mark> he said <period> meadows was among republicans who voted in late december for their partys debt <question_mark> financed tax overhaul <comma> which is expected to balloon the federal budget deficit and add about $1 <period> 5 trillion over 10 years to the $20 trillion national debt <period> <quotation_mark> its interesting to hear mark talk about fiscal responsibility <comma> <quotation_mark> democratic u <period> s <period> representative joseph crowley said on cbs <period> crowley said the republican tax bill would require the united states to borrow $1 <period> 5 trillion <comma> to be paid off by future generations <comma> to finance tax cuts for corporations and the rich <period> <quotation_mark> this is one of the least <period> <period> <period> fiscally responsible bills weve ever seen passed in the history of the house of representatives <period> i think were going to be paying for this for many <comma> many years to come <comma> <quotation_mark> crowley said <period> republicans insist the tax package <comma> the biggest u <period> s <period> tax overhaul in more than 30 years <comma> will boost the economy and job growth <period> house speaker paul ryan <comma> who also supported the tax bill <comma> recently went further than meadows <comma> making clear in a radio interview that welfare or <quotation_mark> entitlement reform <comma> <quotation_mark> as the party often calls it <comma> would be a top republican priority in 2018 <period> in republican parlance <comma> <quotation_mark> entitlement <quotation_mark> programs mean food stamps <comma> housing assistance <comma> medicare and medicaid health insurance for the elderly <comma> poor and disabled <comma> as well as other programs created by washington to assist the needy <period> democrats seized on ryans early december remarks <comma> saying they showed republicans would try to pay for their tax overhaul by seeking spending cuts for social programs <period> but the goals of house republicans may have to take a back seat to the senate <comma> where the votes of some democrats will be needed to approve a budget and prevent a government shutdown <period> democrats will use their leverage in the senate <comma> which republicans narrowly control <comma> to defend both discretionary non <question_mark> defense programs and social spending <comma> while tackling the issue of the <quotation_mark> dreamers <comma> <quotation_mark> people brought illegally to the country as children <period> trump in september put a march 2018 expiration date on the deferred action for childhood arrivals <comma> or daca <comma> program <comma> which protects the young immigrants from deportation and provides them with work permits <period> the president has said in recent twitter messages he wants funding for his proposed mexican border wall and other immigration law changes in exchange for agreeing to help the dreamers <period> representative debbie dingell told cbs she did not favor linking that issue to other policy objectives <comma> such as wall funding <period> <quotation_mark> we need to do daca clean <comma> <quotation_mark> she said <period> on wednesday <comma> trump aides will meet with congressional leaders to discuss those issues <period> that will be followed by a weekend of strategy sessions for trump and republican leaders on jan <period> 6 and 7 <comma> the white house said <period> trump was also scheduled to meet on sunday with florida republican governor rick scott <comma> who wants more emergency aid <period> the house has passed an $81 billion aid package after hurricanes in florida <comma> texas and puerto rico <comma> and wildfires in california <period> the package far exceeded the $44 billion requested by the trump administration <period> the senate has not yet voted on the aid <period> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "['defense', '<quotation_mark>', 'discretionary', '<quotation_mark>', 'spending', 'on', 'programs', 'that', 'support', 'education', '<comma>', 'scientific', 'research', '<comma>', 'infrastructure', '<comma>', 'public', 'health', 'and', 'environmental', 'protection', '<period>', '<quotation_mark>', 'the', '<left_paren>', 'trump', '<right_paren>', 'administration', 'has', 'already', 'been', 'willing', 'to', 'say', '<colon>', 'were', 'going', 'to', 'increase', 'non', '<question_mark>', 'defense', 'discretionary', 'spending', '<period>', '<period>', '<period>', 'by', 'about', '7', 'percent', '<comma>', '<quotation_mark>', 'meadows', '<comma>', 'chairman', 'of', 'the', 'small', 'but', 'influential', 'house', 'freedom', 'caucus', '<comma>', 'said', 'on', 'the', 'program', '<period>', '<quotation_mark>', 'now', '<comma>', 'democrats', 'are', 'saying', 'thats', 'not', 'enough', '<comma>', 'we', 'need', 'to', 'give', 'the', 'government', 'a', 'pay', 'raise', 'of', '10', 'to', '11', 'percent', '<period>', 'for', 'a', 'fiscal', 'conservative', '<comma>', 'i', 'dont', 'see', 'where', 'the', 'rationale', 'is', '<period>', '<period>', '<period>', '<period>', 'eventually', 'you', 'run', 'out', 'of', 'other', 'peoples', 'money', '<comma>', '<quotation_mark>', 'he', 'said', '<period>', 'meadows', 'was', 'among', 'republicans', 'who', 'voted', 'in', 'late', 'december', 'for', 'their', 'partys', 'debt', '<question_mark>', 'financed', 'tax', 'overhaul', '<comma>', 'which', 'is', 'expected', 'to', 'balloon', 'the', 'federal', 'budget', 'deficit', 'and', 'add', 'about', '$1', '<period>', '5', 'trillion', 'over', '10', 'years', 'to', 'the', '$20', 'trillion', 'national', 'debt', '<period>', '<quotation_mark>', 'its', 'interesting', 'to', 'hear', 'mark', 'talk', 'about', 'fiscal', 'responsibility', '<comma>', '<quotation_mark>', 'democratic', 'u', '<period>', 's', '<period>', 'representative', 'joseph', 'crowley', 'said', 'on', 'cbs', '<period>', 'crowley', 'said', 'the', 'republican', 'tax', 'bill', 'would', 'require', 'the', 'united', 'states', 'to', 'borrow', '$1', '<period>', '5', 'trillion', '<comma>', 'to', 'be', 'paid', 'off', 'by', 'future', 'generations', '<comma>', 'to', 'finance', 'tax', 'cuts', 'for', 'corporations', 'and', 'the', 'rich', '<period>', '<quotation_mark>', 'this', 'is', 'one', 'of', 'the', 'least', '<period>', '<period>', '<period>', 'fiscally', 'responsible', 'bills', 'weve', 'ever', 'seen', 'passed', 'in', 'the', 'history', 'of', 'the', 'house', 'of', 'representatives', '<period>', 'i', 'think', 'were', 'going', 'to', 'be', 'paying', 'for', 'this', 'for', 'many', '<comma>', 'many', 'years', 'to', 'come', '<comma>', '<quotation_mark>', 'crowley', 'said', '<period>', 'republicans', 'insist', 'the', 'tax', 'package', '<comma>', 'the', 'biggest', 'u', '<period>', 's', '<period>', 'tax', 'overhaul', 'in', 'more', 'than', '30', 'years', '<comma>', 'will', 'boost', 'the', 'economy', 'and', 'job', 'growth', '<period>', 'house', 'speaker', 'paul', 'ryan', '<comma>', 'who', 'also', 'supported', 'the', 'tax', 'bill', '<comma>', 'recently', 'went', 'further', 'than', 'meadows', '<comma>', 'making', 'clear', 'in', 'a', 'radio', 'interview', 'that', 'welfare', 'or', '<quotation_mark>', 'entitlement', 'reform', '<comma>', '<quotation_mark>', 'as', 'the', 'party', 'often', 'calls', 'it', '<comma>', 'would', 'be', 'a', 'top', 'republican', 'priority', 'in', '2018', '<period>', 'in', 'republican', 'parlance', '<comma>', '<quotation_mark>', 'entitlement', '<quotation_mark>', 'programs', 'mean', 'food', 'stamps', '<comma>', 'housing', 'assistance', '<comma>', 'medicare', 'and', 'medicaid', 'health', 'insurance', 'for', 'the', 'elderly', '<comma>', 'poor', 'and', 'disabled', '<comma>', 'as', 'well', 'as', 'other', 'programs', 'created', 'by', 'washington', 'to', 'assist', 'the', 'needy', '<period>', 'democrats', 'seized', 'on', 'ryans', 'early', 'december', 'remarks', '<comma>', 'saying', 'they', 'showed', 'republicans', 'would', 'try', 'to', 'pay', 'for', 'their', 'tax', 'overhaul', 'by', 'seeking', 'spending', 'cuts', 'for', 'social', 'programs', '<period>', 'but', 'the', 'goals', 'of', 'house', 'republicans', 'may', 'have', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'senate', '<comma>', 'where', 'the', 'votes', 'of', 'some', 'democrats', 'will', 'be', 'needed', 'to', 'approve', 'a', 'budget', 'and', 'prevent', 'a', 'government', 'shutdown', '<period>', 'democrats', 'will', 'use', 'their', 'leverage', 'in', 'the', 'senate', '<comma>', 'which', 'republicans', 'narrowly', 'control', '<comma>', 'to', 'defend', 'both', 'discretionary', 'non', '<question_mark>', 'defense', 'programs', 'and', 'social', 'spending', '<comma>', 'while', 'tackling', 'the', 'issue', 'of', 'the', '<quotation_mark>', 'dreamers', '<comma>', '<quotation_mark>', 'people', 'brought', 'illegally', 'to', 'the', 'country', 'as', 'children', '<period>', 'trump', 'in', 'september', 'put', 'a', 'march', '2018', 'expiration', 'date', 'on', 'the', 'deferred', 'action', 'for', 'childhood', 'arrivals', '<comma>', 'or', 'daca', '<comma>', 'program', '<comma>', 'which', 'protects', 'the', 'young', 'immigrants', 'from', 'deportation', 'and', 'provides', 'them', 'with', 'work', 'permits', '<period>', 'the', 'president', 'has', 'said', 'in', 'recent', 'twitter', 'messages', 'he', 'wants', 'funding', 'for', 'his', 'proposed', 'mexican', 'border', 'wall', 'and', 'other', 'immigration', 'law', 'changes', 'in', 'exchange', 'for', 'agreeing', 'to', 'help', 'the', 'dreamers', '<period>', 'representative', 'debbie', 'dingell', 'told', 'cbs', 'she', 'did', 'not', 'favor', 'linking', 'that', 'issue', 'to', 'other', 'policy', 'objectives', '<comma>', 'such', 'as', 'wall', 'funding', '<period>', '<quotation_mark>', 'we', 'need', 'to', 'do', 'daca', 'clean', '<comma>', '<quotation_mark>', 'she', 'said', '<period>', 'on', 'wednesday', '<comma>', 'trump', 'aides', 'will', 'meet', 'with', 'congressional', 'leaders', 'to', 'discuss', 'those', 'issues', '<period>', 'that', 'will', 'be', 'followed', 'by', 'a', 'weekend', 'of', 'strategy', 'sessions', 'for', 'trump', 'and', 'republican', 'leaders', 'on', 'jan', '<period>', '6', 'and', '7', '<comma>', 'the', 'white', 'house', 'said', '<period>', 'trump', 'was', 'also', 'scheduled', 'to', 'meet', 'on', 'sunday', 'with', 'florida', 'republican', 'governor', 'rick', 'scott', '<comma>', 'who', 'wants', 'more', 'emergency', 'aid', '<period>', 'the', 'house', 'has', 'passed', 'an', '$81', 'billion', 'aid', 'package', 'after', 'hurricanes', 'in', 'florida', '<comma>', 'texas', 'and', 'puerto', 'rico', '<comma>', 'and', 'wildfires', 'in', 'california', '<period>', 'the', 'package', 'far', 'exceeded', 'the', '$44', 'billion', 'requested', 'by', 'the', 'trump', 'administration', '<period>', 'the', 'senate', 'has', 'not', 'yet', 'voted', 'on', 'the', 'aid', '<period>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "df = df[:corpus_size]\n",
    "df = df.astype({'text': 'string'})\n",
    "\n",
    "df['text'] = df.apply(clean_data, axis=1)\n",
    "print(df['text'][0])\n",
    "articles = df['text'].values.tolist()\n",
    "\n",
    "longest_article = 0\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    for key, token in token_dict.items():\n",
    "        article = article.replace(key, token)\n",
    "    article = article.lower()\n",
    "    article = article.split()\n",
    "    if len(article) > longest_article:\n",
    "        longest_article = len(article)\n",
    "    tokenized_articles.append(article)\n",
    "# for key, token in token_dict.items():\n",
    "#     articles[0] = article[0].replace(key, token)\n",
    "\n",
    "print(f'longest article contains {longest_article} tokens')\n",
    "\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "for tokens in tokenized_articles:\n",
    "    tokens = pad_to_max(tokens, longest_article)\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "print(f'there are {len(unique_tokens)} unique tokens')\n",
    "\n",
    "print(\n",
    "    f'articles equal length: {len(tokenized_articles[0])==len(tokenized_articles[1])}')\n",
    "\n",
    "articles = [' '.join(art) for art in tokenized_articles]\n",
    "\n",
    "print(articles[0])\n",
    "print(tokenized_articles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(tokenized_articles, file)\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(unique_tokens, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 20449\n"
     ]
    }
   ],
   "source": [
    "# get words\n",
    "all_words = []\n",
    "\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'rb') as file:\n",
    "    all_words = pickle.load(file)\n",
    "\n",
    "all_words.append(' ')\n",
    "vocab_length = len(all_words)\n",
    "print(f'number of words: {vocab_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "722324\n"
     ]
    }
   ],
   "source": [
    "# read file\n",
    "corpus = []\n",
    "file_content = []\n",
    "with open('./data/tokenized_articles.dat', 'rb') as file:\n",
    "    file_content = pickle.load(file)\n",
    "\n",
    "# this approach uses embedding, and therefore doesn't need padding so we remove it from the prepared data\n",
    "print(len(file_content))\n",
    "for article in file_content:\n",
    "    corpus.extend([word.strip() for word in article if word not in ['<pad>', ' ']])\n",
    "\n",
    "print(len(corpus))\n",
    "# print(corpus[-30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_size * num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        \n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def save(f_path):\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002):\n",
    "        self.chunk_len = chunk_length\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = self.num_epochs // 20 or 1\n",
    "        self.plot_every = self.num_epochs // 40 or 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "\n",
    "\n",
    "    def word_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_words.index(string[c])\n",
    "        return tensor\n",
    "    \n",
    "    def get_random_chunk(self, chunk_length):\n",
    "        start_idx = random.randint(0, len(corpus) - chunk_length)\n",
    "        end_idx = start_idx + chunk_length + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        return text_str\n",
    "\n",
    "    # method should be split to get random string, and convert to tensors\n",
    "    def get_random_batch(self, chunk_length):\n",
    "        start_idx = random.randint(0, len(corpus) - chunk_length)\n",
    "        end_idx = start_idx + chunk_length + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, chunk_length)\n",
    "        text_target = torch.zeros(self.batch_size, chunk_length)\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.word_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.word_tensor(text_str[1:])\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "\n",
    "    def generate(self, initial_str='the president is dead', predict_len=200, temperature=0.85):\n",
    "        initial_words = initial_str.split(' ')\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.word_tensor(initial_words)\n",
    "        predicted = initial_words\n",
    "        \n",
    "        for p in range(len(initial_words) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_word = initial_input[-1]\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_word.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_word = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_word = [all_words[top_word]]\n",
    "            predicted.extend(predicted_word)\n",
    "            last_word = self.word_tensor(predicted_word)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(vocab_length, self.hidden_size, self.num_layers, vocab_length).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f'<{datetime.now()}>starting training')\n",
    "        lowest_loss = 100.0 # just a high value, should not be lower than 10\n",
    "        all_losses = []\n",
    "        accumulated_losses = 0\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            input, target = self.get_random_batch(self.chunk_len)\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(input[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "            accumulated_losses += loss\n",
    "            if loss < lowest_loss:\n",
    "                self.best_model = self.rnn.state_dict()\n",
    "                # print(f'<{datetime.now()}> better model found after {epoch}/{self.num_epochs} epochs with loss: {loss}')\n",
    "                lowest_loss = loss\n",
    "            if epoch % self.plot_every == 0:\n",
    "                all_losses.append(accumulated_losses / self.plot_every)\n",
    "                accumulated_losses = 0\n",
    "            if epoch % self.print_every == 0:\n",
    "                pass\n",
    "                # print(f'\\n\\n<{datetime.now()}> | epoch: {epoch}/{self.num_epochs} | loss: {loss}')\n",
    "                # print(self.generate())\n",
    "        file_path = f'./models/epoc_{self.num_epochs}_chunk_{self.chunk_len}_hiddensize_{self.hidden_size}_lr_{self.lr}__loss_{lowest_loss}.pt'\n",
    "        print(f'saving model at {file_path}')\n",
    "        torch.save(self.best_model, file_path)\n",
    "        return all_losses\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = 'chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002'\n",
    "gen = Generator() # with default parameters: chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002\n",
    "%matplotlib inline\n",
    "losses_to_plot = gen.train()\n",
    "plt.figure()\n",
    "plt.xlabel(run_params)\n",
    "plt.ylabel('loss')\n",
    "plt.text(2, 10, run_params)\n",
    "plt.plot(losses_to_plot)\n",
    "file_name = f'./images/test_run.svg'\n",
    "plt.savefig(file_name, format='svg')\n",
    "\n",
    "temperatures = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    stmt = ' '.join(gen.generate(initial_str='i would like', predict_len=150, temperature=temperature)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "    print(f'temperature: {temperature}\\nstatement:\\n{stmt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2023-05-30 00:44:13.592932>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.472861328125.pt\n",
      "[9.943482666015624, 9.926658325195312, 9.92624755859375, 9.933895263671875, 9.92290283203125, 9.911214599609375, 9.905858764648437, 9.878116455078125, 9.881482543945312, 9.876066284179688, 9.8599462890625, 9.8797509765625, 9.848983154296874, 9.799915771484375, 9.8143798828125, 9.786304931640625, 9.756159057617188, 9.754952392578126, 9.760530395507812, 9.706378173828124, 9.545305786132813, 9.50804931640625, 9.59114013671875, 9.428067626953125, 9.287386474609375, 9.223724365234375, 8.838767700195312, 9.0276513671875, 8.8260986328125, 8.51465576171875, 8.066299438476562, 8.168201293945312, 8.171292114257813, 7.836324462890625, 8.179390869140626, 8.1251513671875, 7.6127001953125, 7.486795654296875, 6.99345458984375, 7.576676025390625, 7.652713623046875, 7.406551513671875, 6.957112426757813, 7.981978759765625, 7.0873779296875, 6.472861328125, 8.335089721679687, 7.1935272216796875, 7.75873779296875, 8.59073486328125]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:44:18.143929>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.786519775390625.pt\n",
      "[9.937301025390624, 9.918948974609375, 9.8926953125, 9.877027587890625, 9.858012084960938, 9.864481201171875, 9.855930786132813, 9.754803466796876, 9.664613037109374, 9.589915161132813, 9.343755493164062, 9.253768920898438, 8.475540771484376, 8.20249267578125, 8.188663330078125, 7.920504760742188, 8.017775268554688, 7.71253173828125, 8.241986694335937, 7.828062744140625, 8.648562622070312, 7.902525024414063, 7.665857543945313, 7.231015014648437, 7.69037841796875, 8.67611328125, 7.712968139648438, 7.4102490234375, 7.478964233398438, 7.7294769287109375, 6.786519775390625, 7.927919921875, 7.560325317382812, 7.406736450195313, 7.72931884765625, 7.603521728515625, 7.538720703125, 7.142100830078125, 7.439309692382812, 7.357493896484375, 7.576553955078125, 7.861339721679688, 6.828298950195313, 7.30404052734375, 7.686650390625, 7.269759521484375, 7.56178955078125, 6.883240966796875, 6.962276000976562, 7.2362451171875]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:44:22.598929>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.7505078125.pt\n",
      "[9.913587036132812, 9.918509521484374, 9.879049072265625, 9.822523193359375, 9.718531494140626, 9.684413452148437, 9.321408081054688, 9.15348876953125, 8.373775634765625, 8.297650756835937, 7.686629028320312, 8.504766845703125, 8.508670043945312, 8.034505004882812, 8.6329541015625, 8.155443725585938, 7.75557373046875, 7.1874932861328125, 7.47220947265625, 7.222822265625, 7.333950805664062, 7.487564697265625, 8.044010620117188, 7.477979125976563, 7.432138061523437, 7.5583953857421875, 7.0853173828125, 7.304677124023438, 7.81376953125, 7.431829223632812, 7.707354125976562, 7.12736328125, 7.663123779296875, 7.998240356445312, 7.887581787109375, 6.89315185546875, 7.1173248291015625, 6.7505078125, 7.310940551757812, 7.676177978515625, 7.54701171875, 7.806285400390625, 7.522152709960937, 7.084150390625, 7.738973388671875, 7.08378662109375, 7.240700073242188, 7.109235229492188, 7.405460205078125, 7.4119775390625]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:44:27.134958>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.1964208984375.pt\n",
      "[9.944007568359375, 9.927926025390626, 9.920010375976563, 9.906721801757813, 9.907230834960938, 9.896863403320312, 9.880023803710937, 9.83495361328125, 9.811492309570312, 9.828779907226563, 9.747985229492187, 9.748882446289063, 9.67427734375, 9.633910522460937, 9.353634033203125, 9.457218017578125, 9.1458203125, 8.5544873046875, 8.846080932617188, 8.288309936523438, 8.375589599609375, 8.150170288085938, 8.312759399414062, 8.82796142578125, 7.305015869140625, 8.27019775390625, 8.729165649414062, 7.453616943359375, 7.5033447265625, 8.24729736328125, 8.990452880859374, 7.990614013671875, 7.924468383789063, 8.013927612304688, 7.799534912109375, 8.253170166015625, 7.347691040039063, 7.282803955078125, 7.483656005859375, 8.319800415039062, 7.59447509765625, 7.256480712890625, 7.767984619140625, 7.99397216796875, 7.76877685546875, 7.1964208984375, 7.29827392578125, 7.2369012451171875, 7.463707275390625, 7.372166137695313]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:44:31.777934>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.519849243164063.pt\n",
      "[9.913884887695312, 9.884700927734375, 9.87680908203125, 9.789481811523437, 9.761107788085937, 9.563375244140625, 9.33248291015625, 8.617044677734375, 8.418158569335937, 7.45139404296875, 8.111979370117188, 7.8328125, 8.613972778320312, 8.028048706054687, 7.823150024414063, 8.297955932617187, 7.131342163085938, 7.838361206054688, 7.612262573242187, 7.718070068359375, 7.355465698242187, 7.294397583007813, 7.466389770507813, 7.982970581054688, 7.232880249023437, 7.77071044921875, 7.55518798828125, 8.067401733398437, 7.844542846679688, 6.519849243164063, 8.202837524414063, 7.190687255859375, 7.460751342773437, 7.833004760742187, 8.555712890625, 7.529880981445313, 8.054234008789063, 7.8718896484375, 7.335598754882812, 7.686388549804687, 7.156971435546875, 7.6959295654296875, 7.359083251953125, 7.8597259521484375, 6.731600341796875, 6.951402587890625, 7.028451538085937, 7.527205200195312, 6.900013427734375, 7.636234130859375]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:44:36.354406>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.52460205078125.pt\n",
      "[9.929395751953125, 9.917273559570312, 9.8628271484375, 9.761791381835938, 9.296435546875, 8.845275268554687, 8.446063842773437, 7.812161865234375, 8.242559814453125, 8.082669677734375, 8.002880859375, 8.253009643554687, 7.597326049804687, 7.289522705078125, 8.333433837890626, 7.384937744140625, 7.528875122070312, 7.510362548828125, 7.874708251953125, 7.376337890625, 8.038300170898438, 7.201482543945312, 7.285955200195312, 8.979440307617187, 7.862846069335937, 7.40245361328125, 7.151314086914063, 7.396801147460938, 8.113466796875, 7.814837646484375, 7.925287475585938, 7.06500732421875, 8.101212768554687, 7.730997314453125, 7.6289013671875, 7.545653686523438, 7.877030029296875, 7.329313354492188, 7.184993896484375, 7.94062744140625, 7.696168212890625, 7.162605590820313, 7.637178955078125, 7.11343017578125, 6.52460205078125, 7.098778076171875, 6.8413037109375, 6.990042724609375, 7.909274291992188, 7.035689697265625]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:44:40.992856>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.403241577148438.pt\n",
      "[9.931577758789063, 9.922347412109374, 9.910047607421875, 9.907781982421875, 9.872794189453124, 9.8501171875, 9.817332153320313, 9.757534790039063, 9.765867919921876, 9.489466552734376, 9.419549560546875, 9.043314208984375, 8.515811157226562, 8.035331420898437, 7.952476806640625, 8.043646240234375, 7.296590576171875, 8.589105224609375, 8.387413330078125, 8.094979248046876, 7.365167236328125, 7.712864379882813, 7.866644287109375, 7.9923248291015625, 7.6361865234375, 6.969328002929688, 8.168366088867188, 7.654408569335938, 8.00912109375, 7.41232421875, 7.733159790039062, 7.434244384765625, 7.635647583007812, 7.516896362304688, 7.924446411132813, 7.628759155273437, 7.222716674804688, 7.879678344726562, 7.660179443359375, 7.3972216796875, 6.403241577148438, 7.424630737304687, 7.95679443359375, 8.859576416015624, 7.792538452148437, 7.66410400390625, 8.127296752929688, 7.3434686279296875, 8.333351440429688, 7.488072509765625]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:44:47.509010>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.621644287109375.pt\n",
      "[9.92376220703125, 9.902235717773438, 9.822645263671875, 9.83326416015625, 9.297302856445313, 8.66001220703125, 7.566679077148438, 8.067427368164063, 7.232408447265625, 7.809012451171875, 7.732666625976562, 7.200311279296875, 8.4160546875, 7.706504516601562, 8.928369140625, 7.685716552734375, 7.545780029296875, 7.84218994140625, 8.539417114257812, 8.272975463867187, 7.9343499755859375, 7.819051513671875, 7.7146539306640625, 7.263563842773437, 7.398582763671875, 7.730964965820313, 7.790914916992188, 7.6866693115234375, 7.832537231445312, 8.0881884765625, 6.78596923828125, 7.489765014648437, 7.335289306640625, 7.317054443359375, 7.313519897460938, 7.838095703125, 7.998350830078125, 7.589488525390625, 7.253510131835937, 8.20472412109375, 7.917445678710937, 7.426622314453125, 7.172457885742188, 7.75224609375, 7.35784912109375, 6.8221337890625, 7.141604614257813, 6.621644287109375, 7.4289208984375, 8.044619140625]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:44:54.296899>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.044789428710938.pt\n",
      "[9.9286669921875, 9.856010131835937, 9.651170654296875, 8.572484130859374, 8.5398388671875, 8.667050170898438, 8.767258911132812, 8.643028564453125, 7.796694946289063, 7.93192626953125, 8.313034057617188, 7.931469116210938, 7.6504248046875, 7.7066796875, 7.29512939453125, 8.143359375, 7.221361694335937, 7.1151666259765625, 7.858433837890625, 7.729046020507813, 8.287763671875, 8.532274169921875, 8.120851440429687, 7.782896728515625, 7.544574584960937, 7.916307373046875, 7.359111328125, 7.565826416015625, 8.01519775390625, 8.115233154296876, 7.044789428710938, 7.5462890625, 7.233536376953125, 7.8455712890625, 7.200750732421875, 7.597317504882812, 7.542948608398437, 8.164412841796874, 7.303826904296875, 8.003511962890625, 7.227266235351562, 7.669937744140625, 7.499716186523438, 7.606805419921875, 7.833616943359375, 7.375138549804688, 7.203314208984375, 7.378995361328125, 7.95784912109375, 7.9176092529296875]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:45:00.483842>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.636508178710938.pt\n",
      "[9.939566650390624, 9.938892211914062, 9.92592529296875, 9.908062133789063, 9.898980102539063, 9.888327026367188, 9.872698364257813, 9.86545166015625, 9.844207763671875, 9.833338012695313, 9.78321044921875, 9.764766235351562, 9.713670043945312, 9.653861694335937, 9.588521728515625, 9.514156494140625, 9.420197143554688, 9.3429931640625, 9.0163134765625, 8.871448364257812, 8.649451293945312, 8.549909057617187, 8.508963012695313, 8.482792358398438, 8.034556884765625, 7.816849975585938, 7.668824462890625, 7.585957641601563, 7.379342651367187, 7.286735229492187, 7.5188836669921875, 7.50322265625, 7.12932373046875, 7.788673706054688, 7.14894287109375, 7.70523193359375, 7.484834594726562, 7.277405395507812, 7.016705932617188, 7.625269775390625, 7.23079833984375, 7.222855834960938, 6.929178466796875, 6.9134454345703125, 6.636508178710938, 7.440255126953125, 7.24531494140625, 7.224224853515625, 7.552866821289062, 7.1226171875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:45:16.907873>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.44857421875.pt\n",
      "[9.929013061523438, 9.912359619140625, 9.879730224609375, 9.851571655273437, 9.775728759765625, 9.667634887695312, 9.545663452148437, 9.220653686523438, 8.945057373046875, 8.295872802734374, 8.085924072265625, 8.087433471679688, 7.867086791992188, 7.373602294921875, 7.253176879882813, 7.407506103515625, 7.145863037109375, 8.013745727539062, 7.871276245117188, 7.375505981445312, 7.245763549804687, 6.999876708984375, 6.999693603515625, 8.214108276367188, 6.703512573242188, 7.9023095703125, 7.369261474609375, 7.550791015625, 6.898234252929687, 7.344646606445313, 7.257868041992188, 7.400350952148438, 7.2986297607421875, 7.1519189453125, 7.273572998046875, 6.772071533203125, 6.960859985351562, 6.788790893554688, 6.708151245117188, 7.638936157226563, 7.689525756835938, 7.202818603515625, 7.4609442138671875, 7.465215454101562, 6.985010986328125, 7.58123779296875, 6.44857421875, 6.762021484375, 7.229014892578125, 6.730548095703125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:45:34.088828>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.633131103515625.pt\n",
      "[9.931104125976562, 9.888646850585937, 9.812997436523437, 9.67820068359375, 9.4470458984375, 9.028443603515624, 8.309208374023438, 7.804387817382812, 7.607374877929687, 7.763533935546875, 8.0018701171875, 7.874725341796875, 7.284110717773437, 7.50610107421875, 8.473876342773437, 8.448381958007813, 7.584603881835937, 8.011466674804687, 7.091721801757813, 7.152766723632812, 7.6130523681640625, 7.331834106445313, 7.026394653320312, 6.633131103515625, 7.799217529296875, 7.1993798828125, 7.594591064453125, 7.310491943359375, 7.174877319335938, 7.220673217773437, 7.540247802734375, 7.270687255859375, 7.272412109375, 8.059796142578126, 7.269111328125, 7.444605712890625, 7.333474731445312, 7.627601928710938, 7.305384521484375, 7.513111572265625, 7.039448852539063, 7.116847534179687, 7.066453857421875, 7.201979370117187, 7.136303100585938, 7.462560424804687, 6.77041748046875, 7.170517578125, 7.386188354492187, 6.968140869140625]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:45:51.200828>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.84454345703125.pt\n",
      "[9.92115234375, 9.914290161132813, 9.897228393554688, 9.89567138671875, 9.875714721679687, 9.85310791015625, 9.817947387695312, 9.78806396484375, 9.73478759765625, 9.680582275390625, 9.56763427734375, 9.46237548828125, 8.9527587890625, 8.676416015625, 8.631309814453125, 8.213073120117187, 7.68441162109375, 7.796495361328125, 7.949091796875, 7.3939447021484375, 8.300084228515624, 7.353414916992188, 7.29101806640625, 7.857552490234375, 7.595194091796875, 7.3721551513671875, 7.716913452148438, 6.89349853515625, 7.012225341796875, 6.8995263671875, 7.3403759765625, 6.84454345703125, 7.447488403320312, 7.16277587890625, 7.027577514648438, 7.387384643554688, 7.191563110351563, 7.111730346679687, 7.194140625, 7.548292236328125, 7.458019409179688, 7.87737060546875, 7.134071044921875, 7.2568701171875, 7.069450073242187, 7.25739990234375, 7.018446044921875, 7.495389404296875, 7.742906494140625, 7.7208319091796875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:46:07.828545>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.56479248046875.pt\n",
      "[9.914395751953125, 9.885709838867188, 9.83875, 9.729713745117188, 9.441651000976563, 8.843201904296874, 8.019120483398437, 7.718187255859375, 7.808969116210937, 8.065324096679687, 7.7821240234375, 7.828226318359375, 7.6774609375, 7.542715454101563, 7.226492919921875, 7.860152587890625, 7.5785400390625, 7.296097412109375, 7.599669189453125, 7.356392822265625, 7.276218872070313, 7.3059716796875, 7.582861328125, 7.878833618164062, 7.1527642822265625, 7.28962646484375, 7.384708862304688, 7.184097900390625, 6.858568725585937, 7.156378173828125, 7.537365112304688, 7.32074462890625, 6.687454833984375, 6.817460327148438, 7.064794921875, 7.306762084960938, 7.767677612304688, 7.45089599609375, 6.886829223632812, 7.17551025390625, 6.704635009765625, 7.010556030273437, 7.813855590820313, 6.56479248046875, 6.7255706787109375, 7.362528686523437, 7.129728393554688, 7.465833129882813, 7.041695556640625, 7.29864501953125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:46:24.362612>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.575958251953125.pt\n",
      "[9.934832763671874, 9.8728564453125, 9.717536010742187, 9.01982421875, 8.054940795898437, 7.67816162109375, 7.892818603515625, 7.716295166015625, 8.116615600585938, 7.9464599609375, 7.8718023681640625, 7.384961547851563, 8.098460693359375, 7.749071044921875, 7.652200317382812, 7.500405883789062, 7.721121215820313, 8.004617919921875, 7.476823120117188, 7.674944458007812, 7.459744262695312, 7.274989624023437, 7.599935302734375, 6.8729345703125, 7.289690551757812, 6.8923101806640625, 7.485218505859375, 6.843412475585938, 6.896635131835938, 7.750823364257813, 7.333757934570312, 6.719607543945313, 6.9570166015625, 7.606956787109375, 6.676605834960937, 7.461275024414062, 7.24738525390625, 7.08598388671875, 7.359309692382812, 6.6421441650390625, 6.942777099609375, 7.222986450195313, 7.283451538085938, 7.567815551757812, 7.0474615478515625, 6.923824462890625, 6.575958251953125, 7.651094970703125, 6.900910034179687, 6.93556396484375]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:46:40.869571>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.724110717773438.pt\n",
      "[9.928211059570312, 9.91573974609375, 9.8904541015625, 9.85742431640625, 9.835647583007812, 9.739180297851563, 9.617636108398438, 9.22721435546875, 8.71130859375, 8.252080078125, 7.654537353515625, 7.117095336914063, 7.581970825195312, 7.952396240234375, 7.624097900390625, 7.44609130859375, 6.954761962890625, 7.693994140625, 7.761163330078125, 7.477940063476563, 7.570668334960938, 7.524425048828125, 7.35724609375, 6.992166748046875, 7.382958984375, 7.2959747314453125, 7.715269775390625, 7.246268310546875, 7.526175537109375, 6.805827026367187, 7.394050903320313, 7.74281982421875, 7.441640625, 7.310457153320312, 7.6270458984375, 7.409078369140625, 7.35675537109375, 7.239859008789063, 7.132709350585937, 6.906773071289063, 7.739564208984375, 6.898770141601562, 6.8965692138671875, 7.523011474609375, 6.76228271484375, 6.724110717773438, 7.5199267578125, 7.027351684570313, 6.762144775390625, 6.7537841796875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:47:04.317851>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.80482666015625.pt\n",
      "[9.930237426757813, 9.876273803710937, 9.781867065429687, 9.10240966796875, 7.562516479492188, 8.04643310546875, 7.725479736328125, 8.21088134765625, 7.976100463867187, 7.578349609375, 7.690184936523438, 7.5661181640625, 7.824898071289063, 7.988154296875, 7.445115356445313, 7.383018798828125, 8.081865234375, 7.358103637695312, 7.166509399414062, 7.551005859375, 7.5062060546875, 7.2870849609375, 7.816337890625, 7.727346801757813, 7.210851440429687, 7.607686157226563, 7.234365844726563, 7.33843994140625, 7.229366455078125, 7.876098022460938, 6.9510565185546875, 7.006183471679687, 7.21506591796875, 7.327254028320312, 6.923673095703125, 7.12238037109375, 6.8122705078125, 6.9691943359375, 7.04275634765625, 7.105947265625, 7.567281494140625, 6.882659912109375, 7.213853149414063, 6.80482666015625, 7.227314453125, 7.144708862304688, 7.646096801757812, 6.966172485351563, 7.59866943359375, 7.252249145507813]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:47:27.634833>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.619524536132812.pt\n",
      "[9.919800415039063, 9.8378466796875, 9.115559692382812, 7.771967163085938, 7.679093017578125, 7.550662231445313, 8.438973999023437, 7.772264404296875, 7.900300903320312, 8.051201782226563, 7.758575439453125, 7.800763549804688, 7.878099975585937, 7.643359375, 7.821688842773438, 7.834382934570312, 7.766201782226562, 7.604765014648438, 7.313961791992187, 7.21800537109375, 6.893326416015625, 7.284085693359375, 7.19482666015625, 7.137077026367187, 7.250755615234375, 6.875282592773438, 7.210159912109375, 7.207575073242188, 6.979735717773438, 7.515845947265625, 6.969410400390625, 7.174580688476563, 6.851749267578125, 7.177814331054687, 7.103906860351563, 6.81467041015625, 7.222659301757813, 6.93665283203125, 6.779939575195312, 6.990253295898437, 7.690988159179687, 7.041350708007813, 6.9215478515625, 6.620968627929687, 7.225856323242187, 7.3205126953125, 7.400391235351562, 7.108257446289063, 6.619524536132812, 6.986707763671875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:47:50.724202>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.8762646484375.pt\n",
      "[9.918793131510416, 9.9060546875, 9.897884928385416, 9.888267415364583, 9.875786946614584, 9.868384602864584, 9.858311360677083, 9.827079264322917, 9.800732421875, 9.798895670572916, 9.74712158203125, 9.7006884765625, 9.67121337890625, 9.6876025390625, 9.542718912760417, 9.406465657552083, 9.242273763020833, 9.10630859375, 9.016626790364583, 8.920721842447916, 8.62687744140625, 8.574610188802083, 8.3502783203125, 8.144117024739584, 8.09208740234375, 8.05126220703125, 7.51707275390625, 7.359612630208333, 7.3923974609375, 7.094724934895833, 6.9551009114583335, 6.990691731770833, 7.001783040364583, 7.003123372395834, 7.023230794270833, 6.967405598958333, 7.303373209635416, 7.012847493489583, 7.102826334635417, 7.02862060546875, 7.471356608072917, 7.1683968098958335, 6.8762646484375, 7.5216943359375, 7.48032470703125, 7.07087158203125, 7.454392903645833, 7.58990234375, 7.422627766927083, 7.545914713541666]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:48:14.983231>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.792912190755208.pt\n",
      "[9.93513427734375, 9.905609537760416, 9.88203125, 9.80983154296875, 9.747892252604167, 9.605152994791666, 9.4018212890625, 8.882003580729167, 8.572688802083333, 8.12310302734375, 7.666459147135416, 7.783582356770833, 7.478641764322917, 7.4751025390625, 7.341759440104167, 7.368118489583333, 8.987708333333334, 8.328446451822916, 7.626607259114583, 7.426507161458333, 7.93823486328125, 7.727060546875, 7.13884033203125, 7.457945149739583, 7.09107421875, 7.648700358072917, 7.281561686197917, 7.026986490885417, 6.792912190755208, 7.41880859375, 7.391644694010417, 7.068800455729167, 7.169026692708333, 7.77055908203125, 7.177679036458334, 6.969491373697917, 7.2247265625, 7.051031087239584, 6.825169677734375, 7.306969401041667, 7.42215087890625, 6.91521240234375, 7.055150553385417, 6.995773111979167, 7.698317057291667, 7.266123046875, 7.3069091796875, 6.9985563151041665, 7.42093017578125, 7.1922021484375]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:48:39.874202>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.805678304036459.pt\n",
      "[9.929109700520833, 9.88946533203125, 9.836852213541667, 9.719867350260417, 9.487967936197917, 9.08130126953125, 8.457544759114583, 7.933030598958333, 7.554906412760417, 7.716414388020834, 7.543673502604166, 7.971487630208333, 7.7926611328125, 8.058704427083333, 7.817111002604166, 7.445817057291666, 7.63837646484375, 8.103936360677084, 7.178604329427083, 7.527085774739583, 7.0783463541666665, 7.483485514322917, 7.33397705078125, 7.295821940104167, 7.290350748697917, 7.145933430989583, 7.078076985677083, 7.571380208333333, 7.43514892578125, 7.120072428385416, 7.040638020833334, 6.878287760416667, 6.805678304036459, 7.178541666666667, 7.2893212890625, 7.209012858072916, 7.008055013020833, 7.2651749674479165, 6.929400227864583, 7.018335774739583, 7.057161458333334, 7.28626953125, 7.010812174479167, 6.938905436197917, 6.997034505208333, 7.040226236979167, 7.18690185546875, 6.92773681640625, 7.4672216796875, 7.2633837890625]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:49:05.130202>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.779195149739583.pt\n",
      "[9.919091796875, 9.896207682291667, 9.886417643229167, 9.876513671875, 9.84115234375, 9.814483235677084, 9.790149739583333, 9.6995068359375, 9.65203369140625, 9.475620930989583, 9.204432779947917, 8.974364420572917, 8.66082275390625, 8.23240966796875, 7.8372607421875, 7.738738606770833, 7.481800130208334, 7.333167317708333, 7.293612467447916, 7.7877734375, 6.90737548828125, 7.562689615885417, 6.833299967447917, 6.779195149739583, 7.364381510416667, 7.34343994140625, 7.1410123697916665, 6.9797762044270835, 7.663068033854167, 6.968995768229167, 7.5765380859375, 6.7997123209635415, 7.244187825520833, 7.096114095052084, 6.956490885416667, 7.019773763020833, 7.2319921875, 6.9069287109375, 7.22326171875, 7.06891845703125, 6.9167472330729165, 7.3910904947916665, 6.88525390625, 7.163782552083333, 6.9084464518229165, 7.39760986328125, 7.1626513671875, 7.143743489583334, 6.928636881510417, 7.4510009765625]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:49:29.877632>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.6100065104166665.pt\n",
      "[9.92887451171875, 9.8927099609375, 9.845442708333334, 9.722301432291667, 9.365740559895833, 8.873960774739583, 7.762096354166666, 7.043567708333334, 7.2115478515625, 8.368728841145833, 7.968173828125, 7.6766015625, 7.273792317708334, 7.697555338541667, 7.20427978515625, 7.5634521484375, 7.566927083333334, 7.627765299479167, 7.230228678385417, 7.528168131510417, 7.262311197916667, 6.856935221354167, 7.318606770833333, 6.966356608072917, 7.3966276041666665, 7.2591455078125, 7.68057373046875, 7.998776041666667, 7.493404134114583, 7.2074365234375, 6.6100065104166665, 7.5222900390625, 6.996128743489583, 7.06960205078125, 7.019046223958333, 6.711551920572917, 7.128799641927083, 6.823446044921875, 7.10664794921875, 6.8672216796875, 7.12314697265625, 7.210078125, 7.285879720052083, 7.411785481770833, 6.966734212239583, 6.96566162109375, 7.390989583333333, 7.24234619140625, 7.148565266927084, 7.515777994791667]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:49:54.616885>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.542119547526042.pt\n",
      "[9.929972330729166, 9.876251627604166, 9.686678873697916, 9.11079345703125, 7.975908203125, 7.826886393229167, 7.675829264322917, 7.70333740234375, 7.98607177734375, 7.64318603515625, 7.54792236328125, 7.301565755208333, 7.536162923177083, 7.1482861328125, 7.2571044921875, 7.085497233072917, 7.48535888671875, 7.304185384114583, 6.929886881510416, 7.324364420572917, 7.501461588541667, 7.08921630859375, 7.134341634114583, 7.121587727864584, 7.0908154296875, 7.425421549479167, 6.9539599609375, 7.513546549479167, 7.00695556640625, 7.097357584635417, 7.1710498046875, 7.075079752604167, 6.542119547526042, 6.77405517578125, 7.221681315104167, 7.497743326822917, 7.14357177734375, 7.2787247721354165, 6.874384765625, 7.0933056640625, 7.06028076171875, 6.960696614583333, 7.4787255859375, 7.017377115885417, 6.699789632161458, 6.70319091796875, 7.1572607421875, 7.494576009114583, 6.834264322916667, 6.983143717447916]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:50:19.365851>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.4677685546875.pt\n",
      "[9.928560384114583, 9.905595703125, 9.880559895833333, 9.846952311197917, 9.76273193359375, 9.672217610677084, 9.39426025390625, 8.968668619791666, 8.314076334635416, 7.910664876302083, 7.822015787760416, 7.284210611979167, 7.520533854166667, 6.795834554036459, 7.368499348958333, 7.061800944010416, 7.7545564778645835, 7.160511881510416, 7.749312337239584, 7.518653157552083, 7.6771858723958335, 6.891533203125, 7.07641357421875, 6.92114501953125, 7.57666015625, 7.335455729166667, 7.526521809895834, 7.6135481770833335, 7.28419921875, 6.723025716145833, 7.355913899739583, 7.033810221354167, 6.946175130208333, 7.566563313802083, 7.075485026041667, 6.4677685546875, 7.054834798177083, 7.485829264322916, 7.053754069010417, 6.819075113932292, 7.831424153645833, 6.641667073567708, 7.087320963541667, 6.6084077962239585, 7.310957845052084, 7.11802734375, 6.999493001302083, 6.932607421875, 6.919022623697916, 6.716429443359375]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:50:54.105980>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.666438802083333.pt\n",
      "[9.9252392578125, 9.878614095052084, 9.72462890625, 8.915371907552084, 7.7116259765625, 7.449313151041666, 7.675419921875, 8.24888671875, 7.9032666015625, 8.05944580078125, 7.725712076822917, 7.46276123046875, 7.587881673177083, 7.536265462239584, 7.577096354166667, 7.370087076822917, 7.535947265625, 7.129134114583334, 7.068853352864584, 7.499292805989583, 7.92248046875, 7.138898111979167, 7.878645833333334, 7.521044921875, 7.481426595052083, 7.5477294921875, 7.189296875, 7.304265950520834, 7.116171875, 7.307789713541666, 6.684982096354167, 7.145982259114583, 7.4527783203125, 6.913221028645833, 6.76164306640625, 7.31293212890625, 6.8748681640625, 6.748160400390625, 7.01051025390625, 7.365392252604167, 6.959925944010417, 6.848072102864584, 6.933369954427083, 7.4508251953125, 6.767701416015625, 6.776350504557292, 7.07248291015625, 7.44650390625, 6.90643310546875, 6.666438802083333]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:51:28.643639>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.24446044921875.pt\n",
      "[9.921825358072917, 9.79642333984375, 8.497845865885417, 8.03120361328125, 8.102779947916666, 7.99631591796875, 7.615702311197917, 8.180345865885416, 7.943356119791667, 7.6942236328125, 7.806666666666667, 7.696033528645834, 7.78858154296875, 7.589482421875, 7.254945475260417, 7.52317138671875, 7.450342610677083, 7.4545515950520835, 6.794762369791667, 6.893136393229167, 7.06502685546875, 7.12854736328125, 7.568597819010416, 7.31221923828125, 7.435979817708334, 6.873819986979167, 7.21493896484375, 7.358711751302083, 7.051419270833334, 6.967860514322917, 7.192202962239583, 6.7139208984375, 7.4625537109375, 6.648329671223959, 6.793031819661459, 7.122649739583333, 6.996058756510417, 7.0842887369791665, 6.298468831380208, 6.9241259765625, 6.916663411458333, 7.525115559895833, 6.8811669921875, 7.326366373697916, 6.939945475260417, 6.913994140625, 6.24446044921875, 7.17027587890625, 6.705375162760417, 6.732926025390625]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:52:03.921613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.854296264648437.pt\n",
      "[9.925198364257813, 9.90041015625, 9.91776123046875, 9.90821044921875, 9.884589233398437, 9.871092529296876, 9.902367553710938, 9.880740966796875, 9.85173828125, 9.861487426757812, 9.858958740234375, 9.848919677734376, 9.849364013671876, 9.781943969726562, 9.751168212890626, 9.713211669921876, 9.713439331054687, 9.756272583007812, 9.64383056640625, 9.552979736328124, 9.600259399414062, 9.468697509765626, 9.356620483398437, 9.269459838867187, 9.279790649414062, 8.681190185546875, 8.817086791992187, 8.72522705078125, 8.6568408203125, 8.419208984375, 8.455081787109375, 8.011920166015624, 8.17377685546875, 7.925317993164063, 7.609312744140625, 7.572398681640625, 7.430169677734375, 7.588444213867188, 7.550143432617188, 7.2635986328125, 7.06824951171875, 7.485546875, 7.698030395507812, 7.514658203125, 7.887615966796875, 7.2508612060546875, 6.854296264648437, 7.714326171875, 8.030817260742188, 8.105707397460938]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:52:12.421645>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.9939501953125.pt\n",
      "[9.91231689453125, 9.90131103515625, 9.853886108398438, 9.85542724609375, 9.790213012695313, 9.835039672851563, 9.704688720703125, 9.621909790039062, 9.322171630859375, 9.066483154296876, 8.806246337890625, 8.223685302734374, 8.417684936523438, 8.177545776367188, 8.038075561523437, 8.256859130859375, 8.576442260742187, 6.9939501953125, 7.803175048828125, 9.04173828125, 7.978941040039063, 8.003457641601562, 8.069757690429688, 8.180811767578126, 8.33118408203125, 7.763558959960937, 8.612010498046875, 7.565965576171875, 7.512160034179687, 7.9396826171875, 7.74244140625, 7.604663696289062, 7.9192437744140625, 7.38657470703125, 7.380848388671875, 7.659714965820313, 7.956062622070313, 7.675577392578125, 7.29028564453125, 7.390250854492187, 7.541123657226563, 7.181787109375, 8.841537475585938, 7.521694946289062, 7.39001220703125, 7.943636474609375, 7.692239990234375, 7.713920288085937, 7.396946411132813, 8.196587524414063]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:52:21.181614>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.5012841796875.pt\n",
      "[9.92546630859375, 9.909041748046874, 9.874207153320313, 9.812271728515626, 9.72490234375, 9.652350463867187, 9.387587890625, 8.705287475585937, 8.5103759765625, 8.7266455078125, 8.404277954101563, 8.052151489257813, 8.401810302734376, 7.936607666015625, 7.925787353515625, 7.908278198242187, 8.60453369140625, 7.578604125976563, 8.253506469726563, 8.0591162109375, 8.006885986328125, 7.7394873046875, 7.671010131835938, 7.449456787109375, 7.488223266601563, 8.268331298828125, 7.306428833007812, 7.718394775390625, 7.577078857421875, 7.230952758789062, 7.422151489257812, 6.77629150390625, 7.694736938476563, 6.5012841796875, 7.521858520507813, 7.430530395507812, 7.529096069335938, 6.65782958984375, 6.96180419921875, 6.751959228515625, 7.1740264892578125, 7.295977172851562, 8.164146118164062, 7.42220458984375, 7.6470947265625, 7.2075567626953125, 7.520127563476563, 6.786934204101563, 7.462865600585937, 8.107390747070312]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:52:29.931612>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.677745361328125.pt\n",
      "[9.9245703125, 9.928508911132813, 9.905953979492187, 9.903033447265624, 9.906017456054688, 9.879056396484375, 9.873070068359375, 9.852477416992187, 9.8480517578125, 9.824021606445312, 9.732311401367188, 9.7822412109375, 9.664959716796876, 9.599501953125, 9.556524047851562, 9.285410766601563, 9.29722412109375, 9.038050537109376, 8.795695190429688, 7.914591674804687, 8.534996337890625, 7.358421630859375, 8.0892529296875, 8.155036010742187, 8.152549438476562, 8.034912719726563, 8.143323364257812, 7.909723510742188, 8.273780517578125, 7.61748046875, 7.39338623046875, 7.156362915039063, 7.323924560546875, 8.21461181640625, 7.674427490234375, 6.971150512695313, 7.220211181640625, 7.433838500976562, 7.048116455078125, 6.95087890625, 7.3111724853515625, 7.953084716796875, 7.49166748046875, 7.319884643554688, 7.945887451171875, 7.275545654296875, 7.874244995117188, 6.677745361328125, 7.34779052734375, 7.541539916992187]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:52:38.715616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.937131958007813.pt\n",
      "[9.937056274414063, 9.920565795898437, 9.899053955078125, 9.857774047851562, 9.778604736328125, 9.663323974609375, 9.340573120117188, 8.87938720703125, 9.05431396484375, 7.96021484375, 8.722236938476563, 8.630712890625, 7.19062255859375, 7.98093994140625, 7.931492919921875, 8.159010620117188, 7.816318969726563, 7.3259130859375, 7.038798828125, 7.816181030273437, 7.320986328125, 8.104647827148437, 7.642445678710938, 7.281991577148437, 8.1062109375, 7.108314208984375, 8.108533325195312, 7.28888427734375, 7.65676025390625, 7.179658203125, 7.3839959716796875, 7.420682373046875, 6.937131958007813, 7.3530426025390625, 7.27745361328125, 7.027320556640625, 8.04573974609375, 7.532224731445313, 7.53533447265625, 7.527691650390625, 6.947220458984375, 7.802725219726563, 8.51913818359375, 8.100968017578126, 7.921295166015625, 7.025729370117188, 8.138812866210937, 7.769681396484375, 7.60427001953125, 7.564413452148438]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:52:47.345617>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.938782348632812.pt\n",
      "[9.932735595703125, 9.890078125, 9.853162231445312, 9.585817260742187, 9.044094848632813, 8.08639892578125, 8.383952026367188, 8.3572900390625, 8.1666455078125, 7.9275341796875, 7.984994506835937, 8.204322509765625, 7.885621948242187, 8.368822631835938, 7.79700439453125, 7.68472900390625, 7.659483642578125, 8.076534423828125, 7.854113159179687, 8.46178955078125, 7.948722534179687, 7.328243408203125, 7.85558837890625, 7.364306640625, 7.520899047851563, 7.87348876953125, 7.439407958984375, 7.846407470703125, 7.356473388671875, 8.25985107421875, 7.188358764648438, 7.5720977783203125, 7.248485107421875, 7.730213623046875, 7.201742553710938, 8.19425048828125, 6.938782348632812, 7.922821044921875, 7.4476220703125, 7.3564825439453125, 7.5799951171875, 7.974814453125, 7.76060546875, 7.46158447265625, 8.11682373046875, 7.159999389648437, 7.196817016601562, 7.155399169921875, 7.698363037109375, 7.2656689453125]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:52:55.895615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.960010375976562.pt\n",
      "[9.93810546875, 9.915908813476562, 9.902238159179687, 9.882297973632813, 9.841649780273437, 9.8473681640625, 9.823589477539063, 9.71490966796875, 9.638935546875, 9.484288940429687, 9.223460693359375, 9.046009521484375, 8.027042236328125, 7.5155078125, 8.645031127929688, 7.4339447021484375, 8.043234252929688, 8.805029296875, 7.426521606445313, 8.429192504882813, 7.434630126953125, 7.742105102539062, 7.669010620117188, 8.32546142578125, 8.176129760742187, 8.145969848632813, 7.933441162109375, 7.545953979492188, 8.010103759765625, 7.886853637695313, 6.960010375976562, 7.257485961914062, 7.518385009765625, 7.642050170898438, 7.827700805664063, 7.5236578369140625, 7.076322021484375, 7.75574462890625, 7.0530078125, 7.760747680664062, 7.75896728515625, 8.103662109375, 7.636909790039063, 7.210130615234375, 7.620814819335937, 7.504625244140625, 7.744609375, 7.028748168945312, 7.416983642578125, 7.5655059814453125]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:53:06.148618>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.724761962890625.pt\n",
      "[9.930199584960938, 9.89773193359375, 9.836229858398438, 9.6122216796875, 9.032474365234375, 8.237572631835938, 8.639015502929688, 7.787227783203125, 7.770316772460937, 8.633295288085938, 8.155162963867188, 7.671657104492187, 8.214573974609374, 8.220201416015625, 7.891754760742187, 6.724761962890625, 8.394468383789063, 7.738129272460937, 8.125127563476562, 8.377867431640626, 6.9256884765625, 8.880867919921876, 7.351134643554688, 7.75532958984375, 8.0542529296875, 8.306898193359375, 8.091483764648437, 7.473901977539063, 7.819605102539063, 7.642005615234375, 7.273031005859375, 7.732020263671875, 7.727374267578125, 7.295350341796875, 6.984100341796875, 7.667796020507812, 8.146649169921876, 7.5424432373046875, 7.518671875, 7.243179931640625, 7.591776123046875, 7.380542602539062, 7.12979736328125, 6.977293090820313, 7.497923583984375, 7.274252319335938, 7.5976123046875, 7.45304931640625, 7.2885894775390625, 6.957206420898437]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:53:16.381616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.888925170898437.pt\n",
      "[9.913790893554687, 9.88090087890625, 9.714663696289062, 8.728297119140626, 9.055611572265626, 7.411436767578125, 8.075948486328125, 8.367110595703124, 8.656781616210937, 8.503875732421875, 7.930340576171875, 7.958583984375, 7.763902587890625, 7.605928955078125, 7.621845703125, 8.337492065429688, 8.089226684570313, 7.512574462890625, 7.987388916015625, 7.362689208984375, 7.921204223632812, 7.99787353515625, 7.50576171875, 8.144442749023437, 7.7976312255859375, 7.711639404296875, 7.489034423828125, 7.4314453125, 7.760474853515625, 7.25645751953125, 7.890519409179688, 7.4398870849609375, 7.724578857421875, 8.207685546875, 7.93414794921875, 7.366359252929687, 7.767782592773438, 7.88363037109375, 7.611417846679688, 7.11078369140625, 7.527811889648437, 7.110916748046875, 7.228192749023438, 7.232114868164063, 7.696732177734375, 7.2289276123046875, 6.888925170898437, 7.625818481445313, 8.077069702148437, 7.413211059570313]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:53:26.399614>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.9006280517578125.pt\n",
      "[9.926396484375, 9.920238037109375, 9.911580200195312, 9.905891723632813, 9.898806762695312, 9.891629028320313, 9.8895166015625, 9.872776489257813, 9.849983520507813, 9.80463623046875, 9.821250610351562, 9.80975341796875, 9.753916625976563, 9.759766235351563, 9.709669189453125, 9.66351318359375, 9.607297973632813, 9.544749145507813, 9.378013305664062, 9.336630859375, 9.1135009765625, 8.973277587890625, 8.8749072265625, 8.854392700195312, 8.410709838867188, 8.251890258789063, 7.808552856445313, 7.8167352294921875, 7.897325439453125, 7.729412231445313, 6.931669311523438, 7.285414428710937, 6.9006280517578125, 7.111508178710937, 7.2208935546875, 7.138872680664062, 7.067704467773438, 7.140556640625, 7.2521826171875, 7.52865234375, 7.437467041015625, 6.97651123046875, 7.16275146484375, 7.601167602539062, 7.294853515625, 7.319373779296875, 7.44908935546875, 7.413486328125, 7.510631713867188, 6.923360595703125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:53:59.835618>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.43937744140625.pt\n",
      "[9.917733764648437, 9.897982177734375, 9.877893676757813, 9.829475708007813, 9.77652587890625, 9.708458251953125, 9.51519287109375, 9.130355834960938, 8.825963745117187, 8.716951904296875, 7.988799438476563, 8.432302856445313, 8.099808349609376, 7.2441064453125, 7.462032470703125, 7.3156976318359375, 8.019435424804687, 7.811417236328125, 7.930289306640625, 6.43937744140625, 7.727239990234375, 7.7047412109375, 7.097345581054688, 7.6783642578125, 7.4884710693359375, 7.543557739257812, 7.22284912109375, 7.077623901367187, 7.070521850585937, 7.547486572265625, 7.353035278320313, 7.506316528320313, 7.142272338867188, 7.488136596679688, 6.946534423828125, 7.360054931640625, 7.724315795898438, 7.4155712890625, 7.34381103515625, 7.236790161132813, 7.026534423828125, 7.167340698242188, 6.925230102539063, 7.220097045898438, 7.444657592773438, 7.081251220703125, 6.92775146484375, 6.57355224609375, 7.402587890625, 7.563250732421875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:54:33.181645>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.763204956054688.pt\n",
      "[9.914583740234375, 9.876016235351562, 9.807998657226562, 9.670048217773438, 9.303999633789063, 9.072614135742187, 8.338811645507812, 7.692810668945312, 7.841123046875, 7.90816162109375, 7.9312109375, 7.095926513671875, 7.726982421875, 7.603817138671875, 7.616167602539062, 7.7179937744140625, 7.095152587890625, 7.897100219726562, 7.557303466796875, 7.0647125244140625, 6.763204956054688, 7.446353759765625, 7.70645263671875, 7.266261596679687, 7.01368408203125, 7.076663818359375, 7.44240234375, 7.4921533203125, 7.779230346679688, 7.319623413085938, 7.060724487304688, 7.228380126953125, 7.0504913330078125, 7.3110009765625, 7.9220208740234375, 7.254689331054688, 7.003343505859375, 7.1109814453125, 7.131469116210938, 6.840169677734375, 7.492189331054687, 7.157298583984375, 6.886402587890625, 7.191512451171875, 7.251594848632813, 7.5375244140625, 6.816265258789063, 7.168516845703125, 6.986505737304688, 7.6566534423828125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:55:05.849616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.627369995117188.pt\n",
      "[9.9192333984375, 9.914373168945312, 9.899974365234375, 9.883771362304687, 9.872633056640625, 9.84825927734375, 9.828215942382812, 9.777415161132813, 9.731503295898438, 9.6728955078125, 9.54453369140625, 9.287808227539063, 8.8829541015625, 8.869986572265624, 8.614705200195312, 8.454616088867187, 7.478443603515625, 7.509247436523437, 7.209298706054687, 7.37384521484375, 7.4418792724609375, 7.494489135742188, 7.320048828125, 7.983699340820312, 7.871939697265625, 7.544854736328125, 7.4492236328125, 7.563375244140625, 7.265255737304687, 7.539843139648437, 7.2635565185546875, 7.637676391601563, 7.597049560546875, 7.465698852539062, 7.180427856445313, 7.135176391601562, 6.7098486328125, 7.286510620117188, 7.380820922851562, 6.627369995117188, 7.080564575195313, 6.820886840820313, 7.039907836914063, 7.523065795898438, 7.184862060546875, 6.754111938476562, 7.188670654296875, 6.738556518554687, 7.231290893554688, 7.596621704101563]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:55:38.839614>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.878875732421875.pt\n",
      "[9.9278369140625, 9.892962646484374, 9.842098388671875, 9.738170776367188, 9.394388427734375, 8.947338256835938, 8.1653564453125, 7.510637817382812, 7.594033203125, 8.514066162109375, 8.109244995117187, 8.243733520507812, 7.2526513671875, 7.72358154296875, 7.477740478515625, 7.396906127929688, 7.09792724609375, 7.639270629882812, 7.56908203125, 7.6245849609375, 7.283681640625, 7.472578125, 7.331286010742187, 7.55470458984375, 7.260355224609375, 7.544828491210938, 7.201483154296875, 7.365848999023438, 6.878875732421875, 7.561905517578125, 7.191644287109375, 7.1955499267578125, 7.673275146484375, 7.489453125, 7.228453369140625, 7.1873583984375, 7.151826171875, 7.334208374023437, 6.99476806640625, 7.153924560546875, 7.117326049804688, 7.462254028320313, 6.98852783203125, 7.1162347412109375, 7.03080322265625, 7.308370361328125, 6.974376831054688, 7.754349365234375, 7.319487915039063, 7.15080810546875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:56:12.029619>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.5477685546875.pt\n",
      "[9.924322509765625, 9.863661499023438, 9.713046875, 9.009346313476563, 8.038265380859375, 8.22330810546875, 7.525599975585937, 8.544710083007812, 7.700448608398437, 7.676290893554688, 7.175051879882813, 7.369348754882813, 7.944915771484375, 7.472320556640625, 7.738226318359375, 7.7104345703125, 7.286998901367188, 7.163696899414062, 7.681927490234375, 7.118407592773438, 7.498116455078125, 7.130030517578125, 7.495236206054687, 7.3903656005859375, 7.826514892578125, 6.966231689453125, 7.185671997070313, 7.142879028320312, 7.502656860351562, 6.794246215820312, 6.7966650390625, 7.328922119140625, 7.047598876953125, 7.207611694335937, 7.712081298828125, 7.070667724609375, 7.038311767578125, 7.451974487304687, 7.297842407226563, 6.8519677734375, 7.078848876953125, 6.628541870117187, 7.209583740234375, 6.729993286132813, 7.3938360595703125, 6.853554077148438, 6.777412109375, 7.06869140625, 6.5477685546875, 6.665618286132813]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:56:44.763616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.543818359375.pt\n",
      "[9.9349853515625, 9.908818969726562, 9.8920361328125, 9.852389526367187, 9.812698974609376, 9.764081420898437, 9.618975219726563, 9.257454223632813, 8.885861206054688, 8.148142700195313, 7.638688354492188, 7.181669311523438, 7.384281616210938, 7.38067138671875, 8.462388305664062, 7.794317016601562, 7.80576416015625, 8.235595092773437, 8.183323364257813, 6.817363891601563, 7.15515869140625, 7.525348510742187, 7.58712646484375, 7.244193725585937, 7.4066796875, 6.6119580078125, 7.702017822265625, 7.562557983398437, 7.217275390625, 7.434999389648437, 7.179889526367187, 7.257554321289063, 6.765052490234375, 7.056213989257812, 7.065541381835938, 7.558897094726563, 7.451400146484375, 6.6018609619140625, 7.426663818359375, 6.911864013671875, 7.429283447265625, 6.581092529296875, 7.389276123046875, 7.072361450195313, 6.543818359375, 7.0057763671875, 7.156790161132813, 7.139285888671875, 7.280979614257813, 7.783350830078125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:57:22.949615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.759365234375.pt\n",
      "[9.932130126953124, 9.890875854492187, 9.743784790039063, 9.214351196289062, 8.00033935546875, 8.054595947265625, 8.2822021484375, 7.77196044921875, 7.257551879882812, 7.325621337890625, 7.60685546875, 7.610112915039062, 7.59281494140625, 7.7268524169921875, 7.881549072265625, 7.2521820068359375, 7.093794555664062, 7.700679321289062, 8.175726928710937, 7.447202758789063, 7.7302783203125, 7.2238525390625, 7.357705688476562, 7.706525268554688, 7.276681518554687, 7.28403564453125, 6.867850952148437, 7.2656793212890625, 7.80071533203125, 8.092229614257812, 7.122383422851563, 7.140655517578125, 7.098738403320312, 7.305001220703125, 7.07546142578125, 7.37789306640625, 7.3472607421875, 7.18110595703125, 7.384884033203125, 7.392835693359375, 7.353511962890625, 7.2946234130859375, 7.20075439453125, 6.955484619140625, 6.786766357421875, 6.981725463867187, 7.212724609375, 6.759365234375, 7.543247680664063, 7.03094970703125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 00:58:01.344616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.2833203125.pt\n",
      "[9.918921508789062, 9.840553588867188, 9.086736450195312, 8.099061279296874, 7.88882080078125, 7.363024291992187, 7.656810913085938, 7.340545654296875, 7.545556640625, 7.383927612304688, 7.345040893554687, 7.553070678710937, 7.51150390625, 7.4024505615234375, 7.7198858642578125, 7.2972869873046875, 7.548824462890625, 6.9649169921875, 7.092500610351562, 7.287120361328125, 6.93752685546875, 8.121285400390626, 7.508920288085937, 7.365858764648437, 7.320630493164063, 7.208670043945313, 7.1077294921875, 6.77477294921875, 7.289804077148437, 6.775012817382812, 8.427784423828125, 7.0829705810546875, 7.07847412109375, 7.165662231445313, 6.88146728515625, 7.1747894287109375, 7.0713037109375, 6.992818603515625, 7.1146783447265625, 6.89599609375, 7.19824951171875, 7.157044677734375, 6.5120782470703125, 6.905405883789062, 6.81540771484375, 7.0396484375, 7.338345336914062, 6.5542144775390625, 6.292814331054688, 6.2833203125]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 00:58:39.540618>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.802109375.pt\n",
      "[9.9265380859375, 9.917909342447917, 9.908575846354166, 9.906056315104166, 9.882950846354166, 9.886900227864583, 9.879879557291666, 9.858797200520833, 9.831451822916666, 9.797083333333333, 9.79423828125, 9.776090494791667, 9.710069173177084, 9.680782877604166, 9.622630208333334, 9.552236328125, 9.45142822265625, 9.305162760416666, 9.1852734375, 9.0988427734375, 8.874790852864583, 8.76868896484375, 8.46752197265625, 8.28931640625, 8.033084309895834, 8.000088704427084, 7.627547200520834, 7.741707356770833, 7.348159993489583, 7.451547037760417, 7.066031087239583, 6.802109375, 7.215123697916667, 7.163323567708333, 6.998321126302083, 7.446968587239583, 7.28900634765625, 7.36058837890625, 7.048601888020833, 7.34156982421875, 6.841105143229167, 7.166148274739584, 6.958616536458333, 7.33139892578125, 7.248711751302083, 7.390570475260417, 7.3273527018229165, 7.351028645833333, 6.85306640625, 7.2739046223958335]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 00:59:28.172617>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.3627022298177085.pt\n",
      "[9.929536946614583, 9.903312174479167, 9.879391276041666, 9.83417236328125, 9.776580403645834, 9.658289388020833, 9.4984521484375, 9.00359619140625, 8.7922119140625, 8.117889811197916, 7.91552001953125, 8.091639811197917, 7.652916666666667, 7.300506998697917, 7.116409505208333, 7.730360514322917, 7.803052571614583, 7.429007975260417, 7.607035319010417, 7.318550618489583, 7.085774739583333, 7.156375325520833, 7.597445475260416, 7.43289794921875, 7.77737060546875, 7.002173665364583, 7.5522802734375, 7.128444010416667, 7.013384602864583, 7.284306640625, 7.678563639322917, 7.578201497395833, 6.9246598307291665, 6.3627022298177085, 7.53228515625, 7.121214192708333, 7.323447265625, 6.9515625, 6.768180338541667, 7.1927734375, 7.244111328125, 7.384768880208333, 7.462041829427084, 7.4147721354166665, 6.996043294270834, 6.868016764322917, 7.0896044921875, 7.355445149739583, 6.7503873697916665, 7.117704264322916]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:00:17.905613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.6705839029947915.pt\n",
      "[9.9296240234375, 9.901993815104166, 9.835821126302083, 9.707215983072917, 9.403282877604166, 8.983458658854167, 8.3796728515625, 7.54935302734375, 7.379026692708333, 7.564600423177083, 7.6113134765625, 7.58421875, 7.750401204427083, 7.750109049479167, 7.7322900390625, 7.86743408203125, 7.474497884114584, 7.73312255859375, 7.183501790364583, 7.443465169270834, 7.633740234375, 7.236094563802084, 7.40720458984375, 6.837466634114583, 7.09045654296875, 7.0796875, 6.919917805989583, 7.2791943359375, 7.232416178385416, 7.23198486328125, 7.197156575520833, 7.45515625, 7.54971923828125, 7.2218229166666665, 7.445267740885416, 7.042158203125, 7.191089680989584, 7.202284342447917, 7.269795735677083, 7.234510091145833, 7.139031575520833, 6.8951513671875, 6.8578271484375, 6.973704427083334, 6.858910319010417, 7.157478841145833, 6.786941731770833, 6.6705839029947915, 7.385614420572916, 6.782947998046875]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:01:08.137615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.635176595052084.pt\n",
      "[9.9243115234375, 9.91000244140625, 9.897059733072917, 9.875913899739583, 9.870524088541666, 9.829112955729167, 9.813753255208333, 9.743440755208333, 9.6632177734375, 9.547920735677083, 9.407451171875, 9.1175927734375, 8.75278076171875, 8.256920572916666, 8.000001627604167, 8.020391438802083, 7.827938639322917, 7.318182779947917, 6.994208170572917, 7.5862255859375, 7.296724446614584, 6.635176595052084, 7.452024739583333, 7.213118489583334, 7.25247314453125, 6.835618489583333, 7.952189127604167, 7.363072102864583, 7.5548779296875, 7.250994466145833, 7.562314453125, 7.7165380859375, 7.131360677083333, 7.329532877604167, 7.219014485677083, 7.015211588541667, 6.978409830729166, 6.736782633463542, 7.454861653645834, 7.30002197265625, 7.488811848958333, 6.918392740885417, 7.356175130208333, 7.649701334635417, 6.765774739583334, 6.867229817708333, 7.03032958984375, 6.8554475911458335, 6.993619791666666, 6.778025716145834]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:01:57.992613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.658440348307292.pt\n",
      "[9.928187662760417, 9.89423095703125, 9.822864583333333, 9.709536946614584, 9.3004638671875, 8.462808430989583, 7.929683430989583, 7.679414876302084, 7.4060595703125, 8.138519694010416, 7.86842529296875, 7.696407877604167, 7.335492350260417, 7.387250162760417, 7.62173583984375, 7.212478841145833, 7.335615234375, 7.615467936197916, 7.095994466145833, 7.600916341145833, 7.495672200520834, 7.173026529947917, 7.278793131510417, 7.5130419921875, 7.323189290364583, 7.194322916666667, 7.18885986328125, 7.753435872395833, 7.455730794270833, 6.96247802734375, 7.379668782552083, 7.4964827473958335, 7.157705078125, 7.464496256510417, 7.271006673177084, 7.010718587239583, 7.121806640625, 6.897196451822917, 6.818677978515625, 7.094302571614583, 6.926731770833333, 7.532459309895834, 7.251632486979167, 7.032893880208333, 6.967889811197916, 7.394397786458334, 7.04466064453125, 6.658440348307292, 7.260042317708334, 7.072476399739584]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:02:47.704613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.6565169270833335.pt\n",
      "[9.931035970052083, 9.871287434895834, 9.751353352864584, 8.955634765625, 8.04214111328125, 8.062257486979167, 7.622737630208333, 7.960860188802084, 7.748507486979166, 7.450027669270833, 7.238859049479166, 7.422265625, 7.500359700520833, 7.236057942708333, 7.228787434895834, 7.457071940104167, 7.505437825520834, 7.0212646484375, 7.652626953125, 7.376383463541667, 7.682355143229167, 7.079253743489583, 7.026415201822917, 7.155704752604167, 7.234236653645834, 6.87517822265625, 7.066743977864584, 7.324398600260417, 7.357730305989583, 7.111355794270834, 7.631034342447917, 7.066552734375, 7.055450032552083, 7.22294921875, 6.969464518229167, 6.954642740885417, 6.6565169270833335, 7.5194970703125, 6.785396728515625, 6.751306966145833, 7.157097981770833, 7.192945149739583, 7.200369466145833, 7.000337727864584, 7.1497900390625, 7.456756998697917, 6.848656412760417, 7.060352376302084, 7.187802734375, 6.820920817057291]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:03:37.915617>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.583174641927084.pt\n",
      "[9.918665364583333, 9.895602213541666, 9.876863606770833, 9.845587565104166, 9.7707421875, 9.646561686197916, 9.372969563802084, 9.286617024739583, 8.49931640625, 7.7883251953125, 7.5242521158854165, 7.851996256510417, 7.607945149739583, 7.12997802734375, 7.55261474609375, 7.162501627604167, 7.390431315104166, 7.432305501302083, 7.062693684895834, 7.0821630859375, 7.368525390625, 7.306909993489583, 7.343138020833333, 7.039117838541666, 7.385263671875, 7.50710693359375, 7.395594889322917, 7.221228841145833, 7.308406575520833, 7.346729329427084, 7.552510579427083, 6.761769612630208, 7.4199462890625, 7.23954345703125, 7.266407063802084, 7.1343701171875, 7.780774739583333, 7.333529459635416, 7.27636474609375, 6.960431315104167, 6.941786295572917, 7.30322265625, 7.09861572265625, 6.583174641927084, 7.294501139322917, 6.671121826171875, 6.8714591471354165, 7.24444580078125, 6.8902376302083335, 7.193090006510417]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:04:35.230615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.38935546875.pt\n",
      "[9.920582682291666, 9.866106770833333, 9.658745930989584, 8.616819661458333, 7.813693033854166, 7.706180013020833, 7.881607259114583, 7.33895751953125, 7.544967447916667, 7.605328776041667, 7.42681884765625, 7.081553548177084, 7.3503759765625, 7.702300618489583, 7.6483162434895835, 7.33272216796875, 7.076207682291667, 8.028213704427083, 7.104903157552084, 7.404786783854167, 6.960874837239583, 7.3226529947916665, 7.1287613932291665, 7.297936197916667, 7.13359619140625, 7.36375, 7.404895833333334, 7.19905517578125, 7.0537353515625, 7.00182861328125, 7.151572265625, 7.748002115885416, 7.016598307291667, 7.0820654296875, 7.401439615885416, 6.965069986979167, 6.847403971354167, 7.27271728515625, 6.850088704427083, 7.383626302083333, 7.3474365234375, 6.815702718098958, 7.496746419270833, 6.9812630208333335, 6.92028076171875, 6.43569091796875, 6.38935546875, 7.2010107421875, 6.84816650390625, 7.171078287760417]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:05:31.823616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.209806315104166.pt\n",
      "[9.922870279947917, 9.831142578125, 8.78335205078125, 7.865226236979167, 7.750851236979167, 8.09980712890625, 7.9531640625, 7.48281494140625, 7.602220865885417, 7.503238118489583, 7.510220540364584, 7.704823404947916, 7.675320638020834, 7.28808349609375, 7.318536783854166, 7.31219482421875, 7.0785750325520835, 7.363650716145833, 7.1691682942708335, 7.44185302734375, 7.157340494791667, 6.9898583984375, 7.613997395833334, 7.290973307291667, 7.55580810546875, 6.88944091796875, 7.230819498697917, 7.463851725260417, 6.79384765625, 6.669676106770833, 7.291004231770834, 7.040673014322917, 6.7475288899739585, 6.562657470703125, 7.017439778645834, 7.255460611979166, 6.8595703125, 6.598155110677084, 7.175371907552083, 6.209806315104166, 6.949029947916666, 7.296233723958333, 6.4181901041666665, 6.867255045572917, 6.871366373697916, 6.880068359375, 6.905992024739583, 6.836399739583333, 7.015906575520833, 6.813271891276042]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:06:28.378618>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.671253051757812.pt\n",
      "[9.933036499023437, 9.909927368164062, 9.913346557617187, 9.911803588867187, 9.887064819335938, 9.899900512695313, 9.898155517578125, 9.878339233398437, 9.88959228515625, 9.865474243164062, 9.86466064453125, 9.841216430664062, 9.8411474609375, 9.808375244140626, 9.842823486328125, 9.74208984375, 9.766588134765625, 9.728590087890625, 9.720427856445312, 9.695444946289063, 9.59701416015625, 9.438787841796875, 9.5844140625, 9.272095947265624, 9.133255004882812, 9.357755126953125, 9.29286376953125, 9.257835693359375, 8.646821899414062, 8.556787109375, 8.567507934570312, 8.318587646484374, 8.030708618164063, 7.717686767578125, 7.663948974609375, 8.080276489257812, 7.405661010742188, 7.511396484375, 8.292987670898437, 7.58421630859375, 7.73441650390625, 7.800965576171875, 8.4584375, 7.6501171875, 7.2780322265625, 6.671253051757812, 7.072133178710938, 7.724678955078125, 8.2807275390625, 7.455931396484375]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:06:41.749613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.812799072265625.pt\n",
      "[9.92333984375, 9.910166015625, 9.879200439453125, 9.858297729492188, 9.8573828125, 9.799816284179688, 9.710885620117187, 9.659766235351562, 9.6048779296875, 9.39076171875, 8.95089599609375, 9.021962280273437, 8.230953979492188, 8.142576293945313, 7.8054437255859375, 7.6097900390625, 7.9464569091796875, 7.264269409179687, 7.664771728515625, 8.28516357421875, 8.586203002929688, 8.310477905273437, 8.419024658203124, 7.3069921875, 7.499895629882812, 7.639828491210937, 7.5259912109375, 6.812799072265625, 8.487027587890625, 7.380409545898438, 7.150850219726562, 7.471180419921875, 8.18696533203125, 7.292440795898438, 8.051951904296875, 7.956925659179688, 7.375156860351563, 7.712337646484375, 7.42249755859375, 7.61961181640625, 7.837565307617187, 7.770563354492188, 7.084656982421875, 8.009854125976563, 7.769334716796875, 7.120648803710938, 7.276492919921875, 7.8278173828125, 7.188106689453125, 7.206680908203125]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:06:54.838616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.778849487304687.pt\n",
      "[9.936185302734375, 9.9097021484375, 9.856055908203125, 9.858978881835938, 9.769696044921876, 9.592612915039062, 9.1341552734375, 8.5161865234375, 8.575009155273438, 8.602069091796874, 7.119649658203125, 8.356548461914063, 7.70329833984375, 7.792369995117188, 7.867396850585937, 8.7002392578125, 7.8543359375, 7.46521484375, 8.035877075195312, 7.179188842773438, 7.578065795898437, 8.4043212890625, 8.482255859375, 7.68677001953125, 7.9525689697265625, 7.962904052734375, 7.451704711914062, 7.371359252929688, 7.690243530273437, 7.981995239257812, 7.471817626953125, 7.816907348632813, 8.069848022460938, 7.580487060546875, 7.698805541992187, 8.398800048828125, 7.514718017578125, 6.977537231445313, 7.213525390625, 7.773554077148438, 7.25858154296875, 7.729673461914063, 7.557189331054688, 7.098804321289062, 6.778849487304687, 7.606699829101562, 7.090062866210937, 7.713221435546875, 7.545536499023438, 7.473447875976563]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:07:07.908643>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.904946899414062.pt\n",
      "[9.920473022460937, 9.915538330078125, 9.90519287109375, 9.889462280273438, 9.895485229492188, 9.872632446289062, 9.846040649414062, 9.857616577148438, 9.835385131835938, 9.80303955078125, 9.761900634765626, 9.763797607421875, 9.700673217773437, 9.526953125, 9.224393310546875, 9.0419384765625, 9.044647216796875, 8.708202514648438, 8.189695434570313, 8.48285400390625, 7.948452758789062, 7.856049194335937, 7.699404296875, 7.79210205078125, 7.302274169921875, 7.344129028320313, 7.895992431640625, 8.354859008789063, 7.079734497070312, 6.904946899414062, 7.630084838867187, 7.609661254882813, 7.863894653320313, 8.211102294921876, 7.276367797851562, 7.857075805664063, 7.413228759765625, 7.27039794921875, 7.19849365234375, 7.118123779296875, 7.364452514648438, 7.878107299804688, 7.789801025390625, 7.761763916015625, 6.999119873046875, 7.205139770507812, 7.193001708984375, 7.179248657226562, 7.5776953125, 8.290726928710937]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:07:21.611613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.857619018554687.pt\n",
      "[9.9324365234375, 9.907681274414063, 9.85843505859375, 9.836370849609375, 9.7099365234375, 9.560732421875, 8.931732177734375, 9.141058349609375, 8.205272827148438, 7.708189697265625, 9.147418212890624, 8.257112426757812, 8.4961279296875, 8.382457885742188, 7.688081665039062, 7.372041625976562, 8.372408447265625, 8.358758544921875, 7.681068725585938, 6.9912786865234375, 7.613890380859375, 7.5219970703125, 7.408074951171875, 8.484229736328125, 7.57294677734375, 7.897440185546875, 7.866044921875, 7.94837890625, 7.204782104492187, 7.15159912109375, 7.643421630859375, 7.285701293945312, 7.032448120117188, 6.857619018554687, 7.55188232421875, 7.176676635742187, 7.024849853515625, 7.701456909179687, 7.854801025390625, 7.876527709960937, 7.28147216796875, 7.293534545898438, 7.41107666015625, 7.623452758789062, 8.553092651367187, 7.287364501953125, 7.854365234375, 7.336903076171875, 7.653426513671875, 7.420924682617187]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:07:34.806616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.791304931640625.pt\n",
      "[9.923504638671876, 9.898681640625, 9.826317749023438, 9.6041015625, 9.489600830078125, 8.829232177734376, 8.546643676757812, 8.162245483398438, 7.29850830078125, 8.015724487304688, 8.156195068359375, 8.4777685546875, 8.142012939453124, 8.333525390625, 8.278681030273438, 7.875864868164062, 7.749328002929688, 7.624246215820312, 7.927903442382813, 7.845783081054687, 8.369654541015626, 7.839801025390625, 8.0731689453125, 7.198849487304687, 8.430801391601562, 7.82644775390625, 8.247600708007813, 7.230577392578125, 7.710413818359375, 6.934266967773437, 7.991689453125, 7.938294677734375, 8.001227416992187, 7.580863647460937, 7.20514892578125, 7.656071166992188, 7.805466918945313, 7.874943237304688, 7.5596551513671875, 7.64006591796875, 7.39884521484375, 7.544564208984375, 6.791304931640625, 7.40224853515625, 7.2372509765625, 7.910861206054688, 6.847735595703125, 7.672921752929687, 7.819363403320312, 7.044443969726562]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:07:47.899613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.508845825195312.pt\n",
      "[9.933643798828125, 9.901946411132812, 9.895108642578125, 9.899122314453125, 9.893071899414062, 9.841011962890626, 9.773311157226562, 9.732412719726563, 9.640023193359376, 9.371076049804687, 9.204623413085937, 8.976754150390626, 8.180599365234375, 8.492474975585937, 7.560236206054688, 7.396870727539063, 8.301104125976563, 8.446978149414063, 8.294527587890625, 7.579716796875, 7.72432861328125, 7.615978393554688, 7.518871459960938, 8.1397998046875, 7.187518920898437, 8.276253051757813, 7.290357666015625, 7.26721435546875, 7.72150634765625, 8.24407470703125, 7.202565307617188, 7.658871459960937, 7.059490356445313, 7.941371459960937, 6.939656982421875, 8.062671508789062, 7.347440795898438, 7.216992797851563, 6.508845825195312, 7.527683715820313, 7.064691772460938, 7.526364135742187, 7.028311157226563, 7.530913696289063, 7.704283447265625, 8.264134521484374, 7.007203369140625, 7.081207275390625, 7.874908447265625, 7.365316162109375]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:08:02.532645>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.578992309570313.pt\n",
      "[9.924707641601563, 9.901399536132812, 9.834896850585938, 9.751958618164062, 9.003280029296874, 8.82826171875, 8.411257934570312, 7.42826171875, 8.533804321289063, 8.454036254882812, 8.782229614257812, 7.89783203125, 7.791408081054687, 7.929645385742187, 8.285807495117188, 8.040972290039063, 7.481359252929687, 7.535191040039063, 8.065911865234375, 7.92008544921875, 7.049884033203125, 7.806441040039062, 7.3089794921875, 8.051635131835937, 7.274825439453125, 7.653612060546875, 6.715006713867187, 8.7385009765625, 7.40100341796875, 7.321578369140625, 8.680136108398438, 7.750111083984375, 7.593741455078125, 6.858985595703125, 7.407281494140625, 8.1753076171875, 7.621784057617187, 7.823389282226563, 7.741832275390625, 7.386278076171875, 7.476834106445312, 8.576017456054688, 7.912718505859375, 7.685391845703125, 7.903619995117188, 7.348912353515625, 7.521214599609375, 7.262772216796875, 6.970479736328125, 6.578992309570313]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:08:17.302616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.6041021728515625.pt\n",
      "[9.920772094726562, 9.859973754882812, 9.651797485351562, 8.88398681640625, 8.732557373046875, 8.89559326171875, 8.0741650390625, 8.328444213867188, 8.09917236328125, 7.871998901367188, 7.915885009765625, 8.2366455078125, 8.208565063476563, 7.70529296875, 8.06455078125, 7.590757446289063, 7.965357055664063, 7.794212646484375, 7.62298095703125, 7.142899169921875, 7.216846313476562, 7.374827880859375, 7.64729248046875, 7.589846801757813, 8.185823364257812, 7.057021484375, 7.67183349609375, 7.902669067382813, 7.056035766601562, 6.765645141601563, 7.351121215820313, 7.530428466796875, 7.7260009765625, 7.564076538085938, 7.4035107421875, 6.8020458984375, 7.004805297851562, 7.715446166992187, 7.395418090820312, 7.179917602539063, 7.0133294677734375, 7.564842529296875, 7.2696905517578125, 7.259954833984375, 7.771709594726563, 6.6041021728515625, 7.712264404296875, 8.281599731445313, 6.78505126953125, 7.537767944335937]\n",
      "epochs: 50\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:08:31.832613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.9368829345703125.pt\n",
      "[9.94585205078125, 9.928658447265626, 9.918989868164063, 9.902008056640625, 9.896906127929688, 9.869534301757813, 9.882207641601562, 9.85541259765625, 9.847393798828126, 9.825792846679688, 9.813034057617188, 9.765977783203125, 9.795880737304687, 9.718589477539062, 9.633305053710938, 9.600435791015625, 9.538486938476563, 9.343363647460938, 9.28643798828125, 9.004624633789062, 9.049607543945312, 8.841715087890625, 8.49937255859375, 8.258298950195313, 8.12416259765625, 8.421615600585938, 7.935546875, 7.60998291015625, 7.357960205078125, 7.673455810546875, 7.79889892578125, 7.3058935546875, 7.211200561523437, 6.969238891601562, 7.196458129882813, 7.465706176757813, 7.2883642578125, 7.376055297851562, 7.253016967773437, 7.1383447265625, 7.65458251953125, 7.42008544921875, 7.370298461914063, 6.9368829345703125, 7.610048828125, 7.774345703125, 7.224932250976562, 7.2435369873046875, 7.345721435546875, 7.799634399414063]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:09:22.212642>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.666004028320312.pt\n",
      "[9.92231201171875, 9.90427978515625, 9.869561157226563, 9.854440307617187, 9.790177612304687, 9.646380004882813, 9.484334106445312, 9.30233642578125, 8.793165893554688, 8.428023071289063, 8.102993774414063, 7.74794189453125, 7.597454223632813, 7.351478271484375, 8.620660400390625, 7.953646240234375, 8.001680297851562, 7.391176147460937, 7.412437744140625, 7.479296875, 7.410106811523438, 7.8504541015625, 7.582559204101562, 7.027486572265625, 7.38597900390625, 7.968244018554688, 7.309315185546875, 7.034208984375, 6.814068603515625, 7.85018798828125, 7.159200439453125, 7.3705859375, 7.2735223388671875, 7.274390869140625, 7.2957763671875, 7.283071899414063, 6.987452392578125, 7.413789672851562, 7.558387451171875, 7.119354248046875, 7.025458984375, 7.105784301757812, 7.09612060546875, 6.666004028320312, 7.2617254638671875, 6.94010986328125, 7.35719482421875, 7.808590698242187, 7.555361328125, 6.799598388671875]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:10:12.759618>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.67155517578125.pt\n",
      "[9.928480834960938, 9.888490600585937, 9.822841796875, 9.703779907226563, 9.453280029296875, 9.094475708007813, 8.4429345703125, 8.015023193359376, 7.746753540039062, 7.779715576171875, 7.443556518554687, 8.0003271484375, 8.44688232421875, 7.682838745117188, 8.053030395507813, 7.257460327148437, 7.385963745117188, 7.589049072265625, 7.66508056640625, 6.817213745117187, 7.147716064453125, 7.593720092773437, 7.205094604492188, 7.521903686523437, 7.467109375, 7.0481939697265625, 7.152880859375, 7.57281982421875, 7.546417846679687, 6.8535205078125, 7.258001708984375, 7.349564819335938, 7.43912353515625, 7.29609375, 7.082052612304688, 7.039005737304688, 7.19928466796875, 6.851473999023438, 7.18749267578125, 7.233638305664062, 7.083240356445312, 7.4291925048828125, 6.7327099609375, 7.0078192138671875, 6.751287231445312, 6.67155517578125, 7.2406378173828125, 6.754032592773438, 6.989066162109375, 6.84894287109375]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:11:03.336615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.6543426513671875.pt\n",
      "[9.922657470703125, 9.916715087890625, 9.895366821289063, 9.886824951171874, 9.880270385742188, 9.85048583984375, 9.8118896484375, 9.786318359375, 9.7129931640625, 9.609998168945312, 9.536318969726562, 9.410663452148437, 9.09686767578125, 8.707569580078125, 8.314593505859374, 8.2959521484375, 7.966605224609375, 7.7580126953125, 7.76229736328125, 7.90618896484375, 7.443314208984375, 7.000233764648438, 7.221629638671875, 7.8185498046875, 7.7299267578125, 7.45107177734375, 7.6136083984375, 7.3064337158203125, 7.428348999023438, 7.0578155517578125, 7.4466094970703125, 7.617605590820313, 7.193526000976562, 7.364937133789063, 7.432767333984375, 6.787415161132812, 7.556961669921875, 7.249763793945313, 7.097272338867188, 7.217455444335937, 7.303199462890625, 6.947201538085937, 7.248809204101563, 7.175513916015625, 6.83008056640625, 7.112691040039063, 7.2009228515625, 7.172442626953125, 6.6543426513671875, 7.119236450195313]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:11:54.606617>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.722516479492188.pt\n",
      "[9.930443115234375, 9.902820434570312, 9.856211547851563, 9.772735595703125, 9.45724853515625, 8.831264038085937, 8.181044311523438, 7.694304809570313, 7.86542236328125, 7.844892578125, 8.420228271484374, 7.826633911132813, 8.6216845703125, 7.581982421875, 7.38238525390625, 7.16196533203125, 7.579234619140625, 7.485231323242187, 7.432169799804687, 7.87274658203125, 7.675350341796875, 7.706644287109375, 8.476412963867187, 7.624879150390625, 7.37829345703125, 7.387626953125, 7.422942504882813, 7.306298828125, 6.9511175537109375, 7.1156787109375, 7.383543701171875, 7.426441650390625, 6.722516479492188, 7.298037109375, 7.251050415039063, 6.838181762695313, 6.828511352539063, 7.209229736328125, 7.35066650390625, 7.401278686523438, 7.321994018554688, 7.312020263671875, 7.162763061523438, 6.895881958007813, 6.80923583984375, 7.1646875, 7.25937255859375, 7.061683349609375, 7.21870361328125, 6.815341186523438]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:12:45.977612>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.6613330078125.pt\n",
      "[9.9192431640625, 9.86439697265625, 9.701071166992188, 9.114208374023438, 8.216912231445313, 8.09735595703125, 7.730761108398437, 8.391941528320313, 8.216300659179687, 8.049221801757813, 7.304140014648437, 7.444099731445313, 7.614234619140625, 7.65857421875, 7.308484497070313, 7.624398803710937, 7.272064819335937, 7.82779296875, 7.737976684570312, 7.50779296875, 7.610006103515625, 7.896204223632813, 7.263707275390625, 7.00576416015625, 7.291917724609375, 7.4173681640625, 7.239557495117188, 7.182879638671875, 7.35389404296875, 6.6613330078125, 6.994343872070313, 7.477042846679687, 7.138076171875, 7.441640625, 7.222767944335938, 7.40952880859375, 7.390319213867188, 7.075804443359375, 7.29953125, 7.258754272460937, 7.068994750976563, 7.00357666015625, 6.992938232421875, 7.185559692382813, 6.819154663085937, 7.114279174804688, 7.232745971679687, 7.0872125244140625, 6.878192749023437, 7.07357666015625]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:13:37.839612>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.6690521240234375.pt\n",
      "[9.924049072265625, 9.893563842773437, 9.885698852539063, 9.837337646484375, 9.813626098632813, 9.712462158203126, 9.603804931640624, 9.31420654296875, 8.584583129882812, 8.220029296875, 7.7081591796875, 7.963695678710938, 7.755620727539062, 7.140562744140625, 7.913402099609375, 7.671712036132813, 8.143799438476563, 7.615111694335938, 7.109353637695312, 7.796508178710938, 7.381526489257812, 7.384722900390625, 7.678751220703125, 7.532658081054688, 7.06139404296875, 7.26244140625, 7.5308642578125, 7.007076416015625, 7.239744873046875, 7.779924926757812, 7.515321044921875, 7.5048291015625, 6.809190673828125, 6.937997436523437, 7.529050903320313, 7.068349609375, 7.027977294921875, 7.209730834960937, 7.6348974609375, 7.211637573242188, 6.6690521240234375, 7.057841796875, 7.340510864257812, 7.166903076171875, 7.13377197265625, 7.100444946289063, 6.894238891601563, 6.825625, 7.513964233398437, 7.472811889648438]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:14:34.266613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.7348870849609375.pt\n",
      "[9.919354858398437, 9.868487548828124, 9.640621948242188, 8.746151123046875, 7.6639337158203125, 8.095848388671875, 7.934985961914062, 7.201741333007813, 8.004867553710938, 7.697774047851563, 7.139950561523437, 7.2294873046875, 7.556155395507813, 7.769366455078125, 7.562468872070313, 8.095665893554688, 7.626170654296875, 7.393870239257812, 7.9306103515625, 7.34175048828125, 7.679620361328125, 7.526068115234375, 7.689930419921875, 7.155692749023437, 7.5962646484375, 6.882388305664063, 7.08205810546875, 7.31890625, 7.592201538085938, 7.163668212890625, 7.322186279296875, 7.72406005859375, 7.193367309570313, 7.197649536132812, 7.0899658203125, 6.8311767578125, 6.9532177734375, 7.509142456054687, 7.175990600585937, 7.754301147460938, 7.280390014648438, 7.1753564453125, 7.207197875976562, 7.100618896484375, 7.159314575195313, 6.9754266357421875, 6.7348870849609375, 6.948714599609375, 7.17680908203125, 6.787500610351563]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:15:30.290615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.363018188476563.pt\n",
      "[9.923928833007812, 9.838414306640624, 9.030822143554687, 7.966317138671875, 8.06571533203125, 7.963659057617187, 7.768163452148437, 7.9031134033203125, 7.614104614257813, 7.344960327148438, 7.424998779296875, 7.586549682617187, 7.530979614257813, 7.10739990234375, 7.18817138671875, 7.312385864257813, 7.192396240234375, 7.6426708984375, 7.092335205078125, 7.6189599609375, 7.183699340820312, 7.505410766601562, 7.217653198242187, 7.49564453125, 6.723536987304687, 7.779892578125, 6.82774658203125, 6.877593383789063, 7.003546142578125, 7.029114990234375, 6.981378173828125, 7.239332885742187, 6.749194946289062, 6.99680419921875, 7.153495483398437, 7.547747192382812, 7.331729736328125, 7.247349853515625, 6.880768432617187, 7.175711059570313, 7.053336181640625, 6.5395269775390625, 7.2077392578125, 6.363018188476563, 6.83531982421875, 6.8021630859375, 6.550881958007812, 6.858023071289063, 6.787467651367187, 6.812507934570313]\n",
      "epochs: 50\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:16:26.754615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.703563232421875.pt\n",
      "[9.926197916666666, 9.9295166015625, 9.906917317708333, 9.90095703125, 9.896110026041667, 9.883900553385416, 9.877394205729166, 9.860904947916667, 9.839510904947916, 9.827091471354167, 9.8054736328125, 9.747103678385416, 9.7234228515625, 9.647599283854166, 9.572622884114583, 9.508201497395833, 9.412879231770834, 9.221142578125, 9.0612109375, 8.874132486979166, 8.604666341145833, 8.338358561197916, 8.435884602864583, 8.07150390625, 7.920340983072917, 7.831806640625, 7.6292952473958335, 7.66389892578125, 7.508642578125, 7.1042041015625, 7.340686848958334, 7.1772233072916665, 6.970209147135416, 7.352246907552083, 7.2557381184895835, 7.5239208984375, 6.703563232421875, 7.658667805989583, 7.3581754557291665, 7.150860188802083, 7.4724951171875, 7.516543782552083, 8.126385904947917, 7.010675455729166, 7.02746337890625, 7.275947265625, 7.1730574544270835, 6.958938802083333, 7.691853841145833, 6.84861083984375]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:17:41.663615>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.624700520833334.pt\n",
      "[9.92461669921875, 9.898648274739584, 9.862373046875, 9.8311767578125, 9.780319010416667, 9.614606119791667, 9.365350748697917, 9.051991373697916, 8.652792154947917, 8.134112141927083, 8.199545084635417, 7.532122395833333, 7.465839029947917, 7.52773193359375, 7.204784342447916, 7.769547526041666, 6.936651204427084, 7.7210595703125, 8.174620768229167, 7.490142415364583, 7.32673828125, 8.044200846354167, 6.90196044921875, 7.334056803385416, 7.71287841796875, 7.6103662109375, 7.18667724609375, 7.180164388020834, 7.5549308268229165, 7.042549641927083, 7.196568196614583, 7.24798828125, 7.028653157552084, 7.417490234375, 7.374137369791667, 7.078158365885416, 7.251420084635416, 7.326971842447917, 6.822947184244792, 6.852857259114583, 7.2505875651041665, 7.274032389322917, 7.3965771484375, 7.13940185546875, 7.139393717447916, 6.624700520833334, 7.394453938802084, 7.117687174479166, 7.01738037109375, 7.211690266927083]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:18:57.634616>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.617191162109375.pt\n",
      "[9.92125, 9.896123860677083, 9.822984212239584, 9.682830403645834, 9.39085693359375, 8.874569498697916, 8.266895345052083, 7.995098470052083, 7.756127115885417, 7.2911759440104165, 7.453875325520833, 7.734365234375, 7.349984537760417, 7.394300944010417, 7.5018318684895835, 7.758230794270833, 8.006392415364584, 6.956611328125, 7.102157389322917, 6.941715494791667, 7.064970703125, 7.685310872395833, 7.842478841145834, 7.513097330729167, 6.782889404296875, 7.212090657552083, 6.617191162109375, 7.4359749348958335, 7.595201009114583, 7.458035481770834, 7.29902099609375, 7.021468098958334, 7.74985595703125, 7.336455891927083, 6.819724527994792, 7.001700846354167, 7.13554931640625, 7.367145182291667, 7.58945556640625, 7.08200927734375, 7.4467154947916665, 6.7432080078125, 7.272486979166667, 7.005030110677083, 7.263479817708333, 7.31871826171875, 7.4556892903645835, 7.164576009114583, 6.764967447916667, 7.299996744791667]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:20:13.822613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.564609781901042.pt\n",
      "[9.927233072916666, 9.915760091145833, 9.90203125, 9.894002278645834, 9.856121419270833, 9.831256510416667, 9.81836181640625, 9.735992838541666, 9.651756184895833, 9.599678548177083, 9.295071614583334, 9.09753662109375, 8.872124837239584, 8.44786376953125, 8.156016438802084, 7.774038899739583, 7.382504069010417, 7.46824951171875, 7.06019287109375, 7.30976318359375, 8.159700520833333, 7.06282470703125, 8.122569986979167, 7.3350390625, 7.6770751953125, 7.26312744140625, 7.419813639322917, 7.212389322916667, 7.431656087239583, 6.564609781901042, 7.06802734375, 7.115433756510416, 7.1885994466145835, 7.86833251953125, 7.4951416015625, 7.351293131510417, 7.249701334635417, 7.042061360677083, 7.468597005208333, 7.217257486979166, 7.307802734375, 6.837208658854166, 7.00622802734375, 6.881630859375, 6.90250244140625, 6.8480810546875, 7.1253955078125, 7.108798828125, 6.903103841145834, 7.2714453125]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:21:30.553643>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.556359456380208.pt\n",
      "[9.920655924479167, 9.888856608072917, 9.827660319010416, 9.71700439453125, 9.416101888020833, 8.72730712890625, 8.24157958984375, 7.72496826171875, 7.8946875, 7.679935709635417, 7.77142578125, 7.966216634114583, 7.450869140625, 7.74613037109375, 7.539983723958334, 7.821844889322917, 7.21951904296875, 6.854816080729167, 7.571295572916667, 7.19574462890625, 7.08263671875, 7.2465283203125, 7.130721842447917, 7.387921549479167, 7.24828857421875, 7.6154777018229165, 6.693749186197917, 7.3432470703125, 6.937811686197917, 7.195172526041667, 7.21628173828125, 6.8207080078125, 7.3466552734375, 7.129693196614583, 7.144536946614584, 7.014634602864583, 7.46491455078125, 6.9791471354166665, 6.981559244791667, 7.5763427734375, 7.4560107421875, 6.962193196614583, 6.847201334635416, 7.129545084635416, 6.887757975260417, 6.8071366373697915, 7.04870361328125, 7.037945149739583, 6.751788736979167, 6.556359456380208]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:22:46.153617>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.241472981770833.pt\n",
      "[9.9209375, 9.84312744140625, 9.62089111328125, 8.891969401041667, 7.319165852864583, 8.40186279296875, 7.980313313802084, 7.710296223958333, 7.815187174479167, 7.740210774739583, 7.94085693359375, 6.677325846354167, 7.430255533854167, 8.026396484375, 7.622338053385417, 7.217125651041667, 7.21420654296875, 7.176219075520834, 7.300345865885417, 7.81893798828125, 7.607838541666666, 7.334697265625, 7.203362630208333, 6.9777099609375, 7.4330216471354165, 7.179214680989583, 6.241472981770833, 7.1208097330729165, 7.5198982747395835, 7.262383626302083, 7.434991861979166, 7.212154134114583, 7.2767545572916665, 6.704028727213542, 7.182692057291667, 6.934462890625, 7.3766015625, 7.169816080729166, 6.813463134765625, 6.9703157552083335, 7.21069091796875, 7.05406005859375, 6.7385567220052085, 6.9691845703125, 6.724982096354167, 7.146756998697916, 7.02093994140625, 7.1022347005208335, 6.82821533203125, 6.947769368489583]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:24:02.996613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.777093912760416.pt\n",
      "[9.91995361328125, 9.907230631510417, 9.87108154296875, 9.842317708333333, 9.753460286458333, 9.593740234375, 9.25109619140625, 8.959108072916667, 8.098017578125, 7.6091259765625, 7.330147298177083, 7.4731437174479165, 7.258970540364583, 7.37780029296875, 7.276825358072917, 7.68548095703125, 7.664546712239583, 7.68068359375, 7.790654296875, 7.419524739583333, 6.777093912760416, 7.365126139322917, 7.433468424479167, 7.4415380859375, 7.101852213541667, 6.97596923828125, 7.307146809895833, 7.61196044921875, 7.122805989583333, 7.237225748697917, 7.049769694010417, 6.918960774739583, 7.1134375, 7.4144140625, 7.52969482421875, 7.434034830729167, 7.36881103515625, 7.02094970703125, 7.51719482421875, 7.039058430989583, 7.085485026041667, 6.78483154296875, 7.2815169270833335, 7.09671875, 6.974256184895833, 7.320282389322917, 7.151883951822916, 7.276097819010417, 7.383955078125, 7.026279296875]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:25:24.308114>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.190061848958333.pt\n",
      "[9.927217610677083, 9.843248697916666, 9.620376790364583, 8.638580729166666, 7.7596053059895835, 8.0405810546875, 7.891590983072916, 7.3924348958333335, 7.485521647135417, 7.193955078125, 7.316167805989584, 7.78101806640625, 7.5792635091145835, 7.388260091145833, 7.149458821614584, 7.816813151041667, 7.282699381510417, 7.420985514322917, 6.8701171875, 7.190201009114583, 6.85665283203125, 7.568926595052083, 7.440672200520833, 7.018475748697917, 7.107109375, 7.111339518229166, 6.814195963541667, 7.031258951822917, 7.521432291666667, 7.540086263020833, 6.863006184895833, 7.037958984375, 7.41721435546875, 7.73964111328125, 6.951758626302083, 6.896875, 6.409885660807292, 6.190061848958333, 7.558151041666667, 6.700999348958334, 7.392576497395833, 6.801181640625, 6.999444173177083, 7.018614095052083, 7.01244873046875, 6.926500651041667, 6.9614599609375, 7.247994791666667, 7.19064453125, 6.95427001953125]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:26:42.676185>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.49263916015625.pt\n",
      "[9.929077962239583, 9.808133138020834, 8.84292236328125, 7.7682877604166665, 7.955785319010417, 7.543577473958333, 7.719702962239583, 7.81391845703125, 7.60268798828125, 7.7271240234375, 7.37134521484375, 7.325838216145834, 7.291695963541667, 7.872776692708333, 7.444965006510417, 7.267198893229167, 7.425316569010417, 6.823517252604167, 7.738638509114583, 7.24155029296875, 6.73680419921875, 7.021879069010416, 7.01486328125, 7.57761474609375, 7.580013020833333, 7.08359130859375, 7.397368977864583, 7.5611865234375, 7.325101725260416, 7.402705078125, 7.196927897135416, 6.938900553385417, 7.19739990234375, 7.259150390625, 7.0830289713541665, 7.1409326171875, 7.2510009765625, 6.767823486328125, 7.4334944661458335, 7.063052571614583, 6.87994384765625, 7.249276529947917, 7.090887858072917, 6.970906575520833, 6.859456380208333, 6.795994466145833, 6.49263916015625, 6.899595540364583, 7.0215625, 6.810570475260417]\n",
      "epochs: 50\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:28:00.923523>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_9.15114501953125.pt\n",
      "[9.92666015625, 9.928207397460938, 9.91458251953125, 9.9130615234375, 9.899160766601563, 9.892000122070312, 9.884759521484375, 9.881613159179688, 9.874698486328125, 9.87623291015625, 9.853936767578125, 9.846849975585938, 9.830807495117188, 9.830661010742187, 9.78374755859375, 9.809724731445312, 9.762723999023438, 9.802304077148438, 9.688397827148437, 9.710344848632813, 9.69565185546875, 9.552095947265625, 9.392507934570313, 9.3155126953125, 9.15114501953125]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:28:03.122073>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.969157104492187.pt\n",
      "[9.920230712890625, 9.918922729492188, 9.870574340820312, 9.863728637695312, 9.854080810546876, 9.785885009765625, 9.750720825195312, 9.584937133789062, 9.524800415039062, 9.20817626953125, 8.806417236328125, 8.394727783203125, 8.152275390625, 8.251094970703125, 6.993316040039063, 8.131179809570312, 7.684161376953125, 7.274430541992188, 8.21709716796875, 8.657796020507812, 8.666566162109374, 6.969157104492187, 7.57373046875, 7.338387451171875, 7.749700927734375]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:28:05.497070>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.203150024414063.pt\n",
      "[9.9237548828125, 9.903870239257813, 9.84850830078125, 9.780234375, 9.708662109375, 9.462804565429687, 8.915914306640625, 8.464172973632813, 8.771787109375, 8.205842895507812, 7.343946533203125, 8.369912719726562, 7.703611450195313, 7.800791015625, 8.31946533203125, 8.635761108398437, 7.958744506835938, 7.424290771484375, 7.8195953369140625, 7.3106317138671875, 7.482008056640625, 7.920023803710937, 7.203150024414063, 7.822985229492187, 7.7133111572265625]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:28:07.910947>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.494476928710937.pt\n",
      "[9.914689331054687, 9.91737060546875, 9.9223193359375, 9.909658813476563, 9.890407104492187, 9.875303344726563, 9.888787841796875, 9.903206787109376, 9.856061401367187, 9.818549194335937, 9.767147827148438, 9.777191162109375, 9.76476318359375, 9.64578125, 9.69122802734375, 9.48787353515625, 9.385001220703124, 9.169375, 8.8238671875, 8.491317749023438, 8.319290771484376, 7.84476318359375, 7.494476928710937, 7.703692016601562, 8.054328002929687]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:28:10.172975>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.338573608398438.pt\n",
      "[9.934776000976562, 9.921500244140624, 9.876690673828126, 9.866650390625, 9.806224365234375, 9.73049072265625, 9.4096435546875, 9.018741455078125, 8.187810668945312, 7.9406890869140625, 8.069180297851563, 9.103997802734375, 7.8982275390625, 7.534838256835937, 8.278837890625, 7.834146728515625, 8.254130859375, 8.220880126953125, 7.46704833984375, 7.711103515625, 7.659028930664062, 7.6447906494140625, 7.338573608398438, 8.209035034179687, 7.813238525390625]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:28:12.391975>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.999983520507812.pt\n",
      "[9.934288330078125, 9.89798583984375, 9.83476318359375, 9.623220825195313, 8.996319580078126, 8.212664794921874, 8.397537231445312, 8.1227734375, 8.798097534179687, 7.80484375, 8.329498901367188, 7.434282836914062, 8.114554443359374, 7.667957763671875, 7.907283325195312, 8.042982788085938, 7.593072509765625, 6.999983520507812, 8.486248779296876, 7.71273193359375, 8.203995361328126, 8.387462768554688, 7.725703125, 7.4338134765625, 7.360684204101562]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:28:14.818899>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.228425903320312.pt\n",
      "[9.935028686523438, 9.921027221679687, 9.8924853515625, 9.891441040039062, 9.852772216796875, 9.835633544921874, 9.845025024414063, 9.73181884765625, 9.666575317382813, 9.605679931640625, 9.385824584960938, 8.699232788085938, 8.85631591796875, 7.984048461914062, 7.881839599609375, 7.87766845703125, 7.950131225585937, 7.350661010742187, 7.4768017578125, 7.228425903320312, 7.519693603515625, 7.604849243164063, 8.643334350585938, 8.197850341796874, 7.6735888671875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:28:18.009371>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.740435180664062.pt\n",
      "[9.929609985351563, 9.907347412109376, 9.889594116210937, 9.645630493164063, 9.325258178710937, 8.689122314453124, 8.238989868164062, 8.015304565429688, 9.123644409179688, 6.740435180664062, 7.610487670898437, 7.660325317382813, 8.25243408203125, 7.592958374023437, 8.251421508789063, 8.063229370117188, 7.2728955078125, 8.052466430664062, 8.330121459960937, 7.171109619140625, 7.5770025634765625, 7.163367919921875, 8.118007202148437, 7.289686279296875, 8.010816040039062]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:28:21.287608>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.299442749023438.pt\n",
      "[9.933246459960937, 9.892489013671875, 9.70517333984375, 8.797903442382813, 8.281801147460937, 8.576959228515625, 8.0278564453125, 8.00951416015625, 8.519295043945313, 8.10344970703125, 8.530218505859375, 7.892924194335937, 7.66984375, 7.477701416015625, 7.41811767578125, 7.953187866210937, 7.868715209960937, 7.712870483398437, 7.910860595703125, 8.601708374023438, 7.976914672851563, 8.3097509765625, 7.61465576171875, 7.299442749023438, 7.6596875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:28:24.423224>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_8.268441772460937.pt\n",
      "[9.937376708984376, 9.914121704101563, 9.90579833984375, 9.909864501953125, 9.892445678710937, 9.889859619140625, 9.863988037109374, 9.85618896484375, 9.827056274414062, 9.834498291015626, 9.822955932617187, 9.788681640625, 9.728195190429688, 9.707211303710938, 9.677314453125, 9.615143432617188, 9.509317626953125, 9.474869995117187, 9.271957397460938, 9.257604370117187, 9.022048950195312, 8.836002197265625, 8.593193969726563, 8.7346630859375, 8.268441772460937]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:28:32.842325>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.200625610351563.pt\n",
      "[9.9251904296875, 9.910745849609375, 9.870897827148438, 9.840933227539063, 9.767301025390625, 9.64619384765625, 9.558356323242187, 9.24078857421875, 8.888878784179688, 8.402171020507813, 8.365663452148437, 8.016974487304687, 7.200625610351563, 7.408883056640625, 7.671150512695313, 8.104677124023437, 7.903178100585937, 7.7164697265625, 7.535719604492187, 8.029370727539062, 8.103954467773438, 7.796527709960937, 7.464990234375, 7.626140747070313, 7.287723388671875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:28:40.974315>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.08469482421875.pt\n",
      "[9.915155029296875, 9.888873901367187, 9.831836547851562, 9.702090454101562, 9.4678515625, 8.93850341796875, 8.204850463867187, 7.948372192382813, 8.010556640625, 7.65587158203125, 7.449299926757813, 7.743624267578125, 7.457763061523438, 7.9339666748046875, 7.08469482421875, 7.6905670166015625, 7.179053955078125, 7.308072509765625, 7.310701293945312, 7.225682373046875, 7.774541625976562, 7.56738525390625, 7.109074096679688, 7.801995849609375, 7.502301635742188]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:28:49.286319>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.827996826171875.pt\n",
      "[9.927512817382812, 9.91263916015625, 9.903828125, 9.886187133789063, 9.879572143554688, 9.85354736328125, 9.80177490234375, 9.75701416015625, 9.734754028320312, 9.6476416015625, 9.560865478515625, 9.417041625976562, 9.097241821289062, 8.750928955078125, 8.44510009765625, 8.3447119140625, 7.612911987304687, 7.8557501220703125, 7.137216186523437, 7.982300415039062, 7.43231689453125, 7.458573608398438, 7.2376641845703125, 7.33396728515625, 6.827996826171875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:28:57.522384>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.06203125.pt\n",
      "[9.9201123046875, 9.895264282226563, 9.860863037109375, 9.728015747070312, 9.381779174804688, 8.869188232421875, 8.188074340820313, 7.646098022460937, 7.572398681640625, 7.954072265625, 7.822761840820313, 8.00387939453125, 8.299708862304687, 7.782799072265625, 7.56254150390625, 7.06203125, 7.57959228515625, 7.427241821289062, 7.319607543945312, 7.251897583007812, 7.280164184570313, 7.141781616210937, 7.414617309570312, 7.792030029296875, 7.355704345703125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:29:05.714217>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.101890258789062.pt\n",
      "[9.922011108398438, 9.864705200195312, 9.705704956054687, 8.979749145507812, 8.025150756835938, 8.070405883789062, 8.529518432617188, 7.8113336181640625, 7.6075537109375, 7.484328002929687, 7.74688720703125, 7.71505615234375, 7.934285278320313, 7.723899536132812, 7.887716064453125, 7.564661254882813, 7.62383544921875, 7.614537963867187, 7.354207763671875, 7.516336669921875, 7.74455078125, 7.210404663085938, 7.5231695556640625, 7.101890258789062, 7.38114501953125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:29:14.308218>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.997141723632812.pt\n",
      "[9.929690551757812, 9.911034545898438, 9.89112548828125, 9.86728759765625, 9.830014038085938, 9.768417358398438, 9.654498901367187, 9.379401245117187, 9.237163696289063, 8.240454711914062, 7.899768676757812, 7.677078857421875, 7.71033447265625, 7.4639044189453125, 7.674666137695312, 7.851920776367187, 7.85049560546875, 7.706376342773438, 7.22361083984375, 7.277932739257812, 7.381500244140625, 7.28437744140625, 7.2706201171875, 6.997141723632812, 7.528936767578125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:29:26.231078>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.823145751953125.pt\n",
      "[9.924269409179688, 9.862577514648438, 9.666998291015625, 9.018170166015626, 7.770802001953125, 8.257493286132812, 8.837688598632813, 8.032432250976562, 7.797945556640625, 7.48050048828125, 7.7598193359375, 7.302796020507812, 7.9410205078125, 8.02871337890625, 7.884791870117187, 7.697252197265625, 7.114161987304687, 7.669290161132812, 8.2086962890625, 7.503604125976563, 6.823145751953125, 7.397594604492188, 7.605435791015625, 7.234487915039063, 7.7498016357421875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:29:37.911957>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.9324713134765625.pt\n",
      "[9.929136962890626, 9.861367797851562, 9.327440795898438, 7.796712036132813, 8.330403442382812, 8.320281372070312, 8.26945556640625, 7.950050048828125, 7.826249389648438, 7.937965087890625, 7.9512091064453125, 7.396410522460937, 7.58576904296875, 7.569799194335937, 7.33162841796875, 7.545029907226563, 7.819569702148438, 7.168703002929687, 7.140037841796875, 7.4941949462890625, 7.754586791992187, 7.273259887695312, 7.371875, 6.9324713134765625, 7.638056640625]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:29:49.656469>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.904309895833333.pt\n",
      "[9.917250162760416, 9.91427001953125, 9.902086588541666, 9.905896809895834, 9.888355305989583, 9.877682291666666, 9.852135416666666, 9.855443522135417, 9.814115397135417, 9.786577962239583, 9.750777180989584, 9.7386279296875, 9.707401529947917, 9.620367024739583, 9.557211100260417, 9.404681803385417, 9.275952962239584, 9.243580729166666, 9.11861572265625, 8.982325032552083, 8.674934895833333, 8.526885579427084, 8.315027669270833, 7.904309895833333, 7.9146337890625]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:30:01.831007>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.131504720052083.pt\n",
      "[9.936524251302083, 9.901217447916666, 9.886128743489584, 9.848094075520834, 9.788871256510417, 9.66877197265625, 9.5282763671875, 9.307513020833333, 8.751360677083333, 8.383519694010417, 8.143653971354167, 7.743558756510416, 7.324224446614584, 7.3144523111979165, 7.4351188151041665, 7.746555989583333, 7.401844075520834, 7.284529622395834, 7.616556803385417, 7.531519368489583, 8.177818196614583, 7.304105631510416, 7.364820963541667, 7.131504720052083, 7.401279296875]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:30:14.590007>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.815974934895833.pt\n",
      "[9.92544677734375, 9.882284342447917, 9.835162760416667, 9.687030436197917, 9.314578450520834, 8.772639973958333, 8.152645670572916, 7.37389892578125, 7.369488932291667, 6.815974934895833, 8.388779296875, 8.330634765625, 7.8710888671875, 8.00422607421875, 7.2551416015625, 7.636654459635417, 7.183343912760416, 8.147269694010417, 7.896338704427083, 7.210777994791667, 7.442753092447917, 7.616143391927083, 7.2955314127604165, 7.477911783854167, 7.47984375]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:30:27.267007>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.861383463541666.pt\n",
      "[9.926624348958333, 9.910309244791666, 9.902792154947917, 9.88031982421875, 9.866110026041667, 9.8486669921875, 9.81869140625, 9.751439615885417, 9.682379557291666, 9.552080078125, 9.527454427083333, 9.169110514322917, 8.966932779947916, 8.617755533854167, 8.4709033203125, 7.855038248697917, 7.694482421875, 7.737066243489584, 7.1831673177083335, 7.786625162760417, 7.08767578125, 7.105455729166667, 6.861383463541666, 7.710596516927083, 7.471338704427083]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:30:40.078009>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.899036458333334.pt\n",
      "[9.928129069010417, 9.8822119140625, 9.843335774739584, 9.685319010416666, 9.312344563802084, 8.51688232421875, 8.072865397135416, 7.401421712239583, 7.850452473958334, 7.645226236979167, 7.858462727864583, 7.751600748697917, 7.939991048177084, 6.899036458333334, 7.464563802083333, 7.080616048177084, 7.664982096354167, 7.46678466796875, 7.820487467447917, 7.530745442708334, 7.173230794270833, 7.4898909505208335, 7.237639973958333, 7.342296549479166, 7.5018302408854165]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:30:52.486550>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.810723876953125.pt\n",
      "[9.93092529296875, 9.874049479166667, 9.766009114583333, 9.276879069010416, 8.308216959635416, 7.539962565104167, 8.086927083333334, 7.902765299479166, 8.211464029947917, 7.78521240234375, 7.45966552734375, 7.57674560546875, 7.737281087239583, 7.164800618489584, 7.331245930989583, 7.782568359375, 7.657237955729166, 7.5461214192708335, 7.097451171875, 7.236912434895833, 7.124891764322917, 7.552400716145834, 6.810723876953125, 6.822945556640625, 7.706678059895833]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:31:04.917135>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.93457763671875.pt\n",
      "[9.929491373697916, 9.904344075520834, 9.872203776041667, 9.825005696614584, 9.801189778645833, 9.649292805989583, 9.433736979166667, 8.808857421875, 8.421097819010416, 7.883068033854166, 7.403783365885417, 7.2605802408854165, 7.44589599609375, 7.150863444010417, 7.580552571614583, 7.45561279296875, 7.400345865885416, 7.431490071614584, 7.593929036458333, 7.13619384765625, 7.3944873046875, 7.150274251302084, 7.153550618489583, 6.93457763671875, 7.870213216145833]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:31:22.284664>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.31537109375.pt\n",
      "[9.923611653645834, 9.8668798828125, 9.676412760416667, 8.710436197916666, 7.856925455729167, 7.705775553385417, 8.243601888020834, 8.239287109375, 8.224659830729166, 7.870732421875, 7.332960611979167, 7.687806803385417, 7.6740185546875, 7.4714306640625, 7.619230143229166, 7.556474609375, 7.83037353515625, 7.47644287109375, 7.629589029947916, 7.402052408854167, 7.374267578125, 7.40983642578125, 7.411318359375, 7.39679931640625, 7.31537109375]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:31:39.715340>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.0145751953125.pt\n",
      "[9.93698974609375, 9.84424560546875, 9.159176432291666, 7.75623779296875, 8.787412923177083, 7.656875, 7.659306640625, 7.377405598958333, 7.23403564453125, 7.4219352213541665, 7.708994954427084, 7.400203450520833, 7.4263517252604165, 7.891973470052084, 7.09760986328125, 7.0476912434895835, 7.285284830729167, 7.669478352864584, 7.0145751953125, 7.055660807291667, 7.83405029296875, 7.163633626302083, 7.430464680989584, 7.220948893229167, 7.20497314453125]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:31:57.103643>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_9.153699340820312.pt\n",
      "[9.934627075195312, 9.9172412109375, 9.8971142578125, 9.899901733398437, 9.883214721679687, 9.877427978515625, 9.890008544921875, 9.8764501953125, 9.880485229492187, 9.867489013671875, 9.865576171875, 9.837808227539062, 9.800701904296876, 9.8380517578125, 9.756417846679687, 9.793020629882813, 9.804078979492187, 9.673009033203124, 9.661835327148438, 9.617338256835938, 9.480621337890625, 9.414736328125, 9.408902587890625, 9.153699340820312, 9.269152221679688]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:32:01.177071>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.08404541015625.pt\n",
      "[9.930162353515625, 9.902261352539062, 9.895740356445312, 9.892901611328124, 9.858602905273438, 9.816718139648437, 9.713356323242188, 9.71083740234375, 9.474998779296875, 9.4252001953125, 9.042139892578126, 8.513782958984375, 8.340662231445313, 8.018587646484375, 7.08404541015625, 8.0264697265625, 7.68383544921875, 8.141743774414062, 7.5042626953125, 8.610506591796875, 7.936505737304688, 7.758780517578125, 7.235438842773437, 8.091519775390625, 7.696890258789063]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:32:05.401070>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.400691528320312.pt\n",
      "[9.924188842773438, 9.912372436523437, 9.88800048828125, 9.805249633789062, 9.809428100585938, 9.3505615234375, 9.178656005859375, 8.59006103515625, 8.54203125, 8.572827758789062, 8.287232055664063, 8.315745849609375, 8.213389282226563, 7.825836181640625, 8.40062744140625, 7.400691528320312, 8.356741333007813, 8.9590234375, 8.032764282226562, 7.79125732421875, 8.66327880859375, 7.713834228515625, 7.454146728515625, 7.6354217529296875, 7.806751098632812]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:32:09.336613>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.19400390625.pt\n",
      "[9.91241455078125, 9.908197631835938, 9.903330688476563, 9.9059521484375, 9.880717163085938, 9.881453857421874, 9.84209716796875, 9.840921020507812, 9.83510009765625, 9.794957275390624, 9.717261962890625, 9.795584716796874, 9.58347900390625, 9.632611694335937, 9.530418701171875, 9.3248193359375, 9.12631591796875, 9.118499755859375, 8.648151245117187, 8.02332763671875, 8.360960693359376, 7.225809326171875, 7.19400390625, 8.0298681640625, 7.633622436523438]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:32:13.686113>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.782938232421875.pt\n",
      "[9.930582885742188, 9.906131591796875, 9.900738525390626, 9.875075073242188, 9.789944458007813, 9.773019409179687, 9.465789184570312, 9.128051147460937, 8.463629150390625, 7.8238427734375, 7.917493896484375, 7.665995483398437, 8.704003295898438, 8.470270385742188, 8.359181518554687, 8.629574584960938, 8.1426708984375, 7.406456298828125, 7.9117041015625, 6.960745239257813, 7.464435424804687, 7.811574096679688, 8.043837280273438, 7.951613159179687, 6.782938232421875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:32:17.882327>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.969354858398438.pt\n",
      "[9.93330810546875, 9.89604248046875, 9.836590576171876, 9.782634887695313, 9.427567749023437, 8.624176635742188, 8.2699853515625, 8.517041625976562, 8.153370361328125, 8.595097045898438, 7.67102294921875, 8.213042602539062, 6.969354858398438, 8.404407348632812, 8.076961669921875, 7.90256103515625, 7.879588623046875, 7.791981201171875, 7.602730712890625, 7.252988891601563, 7.262232666015625, 7.8205029296875, 8.437293090820312, 7.761776733398437, 8.526447143554687]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:32:22.164477>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.633179931640625.pt\n",
      "[9.925687255859375, 9.910267944335937, 9.913645629882813, 9.882376708984374, 9.85774658203125, 9.845617065429687, 9.82055419921875, 9.753411865234375, 9.675667114257813, 9.521619873046875, 9.171535034179687, 8.705309448242188, 8.253768310546874, 8.218533935546875, 7.62870849609375, 8.420328369140625, 7.94921875, 7.959257202148438, 8.697210693359375, 8.038230590820312, 7.602457275390625, 6.633179931640625, 7.740812377929688, 7.616528930664063, 7.758395385742188]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:32:27.132606>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.572930908203125.pt\n",
      "[9.932017822265625, 9.919494018554687, 9.845560913085938, 9.715801391601563, 9.134256591796875, 8.416572875976563, 8.782623901367188, 8.205808715820313, 7.206229248046875, 7.724891967773438, 7.44662109375, 7.35851318359375, 7.294847412109375, 7.885556030273437, 7.440798950195313, 7.53317626953125, 7.956491088867187, 8.164401245117187, 6.572930908203125, 7.6425927734375, 8.055391845703125, 7.91323486328125, 7.462794799804687, 8.06388916015625, 8.279620361328124]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:32:32.089363>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.123657836914062.pt\n",
      "[9.923999633789062, 9.851495971679688, 9.666854858398438, 8.387066040039063, 10.043074951171874, 8.187866821289063, 8.245239868164063, 8.242734985351563, 8.2925341796875, 8.27563720703125, 8.538211059570312, 7.92293212890625, 7.288844604492187, 8.447445068359375, 7.993151245117187, 7.80239501953125, 7.93908203125, 7.806484375, 7.608338623046875, 7.604830322265625, 8.268646240234375, 7.566441650390625, 8.067843627929687, 7.123657836914062, 7.599111938476563]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:32:36.764106>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_8.067022094726562.pt\n",
      "[9.92878662109375, 9.920847778320313, 9.910707397460937, 9.916881713867188, 9.892852783203125, 9.887008056640624, 9.876455078125, 9.85640625, 9.832931518554688, 9.822260131835938, 9.78472900390625, 9.770279541015626, 9.746661987304687, 9.705424194335938, 9.66780517578125, 9.519232788085937, 9.5142138671875, 9.285364379882813, 9.305905151367188, 9.051094970703126, 8.846090698242188, 8.844805908203124, 8.48956787109375, 8.766117553710938, 8.067022094726562]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:32:52.503590>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.840653686523438.pt\n",
      "[9.920191650390626, 9.897139282226563, 9.876611938476563, 9.842366943359375, 9.791755981445313, 9.687285766601562, 9.54393310546875, 9.301104736328124, 8.979992065429688, 8.497894897460938, 8.340227661132813, 7.896057739257812, 7.37835205078125, 7.176104125976562, 7.588619384765625, 7.42741943359375, 7.2980584716796875, 7.833043212890625, 8.09748779296875, 7.69167236328125, 7.264376220703125, 7.678613891601563, 7.249505615234375, 6.840653686523438, 6.864213256835938]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:33:07.647590>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.96378662109375.pt\n",
      "[9.932864379882812, 9.89281005859375, 9.83405029296875, 9.733464965820312, 9.51290771484375, 9.169358520507812, 8.563196411132813, 8.191390380859374, 7.216428833007813, 7.4309765625, 7.82828857421875, 7.7884228515625, 7.713924560546875, 8.0977783203125, 7.60334716796875, 8.144559326171875, 7.28480712890625, 7.4683984375, 7.69445068359375, 7.519082641601562, 6.96378662109375, 7.244874267578125, 7.660179443359375, 7.5383978271484375, 7.594884643554687]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:33:23.323590>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.225728759765625.pt\n",
      "[9.926155395507813, 9.914496459960937, 9.901702880859375, 9.884590454101563, 9.880609741210938, 9.851044921875, 9.825057373046874, 9.791279296875, 9.72573974609375, 9.5963916015625, 9.513316650390625, 9.278226318359375, 8.919077758789063, 8.850202026367187, 8.262366943359375, 8.023900756835937, 8.0550830078125, 7.57449462890625, 7.24493408203125, 7.6921826171875, 7.225728759765625, 7.588704833984375, 7.890076904296875, 7.4857568359375, 7.815089111328125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:33:39.230897>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.985082397460937.pt\n",
      "[9.920094604492187, 9.88299072265625, 9.845040283203126, 9.729320068359375, 9.359786987304688, 9.038572387695313, 8.151512451171875, 7.32414794921875, 8.148107299804687, 7.67773193359375, 7.655872802734375, 8.031556396484374, 8.304786376953125, 7.77430419921875, 7.297119140625, 7.353455200195312, 7.564966430664063, 7.4449267578125, 7.857401123046875, 7.528880615234375, 7.347456665039062, 6.985082397460937, 7.436931762695313, 7.774295043945313, 7.770296630859375]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:33:54.603897>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.160322875976562.pt\n",
      "[9.92813720703125, 9.887455444335938, 9.798192138671874, 9.267655639648437, 7.766611328125, 7.636382446289063, 8.426114501953125, 8.234991455078125, 7.78580322265625, 7.5633056640625, 7.374533081054688, 7.350191040039062, 7.686771240234375, 7.6730078125, 7.506116333007813, 7.5649658203125, 7.498839111328125, 7.6334716796875, 7.684620971679688, 7.3138916015625, 7.431845092773438, 7.510042724609375, 7.670241088867187, 7.160322875976562, 7.233155517578125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:34:10.110896>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.990747680664063.pt\n",
      "[9.932112426757813, 9.90549072265625, 9.883212280273437, 9.85665283203125, 9.783026123046875, 9.654954833984375, 9.4764404296875, 9.16234130859375, 8.561976318359376, 7.949574584960938, 7.503042602539063, 7.266683349609375, 7.570189819335938, 7.719883422851563, 7.314769897460938, 8.194365844726562, 7.28716552734375, 7.790466918945312, 7.664049072265625, 7.3557421875, 7.2952813720703125, 7.526273193359375, 6.990747680664063, 7.973505249023438, 7.3879052734375]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:34:28.274536>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.941132202148437.pt\n",
      "[9.925146484375, 9.837083740234375, 9.636859130859374, 8.935662841796875, 7.668256225585938, 7.90685791015625, 8.163565673828124, 8.203480224609375, 7.481658325195313, 7.520154418945313, 7.69100830078125, 7.545950927734375, 7.984098510742188, 7.488328247070313, 7.70252197265625, 7.426692504882812, 7.854888305664063, 7.7163018798828125, 6.941132202148437, 7.4249267578125, 7.248248901367187, 7.608665771484375, 7.773856811523437, 7.488634643554687, 7.465194091796875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:34:46.540124>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.8319635009765625.pt\n",
      "[9.928076782226562, 9.8460791015625, 9.08353759765625, 8.268196411132813, 7.760916748046875, 7.76805908203125, 7.589512939453125, 7.411557006835937, 7.410538330078125, 8.141561279296875, 7.387028198242188, 7.442330322265625, 7.63503662109375, 7.565852661132812, 7.374375, 7.545870971679688, 7.428696899414063, 6.8868646240234375, 7.2498291015625, 6.8319635009765625, 7.068153076171875, 7.385475463867188, 7.577664794921875, 7.163632202148437, 6.91729736328125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:35:04.757141>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.931102701822916.pt\n",
      "[9.909775390625, 9.904069010416666, 9.895423177083334, 9.898173014322916, 9.886226399739583, 9.8655615234375, 9.847827962239583, 9.839859212239583, 9.817919108072916, 9.791429036458334, 9.747823079427084, 9.701435546875, 9.654344075520834, 9.596043294270833, 9.53943115234375, 9.36735107421875, 9.327212727864584, 9.03855712890625, 8.967210286458334, 8.679629720052084, 8.46861328125, 8.365875651041666, 8.079852701822917, 8.195093587239583, 7.931102701822916]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:35:27.332884>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.2607373046875.pt\n",
      "[9.943224283854166, 9.91482177734375, 9.865541178385417, 9.824681803385417, 9.746615397135416, 9.65636474609375, 9.433201497395833, 9.073133951822916, 8.59347412109375, 8.23228515625, 7.98031005859375, 7.612178548177083, 7.682940266927083, 7.583748372395833, 7.7676383463541665, 7.267822265625, 7.288644205729167, 7.802279459635416, 7.67761962890625, 7.2607373046875, 7.509617513020833, 8.314812825520834, 7.81865966796875, 7.65279296875, 7.74095947265625]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:35:49.732855>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.04494873046875.pt\n",
      "[9.928829752604166, 9.892266438802084, 9.841702473958334, 9.74863037109375, 9.39621826171875, 8.825888671875, 8.4985498046875, 8.065756022135417, 7.470257975260417, 7.19278076171875, 7.543286946614583, 8.081280924479167, 7.877112630208333, 7.834286295572917, 8.243643391927083, 7.424383138020834, 7.252395833333333, 7.76256591796875, 7.112474772135417, 7.590133463541667, 7.42186279296875, 7.556642252604167, 7.40720947265625, 7.403331705729166, 7.04494873046875]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:36:12.924857>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.774728190104167.pt\n",
      "[9.91568359375, 9.908539225260416, 9.890609537760417, 9.883759765625, 9.8599462890625, 9.818710123697917, 9.78884765625, 9.749453125, 9.665597330729167, 9.483965657552083, 9.265763346354166, 9.25708984375, 8.995892740885417, 8.43359619140625, 7.910594075520834, 7.945950520833334, 7.490423990885417, 7.285236002604167, 7.372784016927083, 7.474742838541666, 7.209969889322917, 7.27556640625, 7.2321923828125, 6.774728190104167, 7.345515950520833]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:36:36.090856>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.761883138020833.pt\n",
      "[9.923792317708333, 9.87943115234375, 9.806312662760417, 9.596631673177084, 9.0696435546875, 8.423042805989583, 7.812059733072917, 7.63422119140625, 7.959215494791667, 8.086256510416666, 7.976066080729167, 7.793583984375, 7.592393391927083, 7.837529296875, 7.465498860677084, 7.1773299153645835, 7.182201334635416, 7.220546875, 6.891640625, 6.761883138020833, 7.704073893229166, 7.560421549479167, 7.364181315104167, 7.715865885416667, 7.771885579427083]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:36:59.408814>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.852206217447916.pt\n",
      "[9.922699381510416, 9.866083170572917, 9.638055013020834, 8.89874267578125, 8.118916829427084, 7.844305013020834, 7.713243815104167, 7.216075032552084, 7.631866861979167, 7.277919108072917, 7.212115885416667, 7.9588395182291665, 7.389111328125, 7.725332845052083, 7.2502392578125, 7.400431315104167, 7.436968587239583, 7.201083984375, 7.58972412109375, 6.852206217447916, 7.299767252604167, 7.51012451171875, 7.373076985677083, 7.096707356770834, 7.216123860677083]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:37:22.597099>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.056139322916667.pt\n",
      "[9.93306640625, 9.912313639322917, 9.879392903645833, 9.83894775390625, 9.7930908203125, 9.703133138020833, 9.510940755208333, 9.079849446614583, 8.555647786458334, 7.915756022135417, 7.328478190104167, 7.500629069010417, 7.550519205729167, 7.85013916015625, 7.628741048177083, 7.300773111979167, 7.566185709635417, 7.39970947265625, 7.607001953125, 7.516904296875, 7.307019856770833, 7.09502197265625, 7.056139322916667, 7.702355143229167, 7.585663248697917]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:37:49.670581>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.85338134765625.pt\n",
      "[9.930045572916667, 9.874649251302083, 9.714973958333333, 8.940349934895833, 8.082368977864583, 8.37648193359375, 7.9640283203125, 7.602045084635416, 7.603197428385417, 7.628280436197917, 7.647937825520834, 7.631912434895833, 7.638726399739583, 7.3869775390625, 7.466192220052084, 7.7511588541666665, 7.053839518229167, 7.98325927734375, 7.5279541015625, 6.85338134765625, 7.1597216796875, 7.5897835286458335, 7.438932291666666, 6.938409016927083, 7.1041048177083335]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:38:16.782728>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.873223470052083.pt\n",
      "[9.929439290364583, 9.852176106770834, 9.114718424479166, 7.756288248697917, 8.528998209635416, 7.5148933919270835, 7.5167854817708335, 7.63611083984375, 7.319586588541667, 7.613985188802084, 8.767962239583333, 7.38384765625, 7.6241064453125, 7.756876627604167, 7.295697428385417, 7.216644694010417, 7.337615559895833, 7.564755859375, 6.9523917643229165, 6.873223470052083, 6.8888151041666665, 7.189256998697917, 7.090470377604166, 7.404286295572916, 7.235318196614584]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:38:43.570216>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_9.039959106445313.pt\n",
      "[9.948829345703125, 9.933831176757813, 9.930151977539062, 9.92017822265625, 9.919605102539062, 9.913578491210938, 9.891130981445313, 9.9023291015625, 9.905098266601563, 9.87830322265625, 9.886085815429688, 9.876256103515624, 9.8267578125, 9.856347045898438, 9.800150756835938, 9.79794921875, 9.777089233398437, 9.775603637695312, 9.67119140625, 9.60613037109375, 9.685769653320312, 9.58006103515625, 9.4725390625, 9.324561157226562, 9.039959106445313]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:38:49.782916>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.404786376953125.pt\n",
      "[9.92384033203125, 9.903654174804688, 9.887620239257812, 9.866944580078124, 9.85211181640625, 9.808421020507813, 9.81248291015625, 9.590401611328126, 9.531759033203125, 9.495930786132812, 8.827676391601562, 8.662721557617187, 8.6389404296875, 8.615345458984375, 8.345606689453126, 7.404786376953125, 8.164295654296875, 8.629237670898437, 7.974232788085938, 8.349661254882813, 8.814099731445312, 7.534028930664062, 7.751858520507812, 8.337459106445312, 7.635609130859375]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:38:55.931133>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.630975341796875.pt\n",
      "[9.922702026367187, 9.901332397460937, 9.88697509765625, 9.871571655273437, 9.79423583984375, 9.673109741210938, 9.6229296875, 9.179134521484375, 8.333350219726562, 8.569669189453125, 8.374725341796875, 7.53616943359375, 7.606685180664062, 8.450985717773438, 7.795357666015625, 7.0885858154296875, 8.205346069335938, 7.759508666992187, 7.825320434570313, 7.34480224609375, 8.02039306640625, 7.838614501953125, 8.262595825195312, 6.630975341796875, 7.494373779296875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:39:02.016104>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.00047607421875.pt\n",
      "[9.932691650390625, 9.919925537109375, 9.925933227539062, 9.912540283203125, 9.880377197265625, 9.888230590820312, 9.863533325195313, 9.865277099609376, 9.87212890625, 9.820902709960938, 9.768690185546875, 9.740855712890625, 9.702344360351562, 9.651295776367187, 9.466997680664063, 9.334533081054687, 9.09569091796875, 9.019118041992188, 8.726788940429687, 8.436873168945313, 7.759166259765625, 7.870308837890625, 7.00047607421875, 8.086554565429687, 7.027035522460937]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:39:08.262507>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.57916748046875.pt\n",
      "[9.920638427734374, 9.901317138671875, 9.879607543945312, 9.821725463867187, 9.752896118164063, 9.45749755859375, 8.80996826171875, 8.801860961914063, 8.219386596679687, 7.995089111328125, 8.540413208007813, 8.122289428710937, 8.167138671875, 7.73471923828125, 7.741035766601563, 7.788052368164062, 8.15232666015625, 7.922763671875, 8.046196899414063, 7.855921020507813, 8.143926391601562, 7.984983520507813, 7.57916748046875, 7.780406494140625, 7.664490966796875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:39:14.596509>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.96385986328125.pt\n",
      "[9.92759033203125, 9.917691040039063, 9.862686157226562, 9.813078002929688, 9.572139282226562, 8.8264453125, 8.571544799804688, 8.655826416015625, 8.04449462890625, 7.9278369140625, 7.132664794921875, 7.940595703125, 8.340385131835937, 6.96385986328125, 7.712625732421875, 7.572968139648437, 8.049664916992187, 7.58715576171875, 7.465535888671875, 7.673504638671875, 8.098106079101562, 7.291596069335937, 7.389998779296875, 7.671403198242188, 7.580906372070313]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:39:20.950508>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.158271484375.pt\n",
      "[9.922509155273438, 9.905133666992187, 9.902075805664062, 9.882283325195312, 9.873397216796874, 9.844835205078125, 9.80861083984375, 9.66703369140625, 9.399524536132812, 9.464822387695312, 8.97946533203125, 9.142896118164062, 8.1059423828125, 7.95948974609375, 8.848572998046874, 8.588233642578125, 7.937744140625, 7.396231079101563, 7.89843505859375, 8.443702392578125, 8.055257568359375, 8.464292602539063, 7.209063110351562, 7.158271484375, 7.355849609375]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:39:27.963960>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_6.856006469726562.pt\n",
      "[9.931740112304688, 9.863202514648437, 9.81874755859375, 9.543203125, 9.005169677734376, 8.700887451171875, 7.702513427734375, 8.467344360351563, 7.921259765625, 8.2102490234375, 7.770888061523437, 8.836552124023438, 8.143819580078125, 8.025588989257812, 8.179404296875, 7.905072631835938, 7.857022094726562, 7.670664672851562, 7.506475830078125, 6.867821044921875, 6.856006469726562, 7.960520629882812, 7.215806884765625, 7.730233154296875, 7.228499755859375]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:39:35.065608>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_50_words_20449_loss_7.212915649414063.pt\n",
      "[9.928140869140625, 9.857179565429687, 9.83217041015625, 8.964088745117188, 8.353946533203125, 8.088732299804688, 7.674796142578125, 8.790427856445312, 7.784302978515625, 7.986309814453125, 8.377406616210937, 7.71508056640625, 8.3174658203125, 8.074448852539062, 7.8108282470703125, 7.298426513671875, 7.8066033935546875, 9.002222290039063, 7.566408081054687, 7.586094970703125, 7.5716357421875, 7.9434375, 7.212915649414063, 7.660306396484375, 7.3881903076171875]\n",
      "epochs: 25\n",
      "chunk length: 50\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:39:42.054899>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_8.247473754882812.pt\n",
      "[9.918560180664063, 9.91577392578125, 9.906561889648437, 9.904365844726563, 9.890861206054687, 9.895711669921875, 9.87148193359375, 9.854856567382813, 9.835648193359376, 9.83172607421875, 9.815119018554688, 9.7677587890625, 9.7491015625, 9.719505004882812, 9.654993286132813, 9.588170776367187, 9.582864379882812, 9.4427197265625, 9.220111083984374, 9.173994750976563, 9.025458984375, 8.673305053710937, 8.557415161132813, 8.459424438476562, 8.247473754882812]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:40:05.427211>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.279529418945312.pt\n",
      "[9.933387451171875, 9.901543579101563, 9.8626220703125, 9.831064453125, 9.765817260742187, 9.650353393554688, 9.436454467773437, 9.111726684570312, 8.803530883789062, 8.310040283203126, 8.2162939453125, 7.279529418945312, 7.59001220703125, 7.523499145507812, 7.47085205078125, 8.063240356445313, 7.889039306640625, 8.037337036132813, 7.976670532226563, 8.181019287109375, 8.209318237304688, 7.680665283203125, 7.410962524414063, 7.404187622070313, 7.390149536132813]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:40:28.094213>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.954027099609375.pt\n",
      "[9.936796264648438, 9.90257080078125, 9.851588134765626, 9.761416015625, 9.57864013671875, 9.149419555664062, 8.425863037109375, 7.860574951171875, 7.426292724609375, 7.439152221679688, 7.848087768554688, 7.747188720703125, 7.727042846679687, 7.921009521484375, 7.93921142578125, 7.125205078125, 7.535384521484375, 7.781880493164063, 7.27341552734375, 7.543404541015625, 7.067313842773437, 6.954027099609375, 7.955294799804688, 7.480338134765625, 7.700303955078125]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:40:51.149242>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.905643310546875.pt\n",
      "[9.924700927734374, 9.917269897460937, 9.904434814453126, 9.894947509765625, 9.87540283203125, 9.842179565429687, 9.8240380859375, 9.778317260742188, 9.725457153320313, 9.638658447265625, 9.433456420898438, 9.36778564453125, 8.955343627929688, 8.840757446289063, 8.288390502929687, 8.230322875976562, 7.6888665771484375, 7.738345336914063, 7.4550732421875, 7.303347778320313, 7.201936645507812, 7.321599731445312, 6.905643310546875, 7.276172485351562, 7.2310009765625]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:41:14.651459>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.152674560546875.pt\n",
      "[9.919545288085937, 9.885419311523437, 9.837807006835938, 9.741759033203126, 9.421806030273437, 8.68635009765625, 8.000015869140626, 7.58501220703125, 8.01394775390625, 7.83832275390625, 7.91050048828125, 8.104078369140625, 7.485921630859375, 8.038971557617188, 8.17360595703125, 7.160673217773438, 7.4033154296875, 7.736821899414062, 7.388397827148437, 7.5352630615234375, 7.480943603515625, 7.321422729492188, 7.217455444335937, 7.228656005859375, 7.152674560546875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:41:38.159916>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.130147094726563.pt\n",
      "[9.919743041992188, 9.878138427734376, 9.760964965820312, 9.227621459960938, 8.267479248046875, 8.101170654296874, 8.3065087890625, 8.201605224609375, 7.952081909179688, 7.885996704101562, 7.667813720703125, 7.438511352539063, 8.119898681640626, 8.085810546875, 7.57108642578125, 7.316860961914062, 7.64773193359375, 7.751185913085937, 7.745576171875, 7.512243041992187, 7.5267822265625, 7.635659790039062, 7.174456787109375, 7.6092681884765625, 7.130147094726563]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:42:01.421916>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.84713623046875.pt\n",
      "[9.9182080078125, 9.8989208984375, 9.880242309570313, 9.850203857421874, 9.793790893554688, 9.601785888671875, 9.43538818359375, 8.9797802734375, 8.259169921875, 8.119304809570313, 7.700317993164062, 7.406243896484375, 7.3227734375, 7.37209716796875, 8.636076049804688, 7.58994873046875, 7.977186279296875, 7.97035888671875, 7.458926391601563, 7.116290283203125, 7.3921881103515625, 7.15197509765625, 6.84713623046875, 7.180955200195313, 7.411206665039063]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:42:27.772297>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_7.127088012695313.pt\n",
      "[9.930101928710938, 9.887689819335938, 9.772727661132812, 9.323929443359376, 8.031561279296875, 8.0758837890625, 8.177774047851562, 7.456638793945313, 8.459349975585937, 7.622722778320313, 7.8839501953125, 7.788573608398438, 7.581856689453125, 7.759884643554687, 7.366223754882813, 7.367178344726563, 7.739659423828125, 7.765059204101562, 8.278667602539063, 7.127088012695313, 7.238629760742188, 7.163555297851563, 7.559026489257812, 7.3310546875, 7.5598602294921875]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:42:53.797881>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_200_words_20449_loss_6.860128173828125.pt\n",
      "[9.923715209960937, 9.844818115234375, 9.149893798828124, 7.909221801757813, 8.486468505859374, 8.213704223632812, 7.821743774414062, 7.836834106445313, 7.5877099609375, 7.6740478515625, 7.721328125, 7.958433837890625, 7.693580322265625, 7.456181640625, 7.260396118164063, 7.530205078125, 7.781572265625, 7.753279418945312, 7.382859497070313, 7.493787841796875, 6.914022216796875, 7.535693969726562, 6.860128173828125, 7.285787353515625, 7.523977661132813]\n",
      "epochs: 25\n",
      "chunk length: 200\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:43:19.579426>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_8.328232421875.pt\n",
      "[9.933775227864583, 9.921177571614583, 9.921648763020833, 9.90753662109375, 9.89536865234375, 9.8950732421875, 9.87568603515625, 9.871595865885416, 9.82936279296875, 9.83477294921875, 9.81054443359375, 9.78201416015625, 9.7510595703125, 9.729352213541667, 9.680970865885417, 9.585721028645834, 9.582718912760416, 9.43410888671875, 9.34018798828125, 9.161883951822917, 9.148255208333333, 8.75424560546875, 8.785393880208334, 8.382513020833333, 8.328232421875]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:43:54.003402>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.210181477864583.pt\n",
      "[9.915138346354167, 9.898460286458333, 9.874292805989583, 9.831522623697916, 9.7433935546875, 9.683648274739584, 9.399381510416667, 9.109986165364583, 8.659673665364583, 8.039742024739583, 7.79325439453125, 7.246717936197917, 7.674736328125, 7.226697591145833, 7.470873209635417, 6.863721516927083, 7.702198079427084, 7.858854166666666, 7.3708935546875, 7.782618815104167, 6.210181477864583, 7.7119873046875, 7.669796549479167, 7.30440185546875, 7.721642252604167]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:44:28.943402>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.897578938802083.pt\n",
      "[9.92912353515625, 9.889508463541667, 9.818346354166666, 9.647569986979166, 9.272835286458333, 8.7429443359375, 8.150526529947916, 7.7456453450520835, 7.66766845703125, 7.311879069010416, 7.408218587239583, 7.669307454427083, 8.096634928385416, 7.48461181640625, 7.582327473958333, 8.063889973958334, 7.4930411783854165, 7.52023681640625, 7.844412434895833, 7.866612955729167, 7.3605810546875, 7.401739095052084, 7.383683268229166, 6.897578938802083, 7.048168131510416]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 64\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:45:03.430431>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.89361572265625.pt\n",
      "[9.928033854166667, 9.919427083333334, 9.888435872395833, 9.880400390625, 9.862112630208333, 9.851288248697916, 9.816979166666666, 9.724866536458334, 9.6579296875, 9.557464192708334, 9.403368326822916, 9.195235188802084, 8.8430712890625, 8.574192708333333, 8.104973958333334, 7.724942220052084, 7.614877115885417, 7.77238525390625, 7.25764404296875, 7.725122884114583, 7.479811197916667, 7.231051432291666, 7.324471028645833, 6.89361572265625, 7.01550048828125]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:45:37.944891>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.0205696614583335.pt\n",
      "[9.908595377604167, 9.882664388020833, 9.815813802083333, 9.671825358072917, 9.3703369140625, 8.65240966796875, 7.7462109375, 7.695586751302083, 7.741419270833333, 8.280677083333334, 7.816569010416667, 7.94474853515625, 8.1326123046875, 7.656481119791667, 7.446141764322917, 7.31991943359375, 7.059989420572917, 7.251387532552084, 7.527393391927084, 7.21318603515625, 7.518199055989584, 7.305996907552084, 7.0205696614583335, 7.095281575520834, 7.398340657552083]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:46:12.161844>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.971507161458334.pt\n",
      "[9.922509765625, 9.858644205729167, 9.675861002604167, 8.860562337239584, 7.885967610677083, 7.344182942708334, 8.039574381510416, 7.893368326822917, 7.56469482421875, 7.875697428385417, 7.571954752604166, 7.7091072591145835, 7.779205729166667, 7.270292154947916, 7.622481282552084, 7.39052734375, 7.10728515625, 7.221328125, 7.0254443359375, 6.971507161458334, 7.166883951822917, 7.258478190104166, 7.6050797526041665, 7.308595377604167, 7.30230712890625]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 128\n",
      "learning rate: 0.005\n",
      "<2023-05-30 01:46:46.456098>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.640320231119792.pt\n",
      "[9.92359375, 9.909178059895833, 9.892430013020833, 9.864368489583333, 9.830059407552083, 9.748473307291667, 9.57461181640625, 9.20123046875, 8.863514811197916, 8.1803857421875, 7.715646158854167, 7.51607177734375, 7.93205078125, 7.6754158528645835, 7.43120361328125, 7.614546712239584, 7.328143717447917, 7.4168994140625, 7.711612141927083, 6.640320231119792, 6.871509602864584, 7.3331884765625, 7.075045572916666, 7.071436360677083, 7.597696940104167]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.001\n",
      "<2023-05-30 01:47:26.225466>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_7.0549169921875.pt\n",
      "[9.92349853515625, 9.862097981770834, 9.651559244791667, 8.714979654947916, 7.7714005533854165, 8.083710123697916, 7.700673014322916, 7.853146158854167, 7.527156575520833, 7.69097412109375, 7.47832275390625, 7.58240966796875, 7.613544108072917, 7.714778645833333, 7.43107666015625, 7.6035400390625, 7.965411783854167, 7.631216634114583, 7.136080729166666, 7.381044108072917, 7.279404296875, 7.237367350260417, 7.589108072916667, 7.0549169921875, 7.410061848958334]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.003\n",
      "<2023-05-30 01:48:05.487530>starting training\n",
      "saving model at ./models/bidir_lstm_chunk_300_words_20449_loss_6.7342138671875.pt\n",
      "[9.93094482421875, 9.84010009765625, 8.921621907552083, 7.827179361979167, 7.557808430989583, 8.197278645833334, 7.97967041015625, 8.09111328125, 7.447285970052083, 7.584402669270833, 7.425731608072916, 7.303958333333333, 7.653422037760417, 8.18096923828125, 7.564947102864584, 7.54843994140625, 7.292340494791667, 7.065520833333333, 7.4348543294270835, 6.967037760416667, 7.50792724609375, 6.936597493489583, 7.31605712890625, 6.7342138671875, 7.707784016927083]\n",
      "epochs: 25\n",
      "chunk length: 300\n",
      "hidden_size: 256\n",
      "learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "# default parameters for generator: \n",
    "# chunk_length=200, num_epochs=4000, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002\n",
    "\n",
    "epoch_numbers = [50, 25] # to save time\n",
    "# epoch_numbers = [500, 1000, 2000, 3000, 5000, 7500]\n",
    "chunk_lengths = [50, 200, 300]\n",
    "batch_sizes = [1, 5, 10]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "layer_numbers = [2, 4] # taken out due to mismatch on layer input/output\n",
    "learning_rates = [0.001, 0.003, 0.005]\n",
    "\n",
    "\n",
    "# gentext = Generator()\n",
    "# gentext.train()\n",
    "\n",
    "# simple naive tuning\n",
    "tuning_results = []\n",
    "for epoch in epoch_numbers:\n",
    "    for batch_size in batch_sizes:\n",
    "        for chunk in chunk_lengths:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for learning_rate in learning_rates:\n",
    "                    gen = Generator(chunk_length=chunk, num_epochs=epoch, batch_size=batch_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "                    losses_to_plot = gen.train()\n",
    "                    print(losses_to_plot)\n",
    "                    run_params = f'epoc_{epoch}_chunk_{chunk}_hidden_s_{hidden_size}_lr_{learning_rate}'\n",
    "                    labelx = f'epochs/{gen.plot_every}'\n",
    "                    plt.figure()\n",
    "                    plt.xlabel(labelx)\n",
    "                    plt.ylabel('loss')\n",
    "                    plt.text(2, 10, run_params)\n",
    "                    plt.plot(losses_to_plot)\n",
    "                    file_name = f'./images/{run_params}.svg'\n",
    "                    plt.savefig(file_name, format='svg')\n",
    "                    plt.close()\n",
    "\n",
    "                    # print('-'*40)\n",
    "                    # number_of_tests = 1000\n",
    "                    # correct = 0\n",
    "                    # for i in range(number_of_tests):\n",
    "                    #     test_words_list = gen.get_random_chunk(4)\n",
    "                    #     input =' '.join(test_words_list[:-1])\n",
    "                    #     target = ' '.join(test_words_list)\n",
    "                    #     result = gen.generate(input)\n",
    "                    #     if target == result:\n",
    "                    #         correct += 1\n",
    "                    # print('-'*40)\n",
    "                    test_res = f'epochs: {epoch}\\nchunk length: {chunk}\\nhidden_size: {hidden_size}\\nlearning rate: {learning_rate}'\n",
    "                    print(test_res)\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the saved plots, it looks like the following parameters are the best:\n",
    "\n",
    "# chunk_size = 200\n",
    "# hidden_size = 128\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# so we're going to go with them\n",
    "\n",
    "gen = Generator(chunk_length=200, num_epochs=5000, hidden_size=128, learning_rate=0.001)\n",
    "gen.train()\n",
    "\n",
    "\n",
    "temperatures = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    stmt = ' '.join(gentext.generate(initial_str='i would like', predict_len=150, temperature=temperature)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "    print(f'temperature: {temperature}\\nstatement:\\n{stmt}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
