{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning - exam project\n",
    "Af: Andreas Nørregaard Pedersen & Michael Rulle\n",
    "## Fake News Text Generator\n",
    "I dette projekt er gruppen blevet hyret til at få produceret fake news til danske medier, får at skabe ravage i det danske samfund. Projektet vil bestå af en Model der vil generere tekst ud fra en start sætning. Tekst generering er svært at få en maskine til at forstå, da tekst har kontekst bagved(en sætning af ord af bogstaver). \n",
    "\n",
    "## Dataindsamling\n",
    "Der skulle indhentes data, hvilket blev taget fra kaggle: dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset.\n",
    "I dette datasæt skulle der være nyhedsartikler der både var rigtige og falske, hvorfra tekst modellen skulle have forskellige scenarier, der er kun brugt rigtige artikler.\n",
    "Herfra\n",
    "\n",
    "## Prepare the data\n",
    "Dataene renses for gentagelser i starten af hver artikel eks. bynavn (nyhedsbureau) og tegnsætning erstattes af tokens.\n",
    "Den længste artikel bliver fundet, og de resterende artikler bliver fyldt ud med padding, for at nå samme antal tokens.\n",
    "\n",
    "## Model\n",
    "Vi har valgt at bruge LSTM grundet dens hukommelse, og oven i LSTM, har vi valgt at bruge Bidirectional, for at få den til at huske sekvensen, altså mere kontekst i en sætning.\n",
    "\n",
    "## Tuning af hyper parametre\n",
    "Vi testede forskellige parameter for at finde den bedste nøjagtighed, med de forskellige parameter.\n",
    "Herfra trænede vi modellen med de fundne bedste hyper parametre.\n",
    "Dropout probability er ikke bygget ind på tidspunkt for aflevering, men kan præsenteres til eksaminationen. Derfor er dropout probability=0\n",
    "\n",
    "\n",
    "## Evaluering\n",
    "Visualisering af nøjagtighed med plots, og lave en forudsigelse på generering af tekst. \n",
    "\n",
    "## How to do\n",
    "Der skal være 3 mapper til hhv. \"data\", \"models\" og \"images\". I data mappen skal True.csv filen fra datasættet være i. Så kan i køre notebook’en.\n",
    "\n",
    "\n",
    "Andreas og Michael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire project can be found at: https://github.com/mrulle/deep_learning_exam_project\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 2000\n",
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/True.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# functions to clean data\n",
    "\n",
    "# filter out first part containing CITY (news agency) and separator \"-\"\n",
    "\n",
    "# to be replaced: “ ”\n",
    "\n",
    "# search and replace regex\n",
    "double_quotes = r'“|”'\n",
    "single_quotes = r'’|‘'\n",
    "backslashes = r'\\\\'\n",
    "multiple_whitespace = r'\\t|\\v|\\f| '\n",
    "double_quotes = re.compile(double_quotes)\n",
    "single_quotes = re.compile(single_quotes)\n",
    "backslashes = re.compile(backslashes)\n",
    "multiple_whitespace = re.compile(multiple_whitespace)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    txt = row[1].lower()\n",
    "    txt = txt[txt.find('-')+1:].lstrip()\n",
    "    txt = double_quotes.sub('\"', txt)\n",
    "    txt = txt.replace(\"’\", \"\")\n",
    "    txt = txt.replace(\"‘\", \"\")\n",
    "    txt = multiple_whitespace.sub(' ', txt)\n",
    "    # remove everything before the first dash (news agency and city)\n",
    "    txt = txt[txt.find('-')+1:]\n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD> '\n",
    "    token[','] = ' <COMMA> '\n",
    "    token['\"'] = ' <QUOTATION_MARK> '\n",
    "    token[':'] = ' <COLON>'\n",
    "    token[';'] = ' <SEMICOLON> '\n",
    "    token['!'] = ' <EXCLAIMATION_MARK> '\n",
    "    token['?'] = ' <QUESTION_MARK> '\n",
    "    token['('] = ' <LEFT_PAREN> '\n",
    "    token[')'] = ' <RIGHT_PAREN> '\n",
    "    token['-'] = ' <QUESTION_MARK> '\n",
    "    token['\\n'] = ' <NEW_LINE> '\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply padding for all articles to match length of longest article\n",
    "# this turned out to be unnecessary due to the get_random_batch and\n",
    "def pad_to_max(tokenized, max):\n",
    "    padding_length = max - len(tokenized)\n",
    "    if padding_length == 0:\n",
    "        return tokenized\n",
    "    padding = ['<pad>' for i in range(padding_length)]\n",
    "    tokenized.extend(padding)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:corpus_size]\n",
    "df = df.astype({'text': 'string'})\n",
    "\n",
    "df['text'] = df.apply(clean_data, axis=1)\n",
    "print(df['text'][0])\n",
    "articles = df['text'].values.tolist()\n",
    "\n",
    "longest_article = 0\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    for key, token in token_dict.items():\n",
    "        article = article.replace(key, token)\n",
    "    article = article.lower()\n",
    "    article = article.split()\n",
    "    if len(article) > longest_article:\n",
    "        longest_article = len(article)\n",
    "    tokenized_articles.append(article)\n",
    "\n",
    "\n",
    "print(f'longest article contains {longest_article} tokens')\n",
    "\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "for tokens in tokenized_articles:\n",
    "    tokens = pad_to_max(tokens, longest_article)\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "print(f'there are {len(unique_tokens)} unique tokens')\n",
    "\n",
    "print(\n",
    "    f'articles equal length: {len(tokenized_articles[0])==len(tokenized_articles[1])}')\n",
    "\n",
    "articles = [' '.join(art) for art in tokenized_articles]\n",
    "\n",
    "print(articles[0])\n",
    "print(tokenized_articles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(tokenized_articles, file)\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(unique_tokens, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words\n",
    "all_words = []\n",
    "\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'rb') as file:\n",
    "    all_words = pickle.load(file)\n",
    "\n",
    "all_words.append(' ')\n",
    "vocab_length = len(all_words)\n",
    "print(f'number of words: {vocab_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "corpus = []\n",
    "file_content = []\n",
    "with open('./data/tokenized_articles.dat', 'rb') as file:\n",
    "    file_content = pickle.load(file)\n",
    "\n",
    "# this approach uses embedding, and therefore doesn't need padding so we remove it from the prepared data\n",
    "print(len(file_content))\n",
    "for article in file_content:\n",
    "    corpus.extend([word.strip() for word in article if word not in ['<pad>', ' ']])\n",
    "\n",
    "print(len(corpus))\n",
    "# print(corpus[-30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=self.dropout_rate, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_size * num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        \n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def save(f_path):\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002, dropout_rate=0.2):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.chunk_len = chunk_length\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = self.num_epochs // 20 or 1\n",
    "        self.plot_every = self.num_epochs // 40 or 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "\n",
    "\n",
    "    def word_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_words.index(string[c])\n",
    "        return tensor\n",
    "    \n",
    "    def get_random_chunk(self, chunk_length):\n",
    "        start_idx = random.randint(0, len(corpus) - chunk_length)\n",
    "        end_idx = start_idx + chunk_length + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        return text_str\n",
    "\n",
    "    # method should be split to get random string, and convert to tensors\n",
    "    def get_random_batch(self, chunk_length):\n",
    "        start_idx = random.randint(0, len(corpus) - chunk_length)\n",
    "        end_idx = start_idx + chunk_length + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, chunk_length)\n",
    "        text_target = torch.zeros(self.batch_size, chunk_length)\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.word_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.word_tensor(text_str[1:])\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "\n",
    "    def generate(self, initial_str='the president is dead', predict_len=200, temperature=0.85):\n",
    "        initial_words = initial_str.split(' ')\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.word_tensor(initial_words)\n",
    "        predicted = initial_words\n",
    "        \n",
    "        for p in range(len(initial_words) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_word = initial_input[-1]\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_word.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_word = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_word = [all_words[top_word]]\n",
    "            predicted.extend(predicted_word)\n",
    "            last_word = self.word_tensor(predicted_word)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(vocab_length, self.hidden_size, self.num_layers, vocab_length, self.dropout_rate).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f'<{datetime.now()}>starting training')\n",
    "        lowest_loss = 100.0 # just a high value, should not be lower than 10\n",
    "        all_losses = []\n",
    "        accumulated_losses = 0\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            input, target = self.get_random_batch(self.chunk_len)\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(input[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "            accumulated_losses += loss\n",
    "            if loss < lowest_loss:\n",
    "                self.best_model = self.rnn.state_dict() # set the model with least loss as the best, so it can be saved\n",
    "                # print(f'<{datetime.now()}> better model found after {epoch}/{self.num_epochs} epochs with loss: {loss}')\n",
    "                lowest_loss = loss\n",
    "            if epoch % self.plot_every == 0:\n",
    "                all_losses.append(accumulated_losses / self.plot_every)\n",
    "                accumulated_losses = 0\n",
    "            if epoch % self.print_every == 0: # enable below lines, if you wish to see print statements on progress\n",
    "                pass\n",
    "                # print(f'\\n\\n<{datetime.now()}> | epoch: {epoch}/{self.num_epochs} | loss: {loss}')\n",
    "                # print(self.generate())\n",
    "        file_path = f'./models/epoc_{self.num_epochs}_chunk_{self.chunk_len}_hiddensize_{self.hidden_size}_lr_{self.lr}__loss_{lowest_loss}.pt'\n",
    "        print(f'saving model at {file_path}')\n",
    "        torch.save(self.best_model, file_path)\n",
    "        return all_losses\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = 'chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002, dropout_rate=0.2'\n",
    "gen = Generator() # with default parameters: chunk_length=200, num_epochs=500, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002, dropout_rate=0.2\n",
    "%matplotlib inline\n",
    "losses_to_plot = gen.train()\n",
    "plt.figure()\n",
    "plt.xlabel(f'epochs/{gen.plot_every}')\n",
    "plt.ylabel('loss')\n",
    "plt.text(2, 10, run_params)\n",
    "plt.plot(losses_to_plot)\n",
    "file_name = f'./images/test_run.svg'\n",
    "plt.savefig(file_name, format='svg')\n",
    "\n",
    "temperatures = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    stmt = ' '.join(gen.generate(initial_str='i would like', predict_len=150, temperature=temperature)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "    print(f'temperature: {temperature}\\nstatement:\\n{stmt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters for generator: \n",
    "# chunk_length=200, num_epochs=4000, batch_size=1, hidden_size=256, num_layers=2, learning_rate=0.002\n",
    "\n",
    "epoch_numbers = [50, 25] # to save time\n",
    "chunk_lengths = [50, 200, 300]\n",
    "batch_sizes = [1, 5, 10]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "dropout_probs = [0.1, 0.2, 0.4] # not implemented in model at time of handin\n",
    "layer_numbers = [2, 4] # taken out due to mismatch on layer input/output\n",
    "learning_rates = [0.001, 0.003, 0.005]\n",
    "\n",
    "\n",
    "# simple naive tuning\n",
    "tuning_results = []\n",
    "for epoch in epoch_numbers:\n",
    "    for batch_size in batch_sizes:\n",
    "        for chunk in chunk_lengths:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for dropout_prob in dropout_probs:\n",
    "                    for learning_rate in learning_rates:\n",
    "                        gen = Generator(chunk_length=chunk, num_epochs=epoch, batch_size=batch_size, hidden_size=hidden_size, learning_rate=learning_rate, dropout_rate=dropout_prob)\n",
    "                        losses_to_plot = gen.train()\n",
    "                        print(losses_to_plot)\n",
    "                        run_params = f'epoc_{epoch}_chunk_{chunk}_hidden_s_{hidden_size}_lr_{learning_rate}'\n",
    "                        labelx = f'epochs/{gen.plot_every}'\n",
    "                        plt.figure()\n",
    "                        plt.xlabel(labelx)\n",
    "                        plt.ylabel('loss')\n",
    "                        plt.text(2, 10, run_params)\n",
    "                        plt.plot(losses_to_plot)\n",
    "                        file_name = f'./images/{run_params}.svg'\n",
    "                        plt.savefig(file_name, format='svg')\n",
    "                        plt.close()\n",
    "\n",
    "                        test_res = f'epochs: {epoch}\\nchunk length: {chunk}\\nhidden_size: {hidden_size}\\nlearning rate: {learning_rate}'\n",
    "                        print(test_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the saved plots, it looks like the following parameters are the best:\n",
    "\n",
    "# chunk_size = 200\n",
    "# hidden_size = 128\n",
    "# learning_rate = 0.001\n",
    "# dropout_rate = 0.2 - this wasnt part of the tuning, new tuning is reqired - we consider 0.2 as a qualified guess\n",
    "\n",
    "# so we're going to go with them\n",
    "\n",
    "gen = Generator(chunk_length=200, num_epochs=5000, hidden_size=128, learning_rate=0.001, dropout_rate=0.2)\n",
    "gen.train()\n",
    "\n",
    "\n",
    "temperatures = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    stmt = ' '.join(gen.generate(initial_str='i would like', predict_len=150, temperature=temperature)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "    print(f'temperature: {temperature}\\nstatement:\\n{stmt}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
