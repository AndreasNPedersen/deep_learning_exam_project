{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 2000\n",
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/True.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# functions to clean data\n",
    "\n",
    "# filter out first part containing CITY (news agency) and separator \"-\"\n",
    "\n",
    "# to be replaced: “ ”\n",
    "\n",
    "# search and replace regex\n",
    "double_quotes = r'“|”'\n",
    "single_quotes = r'’|‘'\n",
    "backslashes = r'\\\\'\n",
    "multiple_whitespace = r'\\t|\\v|\\f| '\n",
    "double_quotes = re.compile(double_quotes)\n",
    "single_quotes = re.compile(single_quotes)\n",
    "backslashes = re.compile(backslashes)\n",
    "multiple_whitespace = re.compile(multiple_whitespace)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    txt = row[1].lower()\n",
    "    txt = txt[txt.find('-')+1:].lstrip()\n",
    "    txt = double_quotes.sub('\"', txt)\n",
    "    txt = txt.replace(\"’\", \"\")\n",
    "    txt = txt.replace(\"‘\", \"\")\n",
    "    txt = multiple_whitespace.sub(' ', txt)\n",
    "    # remove everything before the first dash (news agency and city)\n",
    "    txt = txt[txt.find('-')+1:]\n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD> '\n",
    "    token[','] = ' <COMMA> '\n",
    "    token['\"'] = ' <QUOTATION_MARK> '\n",
    "    token[':'] = ' <COLON>'\n",
    "    token[';'] = ' <SEMICOLON> '\n",
    "    token['!'] = ' <EXCLAIMATION_MARK> '\n",
    "    token['?'] = ' <QUESTION_MARK> '\n",
    "    token['('] = ' <LEFT_PAREN> '\n",
    "    token[')'] = ' <RIGHT_PAREN> '\n",
    "    token['-'] = ' <QUESTION_MARK> '\n",
    "    token['\\n'] = ' <NEW_LINE> '\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max(tokenized, max):\n",
    "    padding_length = max - len(tokenized)\n",
    "    if padding_length == 0:\n",
    "        return tokenized\n",
    "    padding = ['<pad>' for i in range(padding_length)]\n",
    "    tokenized.extend(padding)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:corpus_size]\n",
    "df = df.astype({'text': 'string'})\n",
    "\n",
    "df['text'] = df.apply(clean_data, axis=1)\n",
    "print(df['text'][0])\n",
    "articles = df['text'].values.tolist()\n",
    "\n",
    "longest_article = 0\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    for key, token in token_dict.items():\n",
    "        article = article.replace(key, token)\n",
    "    article = article.lower()\n",
    "    article = article.split()\n",
    "    if len(article) > longest_article:\n",
    "        longest_article = len(article)\n",
    "    tokenized_articles.append(article)\n",
    "# for key, token in token_dict.items():\n",
    "#     articles[0] = article[0].replace(key, token)\n",
    "\n",
    "print(f'longest article contains {longest_article} tokens')\n",
    "\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "for tokens in tokenized_articles:\n",
    "    tokens = pad_to_max(tokens, longest_article)\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "print(f'there are {len(unique_tokens)} unique tokens')\n",
    "\n",
    "print(\n",
    "    f'articles equal length: {len(tokenized_articles[0])==len(tokenized_articles[1])}')\n",
    "\n",
    "articles = [' '.join(art) for art in tokenized_articles]\n",
    "\n",
    "print(articles[0])\n",
    "print(tokenized_articles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(tokenized_articles, file)\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'wb') as file:\n",
    "    pickle.dump(unique_tokens, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words\n",
    "all_words = []\n",
    "\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'rb') as file:\n",
    "    all_words = pickle.load(file)\n",
    "\n",
    "all_words.append(' ')\n",
    "vocab_length = len(all_words)\n",
    "print(f'number of words: {vocab_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "corpus = []\n",
    "file_content = []\n",
    "with open('./data/tokenized_articles.dat', 'rb') as file:\n",
    "    file_content = pickle.load(file)\n",
    "\n",
    "# this approach uses embedding, and therefore doesn't need padding so we remove it from the prepared data\n",
    "print(len(file_content))\n",
    "for article in file_content:\n",
    "    corpus.extend([word.strip() for word in article if word not in ['<pad>', ' ']])\n",
    "\n",
    "print(len(corpus))\n",
    "# print(corpus[-30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_size * num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def save(f_path):\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self):\n",
    "        self.chunk_len = 300\n",
    "        self.num_epochs = 15000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = self.num_epochs // 20 or 1\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.lr = 0.001\n",
    "\n",
    "\n",
    "    def word_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_words.index(string[c])\n",
    "        return tensor\n",
    "\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(corpus) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.word_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.word_tensor(text_str[1:])\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "\n",
    "    def generate(self, initial_str='the president is dead', predict_len=200, temperature=0.85):\n",
    "        initial_words = initial_str.split(' ')\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.word_tensor(initial_words)\n",
    "        predicted = initial_words\n",
    "        \n",
    "        for p in range(len(initial_words) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_word = initial_input[-1]\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_word.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_word = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_word = [all_words[top_word]]\n",
    "            predicted.extend(predicted_word)\n",
    "            last_word = self.word_tensor(predicted_word)\n",
    "            \n",
    "\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(vocab_length, self.hidden_size, self.num_layers, vocab_length).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f'<{datetime.now()}>starting training')\n",
    "        lowest_loss = 100.0\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            input, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(input[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "            if loss < lowest_loss:\n",
    "                self.best_model = self.rnn.state_dict()\n",
    "                print(f'<{datetime.now()}> better model found after {epoch}/{self.num_epochs} epochs with loss: {loss}')\n",
    "                lowest_loss = loss\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f'\\n\\n<{datetime.now()}> | epoch: {epoch}/{self.num_epochs} | loss: {loss}')\n",
    "                # print(self.generate())\n",
    "        file_path = f'./models/bidir_lstm_chunk_{self.chunk_len}_words_{vocab_length}_loss_{lowest_loss}.pt'\n",
    "        print(f'saving model at {file_path}')\n",
    "        torch.save(self.best_model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentext = Generator()\n",
    "gentext.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ' '.join(gentext.generate(initial_str='i would like', predict_len=400, temperature=0.2)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "s2 = ' '.join(gentext.generate(initial_str='i would like', predict_len=400, temperature=0.4)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "s3 = ' '.join(gentext.generate(initial_str='i would like', predict_len=400, temperature=0.6)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "s4 = ' '.join(gentext.generate(initial_str='i would like', predict_len=400, temperature=0.8)).replace('<quotation_mark>', '\"').replace(' <question_mark>','?').replace(' <comma>', ',').replace(' <period>', '.')\n",
    "\n",
    "statements = [s1, s2, s3, s4]\n",
    "for statement in statements:\n",
    "    print(statement)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
