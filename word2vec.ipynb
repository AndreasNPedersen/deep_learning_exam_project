{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchtext as tt\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# 3 Words Sentence (to semplify)\n",
    "# All them form our text corpus\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sentences = ''\n",
    "play = False\n",
    "\n",
    "start_time = datetime.fromtimestamp(time.time())\n",
    "print(f'<{start_time}> training started on {device}')\n",
    "\n",
    "if play:\n",
    "    sentences = [ \"i like dog\", \"i like cat\", \"i like animal\", \n",
    "              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n",
    "              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n",
    "              \"apple i movie\", \"book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/True.csv\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokenizer = tt.data.utils.get_tokenizer('spacy')\n",
    "    np_array = df['text'].values\n",
    "    txt_array = np_array.tolist()\n",
    "    sentences = '\\n'.join(txt_array)\n",
    "    sentences = sentences.lower()\n",
    "\n",
    "number_match = re.compile('\\b.*[0-9].*\\b')\n",
    "punctuation_match = re.compile('\\b.*(\\.|\\\\|\\/|,\\/|\\(|\\)).*\\b')\n",
    "\n",
    "\n",
    "print(sentences[0:100])\n",
    "for sentence in sentences:\n",
    "    sentence = number_match.sub(repl=\"\", string=sentence)\n",
    "    sentence = punctuation_match.sub(repl=\"\", string=sentence)\n",
    "\n",
    "print(sentences[0:100])\n",
    "print(f'number of articles: {len(txt_array)}')\n",
    "print(f'length of sentences: {len(sentences)}')\n",
    "\n",
    "\n",
    "# list all the words present in our corpus\n",
    "word_sequence = tokenizer(sentences)\n",
    "word_list = list(set(word_sequence))\n",
    "\n",
    "print(f'{len(word_list)} unique tokens')\n",
    "\n",
    "# word_list = [word for word in word_list if not number_match.match(word)]\n",
    "# print(f'{len(word_list)} unique tokens after removing numbers')\n",
    "# word_list = [word for word in word_list if not punctuation_match.match(word)]\n",
    "# print(f'{len(word_list)} unique tokens after removing special characters')\n",
    "\n",
    "\n",
    "# print(word_sequence)\n",
    "# build the vocabulary\n",
    "print(f'final length of word list: {len(word_list)}')\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# print(word_dict)\n",
    "\n",
    "# Word2Vec Parameter\n",
    "batch_size = 20  # To show 2 dim embedding graph\n",
    "embedding_size = 2  # To show 2 dim embedding graph\n",
    "voc_size = len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input word\n",
    "# j = 1\n",
    "# print(\"Input word : \")\n",
    "# print(word_sequence[j], word_dict[word_sequence[j]])\n",
    "\n",
    "# context words\n",
    "# print(\"Context words : \")\n",
    "# print(word_sequence[j - 1], word_sequence[j + 1])\n",
    "# print([word_dict[word_sequence[j - 1]], word_dict[word_sequence[j + 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make skip gram of one size window\n",
    "window_size = 2\n",
    "skip_grams = []\n",
    "for i in range(1, len(word_sequence) - window_size):\n",
    "    input = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - window_size]], word_dict[word_sequence[i + window_size]]]\n",
    "    for w in context:\n",
    "        skip_grams.append([input, w])\n",
    "\n",
    "\n",
    "#lets plot some data\n",
    "# skip_grams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(172)\n",
    "\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        # one-hot encoding of words\n",
    "        random_inputs.append(np.eye(voc_size)[data[i][0]])  # input\n",
    "        random_labels.append(data[i][1])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "# random_batch(skip_grams[:6], size=3)\n",
    "\n",
    "# inputs: like , i, dog , context: i, dog, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        # parameters between -1 and + 1\n",
    "        self.W = nn.Parameter(-2 * torch.rand(voc_size, embedding_size) + 1).type(dtype).to(device) # voc_size -> embedding_size Weight\n",
    "        self.V = nn.Parameter(-2 * torch.rand(embedding_size, voc_size) + 1).type(dtype).to(device) # embedding_size -> voc_size Weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
    "        output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
    "        #return output_layer \n",
    "        return output_layer\n",
    "\n",
    "model = Word2Vec()\n",
    "# Set the model in train mode\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Softmax (for multi-class classification problems) is already included\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 1\n",
    "print_freq = (epochs // 20) or 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    input_batch, target_batch = random_batch(skip_grams, batch_size)\n",
    "\n",
    "    # new_tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "    input_batch = torch.Tensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1)%print_freq == 0:\n",
    "        epoch_time = datetime.fromtimestamp(time.time())\n",
    "        print('<', epoch_time, '> Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned W\n",
    "W, _= model.parameters()\n",
    "print(W.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'./models/w2v_wsize_{window_size}_epocs_{epochs}.model'\n",
    "torch.save(model.state_dict(), file_path)\n",
    "\n",
    "end_time = datetime.fromtimestamp(time.time())\n",
    "print(f'training finished at: {end_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, word in enumerate(word_list):\n",
    "#     W, _= model.parameters()\n",
    "#     W = W.detach()\n",
    "#     x,y = float(W[i][0]), float(W[i][1])\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
