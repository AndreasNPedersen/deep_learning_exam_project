{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchtext as tt\n",
    "import spacy\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# 3 Words Sentence (to semplify)\n",
    "# All them form our text corpus\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sentences = ''\n",
    "play = False\n",
    "print(f'training started on {device}')\n",
    "\n",
    "if play:\n",
    "    sentences = [ \"i like dog\", \"i like cat\", \"i like animal\", \n",
    "              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n",
    "              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n",
    "              \"apple i movie\", \"book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/True.csv\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokenizer = tt.data.utils.get_tokenizer('spacy')\n",
    "    np_array = df['text'].values\n",
    "    txt_array = np_array.tolist()\n",
    "    sentences = '\\n'.join(txt_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list all the words present in our corpus\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "print(word_sequence )\n",
    "# build the vocabulary\n",
    "word_list = list(set(word_sequence))\n",
    "print(word_list)\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "print(word_dict)\n",
    "\n",
    "# Word2Vec Parameter\n",
    "batch_size = 20  # To show 2 dim embedding graph\n",
    "embedding_size = 2  # To show 2 dim embedding graph\n",
    "voc_size = len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input word\n",
    "j = 1\n",
    "print(\"Input word : \")\n",
    "print(word_sequence[j], word_dict[word_sequence[j]])\n",
    "\n",
    "# context words\n",
    "print(\"Context words : \")\n",
    "print(word_sequence[j - 1], word_sequence[j + 1])\n",
    "print([word_dict[word_sequence[j - 1]], word_dict[word_sequence[j + 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make skip gram of one size window\n",
    "skip_grams = []\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    input = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "\n",
    "    for w in context:\n",
    "        skip_grams.append([input, w])\n",
    "\n",
    "\n",
    "#lets plot some data\n",
    "skip_grams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(172)\n",
    "\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        # one-hot encoding of words\n",
    "        random_inputs.append(np.eye(voc_size)[data[i][0]])  # input\n",
    "        random_labels.append(data[i][1])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "random_batch(skip_grams[:6], size=3)\n",
    "\n",
    "# inputs: like , i, dog , context: i, dog, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        # parameters between -1 and + 1\n",
    "        self.W = nn.Parameter(-2 * torch.rand(voc_size, embedding_size) + 1).type(dtype).to(device) # voc_size -> embedding_size Weight\n",
    "        self.V = nn.Parameter(-2 * torch.rand(embedding_size, voc_size) + 1).type(dtype).to(device) # embedding_size -> voc_size Weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
    "        output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
    "        #return output_layer \n",
    "        return output_layer\n",
    "\n",
    "model = Word2Vec()\n",
    "# Set the model in train mode\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Softmax (for multi-class classification problems) is already included\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    input_batch, target_batch = random_batch(skip_grams, batch_size)\n",
    "\n",
    "    # new_tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "    input_batch = torch.Tensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1)%1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned W\n",
    "W, _= model.parameters()\n",
    "print(W.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/word2vec.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, word in enumerate(word_list):\n",
    "#     W, _= model.parameters()\n",
    "#     W = W.detach()\n",
    "#     x,y = float(W[i][0]), float(W[i][1])\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
