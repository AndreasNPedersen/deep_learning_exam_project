{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/True.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# reduced corpus size due to hardware and time constraints\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "corpus_size = 2000\n",
    "epochs = 3000\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# functions to clean data\n",
    "\n",
    "# filter out first part containing CITY (news agency) and separator \"-\"\n",
    "\n",
    "# to be replaced: “ ”\n",
    "\n",
    "# search and replace regex\n",
    "double_quotes = r'“|”'\n",
    "single_quotes = r'’|‘'\n",
    "backslashes = r'\\\\'\n",
    "multiple_whitespace = r'\\t|\\v|\\f| '\n",
    "double_quotes = re.compile(double_quotes)\n",
    "single_quotes = re.compile(single_quotes)\n",
    "backslashes = re.compile(backslashes)\n",
    "multiple_whitespace = re.compile(multiple_whitespace)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    txt = row[1].lower()\n",
    "    txt = txt[txt.find('-')+1:].lstrip()\n",
    "    txt = double_quotes.sub('\"', txt)\n",
    "    txt = txt.replace(\"’\", \"\")\n",
    "    txt = txt.replace(\"‘\", \"\")\n",
    "    txt = multiple_whitespace.sub(' ', txt)\n",
    "    # remove everything before the first dash (news agency and city)\n",
    "    txt = txt[txt.find('-')+1:]\n",
    "    \n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD> '\n",
    "    token[','] = ' <COMMA> '\n",
    "    token['\"'] = ' <QUOTATION_MARK> '\n",
    "    token[':'] = ' <COLON>'\n",
    "    token[';'] = ' <SEMICOLON> '\n",
    "    token['!'] = ' <EXCLAIMATION_MARK> '\n",
    "    token['?'] = ' <QUESTION_MARK> '\n",
    "    token['('] = ' <LEFT_PAREN> '\n",
    "    token[')'] = ' <RIGHT_PAREN> '\n",
    "    token['-'] = ' <QUESTION_MARK> '\n",
    "    token['\\n'] = ' <NEW_LINE> '\n",
    "    return token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max(tokenized, max):\n",
    "    padding_length = max - len(tokenized)\n",
    "    if padding_length == 0:\n",
    "        return tokenized\n",
    "    padding = ['<pad>' for i in range(padding_length)]\n",
    "    tokenized.extend(padding)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:corpus_size]\n",
    "df = df.astype({'text': 'string'})\n",
    "\n",
    "df['text'] = df.apply(clean_data, axis=1)\n",
    "print(df['text'][0])\n",
    "articles = df['text'].values.tolist()\n",
    "\n",
    "longest_article = 0\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    for key, token in token_dict.items():\n",
    "        article = article.replace(key, token)\n",
    "    article = article.lower()\n",
    "    article = article.split()\n",
    "    if len(article) > longest_article:\n",
    "        longest_article = len(article)\n",
    "    tokenized_articles.append(article)\n",
    "# for key, token in token_dict.items():\n",
    "#     articles[0] = article[0].replace(key, token)\n",
    "\n",
    "print(f'longest article contains {longest_article} tokens')\n",
    "\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "for tokens in tokenized_articles:\n",
    "    tokens = pad_to_max(tokens, longest_article)\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "print(f'there are {len(unique_tokens)} unique tokens')\n",
    "\n",
    "print(f'articles equal length: {len(tokenized_articles[0])==len(tokenized_articles[1])}')\n",
    "\n",
    "articles = [' '.join(art) for art in tokenized_articles]\n",
    "\n",
    "print(articles[0])\n",
    "print(tokenized_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_articles.dat', 'wb'):\n",
    "    pickle.dump(tokenized_articles)\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'wb'):\n",
    "    pickle.dump(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wor2vec params\n",
    "\n",
    "batch_size = 5\n",
    "embedding_size = 2\n",
    "voc_size = len(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make skip gram of one size window\n",
    "\n",
    "# all tokens in all articles\n",
    "word_sequence = []\n",
    "for tokenized_article in tokenized_articles:\n",
    "    word_sequence.extend(tokenized_article)\n",
    "\n",
    "word_dict = {w: i for i, w in enumerate(unique_tokens)}\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - window_size):\n",
    "    input = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - window_size]],\n",
    "               word_dict[word_sequence[i + window_size]]]\n",
    "    for w in context:\n",
    "        skip_grams.append([input, w])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(172)\n",
    "\n",
    "\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        # one-hot encoding of words\n",
    "        random_inputs.append(np.eye(voc_size)[data[i][0]])  # input\n",
    "        random_labels.append(data[i][1])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        # parameters between -1 and + 1\n",
    "        # voc_size -> embedding_size Weight\n",
    "        self.W = nn.Parameter(-2 * torch.rand(voc_size,\n",
    "                              embedding_size) + 1).type(dtype).to(device)\n",
    "        # embedding_size -> voc_size Weight\n",
    "        self.V = nn.Parameter(-2 * torch.rand(embedding_size,\n",
    "                              voc_size) + 1).type(dtype).to(device)\n",
    "\n",
    "        self.params = nn.ParameterList([self.W, self.V])\n",
    "\n",
    "    def forward(self, X):\n",
    "        # hidden_layer : [batch_size, embedding_size]\n",
    "        hidden_layer = torch.matmul(X, self.params[0])\n",
    "        # output_layer : [batch_size, voc_size]\n",
    "        output_layer = torch.matmul(hidden_layer, self.params[1])\n",
    "        #return output_layer\n",
    "        return output_layer\n",
    "\n",
    "model = Word2Vec()\n",
    "# Set the model in train mode\n",
    "model.train()\n",
    "\n",
    "# Softmax (for multi-class classification problems) is already included\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(params=model.params, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print_freq = (epochs // 10) or 1\n",
    "all_losses = []\n",
    "avg_loss = 0\n",
    "\n",
    "print(f'<{time.now()}> Training started, doing {epochs} epochs on {corpus_size} articles')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    input_batch, target_batch = random_batch(skip_grams, batch_size)\n",
    "\n",
    "    # new_tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "    input_batch = torch.Tensor(input_batch).to(device)\n",
    "    target_batch = torch.LongTensor(target_batch).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    avg_loss += loss.item()\n",
    "    if (epoch + 1) % print_freq == 0 or epoch == 0:\n",
    "        epoch_time = datetime.fromtimestamp(time.time())\n",
    "        print('<', epoch_time, '> Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        all_losses.append(avg_loss / print_freq)\n",
    "        avg_loss = 0\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "time_done = datetime.fromtimestamp(time.time())\n",
    "print('<', time_done, '> Epoch:', '%04d' %\n",
    "      (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# print(all_losses)\n",
    "# matplotlib couldn't plot cuda tensors\n",
    "# first copy to host memory by loss.cpu() - then detach and to np array\n",
    "# losses_to_plot = [loss.cpu().detach().numpy() for loss in all_losses]\n",
    "# this was f\n",
    "# print(losses_to_plot)\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned W\n",
    "W, _ = model.parameters()\n",
    "print(W.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traiing done and saving the model for future additional training\n",
    "\n",
    "file_path = f'./models/w2v_wsize_{window_size}_epocs_{epochs}.model'\n",
    "torch.save(model.state_dict(), file_path)\n",
    "\n",
    "end_time = datetime.fromtimestamp(time.time())\n",
    "print(f'training finished at: {end_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dictionaries for both directions (word2vector and vector2word)\n",
    "\n",
    "import pickle\n",
    "W, _ = model.parameters()\n",
    "W = W.detach()\n",
    "w2v = {}\n",
    "v2w = {}\n",
    "\n",
    "for i, word in enumerate(unique_tokens):\n",
    "    W, _ = model.parameters()\n",
    "    W = W.detach()\n",
    "    x, y = float(W[i][0]), float(W[i][1])\n",
    "    w2v[word] = (x, y)\n",
    "    v2w[(x, y)] = word\n",
    "\n",
    "print(f'vocabulary size: {len(unique_tokens)}')\n",
    "\n",
    "print(w2v['process'])\n",
    "\n",
    "file_info = f'_{epochs}_ep_{len(articles)}_art_{len(unique_tokens)}_words'\n",
    "\n",
    "with open(f'./models/w2v_{file_info}.dat', 'wb') as f_path:\n",
    "    pickle.dump(w2v, f_path)\n",
    "\n",
    "with open(f'./models/v2w{file_info}.dat', 'wb') as f_path:\n",
    "    pickle.dump(v2w, f_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
