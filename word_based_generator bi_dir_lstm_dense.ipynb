{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "# from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# husk undersøgelser\n",
    "forskellige modeller\n",
    "\n",
    "# test af hyperparametre\n",
    "blandede variabler\n",
    "\n",
    "# mål accuraccy\n",
    "loss og accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 20448\n"
     ]
    }
   ],
   "source": [
    "# get words\n",
    "all_words = []\n",
    "\n",
    "\n",
    "with open('./data/unique_words_2k_articles.dat', 'rb') as file:\n",
    "    all_words = pickle.load(file)\n",
    "\n",
    "vocab_length = len(all_words)\n",
    "print(f'number of characters: {vocab_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['defense', '<quotation_mark>', 'discretionary', '<quotation_mark>', 'spending', 'on', 'programs', 'that', 'support', 'education', '<comma>', 'scientific', 'research', '<comma>', 'infrastructure', '<comma>', 'public', 'health', 'and', 'environmental', 'protection', '<period>', '<quotation_mark>', 'the', '<left_paren>', 'trump', '<right_paren>', 'administration', 'has', 'already', 'been', 'willing', 'to', 'say', '<colon>', 'were', 'going', 'to', 'increase', 'non', '<question_mark>', 'defense', 'discretionary', 'spending', '<period>', '<period>', '<period>', 'by', 'about', '7', 'percent', '<comma>', '<quotation_mark>', 'meadows', '<comma>', 'chairman', 'of', 'the', 'small', 'but', 'influential', 'house', 'freedom', 'caucus', '<comma>', 'said', 'on', 'the', 'program', '<period>', '<quotation_mark>', 'now', '<comma>', 'democrats', 'are', 'saying', 'thats', 'not', 'enough', '<comma>', 'we', 'need', 'to', 'give', 'the', 'government', 'a', 'pay', 'raise', 'of', '10', 'to', '11', 'percent', '<period>', 'for', 'a', 'fiscal', 'conservative', '<comma>', 'i', 'dont', 'see', 'where', 'the', 'rationale', 'is', '<period>', '<period>', '<period>', '<period>', 'eventually', 'you', 'run', 'out', 'of', 'other', 'peoples', 'money', '<comma>', '<quotation_mark>', 'he', 'said', '<period>', 'meadows', 'was', 'among', 'republicans', 'who', 'voted', 'in', 'late', 'december', 'for', 'their', 'partys', 'debt', '<question_mark>', 'financed', 'tax', 'overhaul', '<comma>', 'which', 'is', 'expected', 'to', 'balloon', 'the', 'federal', 'budget', 'deficit', 'and', 'add', 'about', '$1', '<period>', '5', 'trillion', 'over', '10', 'years', 'to', 'the', '$20', 'trillion', 'national', 'debt', '<period>', '<quotation_mark>', 'its', 'interesting', 'to', 'hear', 'mark', 'talk', 'about', 'fiscal', 'responsibility', '<comma>', '<quotation_mark>', 'democratic', 'u', '<period>', 's', '<period>', 'representative', 'joseph', 'crowley', 'said', 'on', 'cbs', '<period>', 'crowley', 'said', 'the', 'republican', 'tax', 'bill', 'would', 'require', 'the', 'united', 'states', 'to', 'borrow', '$1', '<period>', '5', 'trillion', '<comma>', 'to', 'be', 'paid', 'off', 'by', 'future', 'generations', '<comma>', 'to', 'finance', 'tax', 'cuts', 'for', 'corporations', 'and', 'the', 'rich', '<period>', '<quotation_mark>', 'this', 'is', 'one', 'of', 'the', 'least', '<period>', '<period>', '<period>', 'fiscally', 'responsible', 'bills', 'weve', 'ever', 'seen', 'passed', 'in', 'the', 'history', 'of', 'the', 'house', 'of', 'representatives', '<period>', 'i', 'think', 'were', 'going', 'to', 'be', 'paying', 'for', 'this', 'for', 'many', '<comma>', 'many', 'years', 'to', 'come', '<comma>', '<quotation_mark>', 'crowley', 'said', '<period>', 'republicans', 'insist', 'the', 'tax', 'package', '<comma>', 'the', 'biggest', 'u', '<period>', 's', '<period>', 'tax', 'overhaul', 'in', 'more', 'than', '30', 'years', '<comma>', 'will', 'boost', 'the', 'economy', 'and', 'job', 'growth', '<period>', 'house', 'speaker', 'paul', 'ryan', '<comma>', 'who', 'also', 'supported', 'the', 'tax', 'bill', '<comma>', 'recently', 'went', 'further', 'than', 'meadows', '<comma>', 'making', 'clear', 'in', 'a', 'radio', 'interview', 'that', 'welfare', 'or', '<quotation_mark>', 'entitlement', 'reform', '<comma>', '<quotation_mark>', 'as', 'the', 'party', 'often', 'calls', 'it', '<comma>', 'would', 'be', 'a', 'top', 'republican', 'priority', 'in', '2018', '<period>', 'in', 'republican', 'parlance', '<comma>', '<quotation_mark>', 'entitlement', '<quotation_mark>', 'programs', 'mean', 'food', 'stamps', '<comma>', 'housing', 'assistance', '<comma>', 'medicare', 'and', 'medicaid', 'health', 'insurance', 'for', 'the', 'elderly', '<comma>', 'poor', 'and', 'disabled', '<comma>', 'as', 'well', 'as', 'other', 'programs', 'created', 'by', 'washington', 'to', 'assist', 'the', 'needy', '<period>', 'democrats', 'seized', 'on', 'ryans', 'early', 'december', 'remarks', '<comma>', 'saying', 'they', 'showed', 'republicans', 'would', 'try', 'to', 'pay', 'for', 'their', 'tax', 'overhaul', 'by', 'seeking', 'spending', 'cuts', 'for', 'social', 'programs', '<period>', 'but', 'the', 'goals', 'of', 'house', 'republicans', 'may', 'have', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'senate', '<comma>', 'where', 'the', 'votes', 'of', 'some', 'democrats', 'will', 'be', 'needed', 'to', 'approve', 'a', 'budget', 'and', 'prevent', 'a', 'government', 'shutdown', '<period>', 'democrats', 'will', 'use', 'their', 'leverage', 'in', 'the', 'senate', '<comma>', 'which', 'republicans', 'narrowly', 'control', '<comma>', 'to', 'defend', 'both', 'discretionary', 'non', '<question_mark>', 'defense', 'programs', 'and', 'social', 'spending', '<comma>', 'while', 'tackling', 'the', 'issue', 'of', 'the', '<quotation_mark>', 'dreamers', '<comma>', '<quotation_mark>', 'people', 'brought', 'illegally', 'to', 'the', 'country', 'as', 'children', '<period>', 'trump', 'in', 'september', 'put', 'a', 'march', '2018', 'expiration', 'date', 'on', 'the', 'deferred', 'action', 'for', 'childhood', 'arrivals', '<comma>', 'or', 'daca', '<comma>', 'program', '<comma>', 'which', 'protects', 'the', 'young', 'immigrants', 'from', 'deportation', 'and', 'provides', 'them', 'with', 'work', 'permits', '<period>', 'the', 'president', 'has', 'said', 'in', 'recent', 'twitter', 'messages', 'he', 'wants', 'funding', 'for', 'his', 'proposed', 'mexican', 'border', 'wall', 'and', 'other', 'immigration', 'law', 'changes', 'in', 'exchange', 'for', 'agreeing', 'to', 'help', 'the', 'dreamers', '<period>', 'representative', 'debbie', 'dingell', 'told', 'cbs', 'she', 'did', 'not', 'favor', 'linking', 'that', 'issue', 'to', 'other', 'policy', 'objectives', '<comma>', 'such', 'as', 'wall', 'funding', '<period>', '<quotation_mark>', 'we', 'need', 'to', 'do', 'daca', 'clean', '<comma>', '<quotation_mark>', 'she', 'said', '<period>', 'on', 'wednesday', '<comma>', 'trump', 'aides', 'will', 'meet', 'with', 'congressional', 'leaders', 'to', 'discuss', 'those', 'issues', '<period>', 'that', 'will', 'be', 'followed', 'by', 'a', 'weekend', 'of', 'strategy', 'sessions', 'for', 'trump', 'and', 'republican', 'leaders', 'on', 'jan', '<period>', '6', 'and', '7', '<comma>', 'the', 'white', 'house', 'said', '<period>', 'trump', 'was', 'also', 'scheduled', 'to', 'meet', 'on', 'sunday', 'with', 'florida', 'republican', 'governor', 'rick', 'scott', '<comma>', 'who', 'wants', 'more', 'emergency', 'aid', '<period>', 'the', 'house', 'has', 'passed', 'an', '$81', 'billion', 'aid', 'package', 'after', 'hurricanes', 'in', 'florida', '<comma>', 'texas', 'and', 'puerto', 'rico', '<comma>', 'and', 'wildfires', 'in', 'california', '<period>', 'the', 'package', 'far', 'exceeded', 'the', '$44', 'billion', 'requested', 'by', 'the', 'trump', 'administration', '<period>', 'the', 'senate', 'has', 'not', 'yet', 'voted', 'on', 'the', 'aid', '<period>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# read file\n",
    "corpus = []\n",
    "with open('./data/tokenized_articles.dat', 'rb') as file:\n",
    "    corpus = pickle.load(file)\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        print(f'input_size: {input_size}')\n",
    "        self.embed = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_size * num_layers, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        print(f'x: {x}')\n",
    "        print(f'x.shape:')\n",
    "        print(x.shape)\n",
    "        print(hidden.shape)\n",
    "        print(cell.shape)\n",
    "        out = self.embed(x)\n",
    "        print('here?')\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, word2vec, vec2word):\n",
    "        self.word2vec = word2vec\n",
    "        self.vec2word = vec2word\n",
    "        self.chunk_len = 20\n",
    "        self.num_epochs = 4000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = self.num_epochs // 25 or 1\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.lr = 0.003\n",
    "\n",
    "    # returns tensor for entire sentence\n",
    "    def word_tensor(self, word_sequence):\n",
    "        tensor = torch.zeros(len(word_sequence)*2).float()\n",
    "        # print(f'word_sequence: {word_sequence}')\n",
    "        word = word_sequence[0][0]\n",
    "        # print(f'word: {word}')\n",
    "        for c in range(0, len(word_sequence), 2):\n",
    "            word = word_sequence[0]\n",
    "            # print(f'word2vec: {self.word2vec[word_sequence[c]]}')\n",
    "            tensor[c], tensor[c+1] = self.word2vec[word_sequence[0][c]]\n",
    "        return tensor\n",
    "\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(corpus) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = corpus[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len * 2)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len * 2)\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.word_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.word_tensor(text_str[1:])\n",
    "        \n",
    "        return text_input.float(), text_target.float()\n",
    "\n",
    "\n",
    "    def generate(self, initial_words=['i', 'would', 'like'], predict_len=200, temperature=0.85):\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_words)\n",
    "        predicted = initial_words\n",
    "        \n",
    "        for p in range(len(initial_words) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_word = initial_input[-1]\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_word.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_word = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_word = all_words[top_word]\n",
    "            predicted += predicted_word\n",
    "            print(predicted)\n",
    "            last_word = self.char_tensor(predicted_word)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(vocab_length, self.hidden_size,\n",
    "                       self.num_layers, vocab_length).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f'<{datetime.now()}>starting training')\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            input, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(0, self.chunk_len, 2):\n",
    "                x = torch.zeros(2).float().to(device)\n",
    "                print('-'*40)\n",
    "                print('x.shape')\n",
    "                print(x.shape)\n",
    "                print('input.shape')\n",
    "                print(input.shape)\n",
    "                print(input[:, 0:1])\n",
    "\n",
    "                print('input')\n",
    "\n",
    "                print(input)\n",
    "                x[0] = input[:,:1]\n",
    "                x[1] = input[:,:-1]\n",
    "                print(x)\n",
    "                print(x.shape)\n",
    "                print('-'*40)\n",
    "                output, (hidden, cell) = self.rnn(x, hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f'\\n\\n<{datetime.now()}> | epoch: {epoch}/{self.num_epochs} | loss: {loss}')\n",
    "                print(self.generate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n"
     ]
    }
   ],
   "source": [
    "lookup_w2v = {}\n",
    "lookup_v2w = {}\n",
    "\n",
    "with open('./models/w2v__10000_ep_2000_art_20448_words.dat', 'rb') as f_path:\n",
    "    lookup_w2v = pickle.load(f_path)\n",
    "\n",
    "\n",
    "with open('./models/v2w_10000_ep_2000_art_20448_words.dat', 'rb') as f_path:\n",
    "    lookup_v2w = pickle.load(f_path)\n",
    "\n",
    "x, y = lookup_w2v['trump']\n",
    "trump = lookup_v2w[(x, y)]\n",
    "print(trump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 20448\n",
      "<2023-05-29 11:22:40.333169>starting training\n",
      "----------------------------------------\n",
      "x.shape\n",
      "torch.Size([2])\n",
      "input.shape\n",
      "torch.Size([1, 40])\n",
      "tensor([[-0.5665]], device='cuda:0')\n",
      "input\n",
      "tensor([[-0.5665,  0.5179, -1.2829,  0.6886, -0.4476, -0.1006,  0.4528, -0.6932,\n",
      "         -1.5671,  0.3445, -0.6784, -0.6908, -0.6283,  1.1758, -0.5382, -0.0641,\n",
      "         -0.7157,  0.7051, -1.1606,  0.6913,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[39]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[304], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gentext \u001b[39m=\u001b[39m Generator(lookup_w2v, lookup_v2w)\n\u001b[1;32m----> 3\u001b[0m gentext\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[302], line 88\u001b[0m, in \u001b[0;36mGenerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m)\n\u001b[0;32m     87\u001b[0m x[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:,:\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 88\u001b[0m x[\u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     89\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[0;32m     90\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[39]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gentext = Generator(lookup_w2v, lookup_v2w)\n",
    "\n",
    "gentext.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gentext.generate(initial_str='i would like', predict_len=400, temperature=0.65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Linear(512, 100)\n",
    "# input = torch.randn(1, 512)\n",
    "# output = m(input)\n",
    "# print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
