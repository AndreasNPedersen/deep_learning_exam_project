{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2023-05-18 11:42:45.485802 training started on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\bachelor\\deep_learning\\deep_learning_exam_project\\.venv\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles: 21417\n",
      "length of sentences: 51064092\n",
      "71483 unique tokens\n",
      "66369 unique tokens after removing numbers\n",
      "66304 unique tokens after removing special characters\n",
      "['arpaio', 'steck', 'federally', 'fawlty', 'massieu', 'heike', 'jerusalem', 'klem', 'quasselbude', 'dublin', 'runningon', 'flam', 'josmon', 'wb.o', 'shipmatrix', 'ioc', 'goraya', 'whiten', 'tmsnrt.rs/2ppjdrb', 'enacts', 'disability', 'raise', 'quantify', 'whitehorse', 'nauseating', 'bulgarian', 'cristiano', 'abdusalam', 'tmsnrt.rs/2m3scbv', 'elbaz', 'unprofessional', 'sporadic', 'subdivisions', 'theblaze', 'regressed', 'strongly', 'sanctioning', 'executors', 'dokora', 'cohort', 'jenn', 'preceded', 'najimy', 'equerry', 'applicants', 'cancels', 'tatp', 'harmonized', 'cochrane', 'signatories']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchtext as tt\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# 3 Words Sentence (to semplify)\n",
    "# All them form our text corpus\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sentences = ''\n",
    "play = False\n",
    "\n",
    "start_time = datetime.fromtimestamp(time.time())\n",
    "print(f'<{start_time} training started on {device}')\n",
    "\n",
    "if play:\n",
    "    sentences = [ \"i like dog\", \"i like cat\", \"i like animal\", \n",
    "              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n",
    "              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n",
    "              \"apple i movie\", \"book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/True.csv\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokenizer = tt.data.utils.get_tokenizer('spacy')\n",
    "    np_array = df['text'].values\n",
    "    txt_array = np_array.tolist()\n",
    "    sentences = '\\n'.join(txt_array)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'number of articles: {len(txt_array)}')\n",
    "print(f'length of sentences: {len(sentences)}')\n",
    "\n",
    "\n",
    "# list all the words present in our corpus\n",
    "word_sequence = tokenizer(sentences)\n",
    "word_list = list(set(word_sequence))\n",
    "\n",
    "print(f'{len(word_list)} unique tokens')\n",
    "number_match = re.compile('^.*[0-9].*$')\n",
    "punctuation_match = re.compile('^.*\\.|\\\\|\\/|,\\/|\\(|\\).*$')\n",
    "word_list = [word for word in word_list if not number_match.match(word)]\n",
    "print(f'{len(word_list)} unique tokens after removing numbers')\n",
    "word_list = [word for word in word_list if not punctuation_match.match(word)]\n",
    "print(f'{len(word_list)} unique tokens after removing special characters')\n",
    "\n",
    "print(word_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 17), match='tmsnrt.rs/2ppjdrb'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"^.*(\\.|\\\\|\\/|,\\/|\\(|\\)).*$\"\n",
    "\n",
    "word = \"tmsnrt.rs/2ppjdrb\"\n",
    "\n",
    "print(re.match(pattern, word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
